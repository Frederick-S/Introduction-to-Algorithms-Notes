{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction to Algorithms\n\n\nSolutions to Introduction to Algorithms, third edition.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction-to-algorithms", 
            "text": "Solutions to Introduction to Algorithms, third edition.", 
            "title": "Introduction to Algorithms"
        }, 
        {
            "location": "/2-Getting-Started/2.1-Insertion-sort/", 
            "text": "2.1 Insertion sort\n\n\n2.1-1\n\n\n\n\n2.1-2\n\n\nINSERTION-SORT (A) (Decreasing order)\n\nfor j = 2 to A.length\n    key = A[j]\n    // Insert A[j] into the sorted sequence A[1..j - 1]\n    i = j - 1\n    while i \n 0 and A[i] \n key\n        A[i + 1] = A[i]\n        i = i - 1\n    A[i + 1] = key\n\n\n\n\n2.1-3\n\n\nPseudocode:\n\n\nLINEAR-SEARCH (A)\n\nfor i = 1 to A.length\n    if A[i] == v\n        return i\n\nreturn NIL\n\n\n\n\nLoop invariant:\n\n\nAt the start of each iteration of the for loop, v doesn't exist in the subarray A[1..i - 1]\n.\n\n\nAnd let's see how the loop invariant fulfills the three necessary properties.\n\n\nInitialization\n: in the first loop iteration, i = 1, the subarray A[1..i - 1] is an empty array, therefore, v doesn't exist\nint A[1..i - 1]. So it shows that the loop invariant holds prior to the first iteration of the loop.\n\n\nMaintenance\n: at the start of each iteration of the for loop, we suppose it is true that v doesn't exist in the subarray A[1..i - 1]. Then we check if A[i] == v, if it is true, then we are done, we found v. If it is not true, then it means v doesn't exist in the subarray A[1..i], which holds the loop invariant before the i + 1 iteration.\n\n\nTermination\n: when the for loop is terminated, i might be some number between 1 and A.length, or i equals to A.length + 1. If i equals to A.length + 1, we have that the subarray A[1..A.length] doesn't contain v, and since the subarray A[1..A.length] is the entire array, we know v doesn't exist in the entir array and it returns NIL, the algorithm is correct. And if i is some number between 1 and A.length, then we know v is found during the for loop, the algorithm is also correct.\n\n\n2.1-4\n\n\nThe problem description:\n\n\nInput\n: a sequence of n numbers $A = \\langle a_1, a_2, \\ldots, a_n \\rangle, a_i \\in \\langle0, 1\\rangle$, and a sequence of n numbers $B = \\langle b_1, b_2, \\ldots, b_n \\rangle, b_i \\in \\langle0, 1\\rangle$. A and B represent the binay form of two integers.\n\n\nOutput\n: a sequence of n + 1 numbers C, the binary integer represented by C is the sum of the binary integers represented by A and B.\n\n\nPseudocode:\n\n\nBINARY-SUM(A, B)\n\ncarry = 0\nC = [0] * (A.length + 1)\n\nfor i = A.length to 1\n    C[i] = (A[i] + B[i] + carry) % 2\n    carry = (A[i] + B[i] + carry) / 2\n\nC[1] = carry\n\nreturn C", 
            "title": "2.1 Insertion sort"
        }, 
        {
            "location": "/2-Getting-Started/2.1-Insertion-sort/#21-insertion-sort", 
            "text": "", 
            "title": "2.1 Insertion sort"
        }, 
        {
            "location": "/2-Getting-Started/2.1-Insertion-sort/#21-1", 
            "text": "", 
            "title": "2.1-1"
        }, 
        {
            "location": "/2-Getting-Started/2.1-Insertion-sort/#21-2", 
            "text": "INSERTION-SORT (A) (Decreasing order)\n\nfor j = 2 to A.length\n    key = A[j]\n    // Insert A[j] into the sorted sequence A[1..j - 1]\n    i = j - 1\n    while i   0 and A[i]   key\n        A[i + 1] = A[i]\n        i = i - 1\n    A[i + 1] = key", 
            "title": "2.1-2"
        }, 
        {
            "location": "/2-Getting-Started/2.1-Insertion-sort/#21-3", 
            "text": "Pseudocode:  LINEAR-SEARCH (A)\n\nfor i = 1 to A.length\n    if A[i] == v\n        return i\n\nreturn NIL  Loop invariant:  At the start of each iteration of the for loop, v doesn't exist in the subarray A[1..i - 1] .  And let's see how the loop invariant fulfills the three necessary properties.  Initialization : in the first loop iteration, i = 1, the subarray A[1..i - 1] is an empty array, therefore, v doesn't exist\nint A[1..i - 1]. So it shows that the loop invariant holds prior to the first iteration of the loop.  Maintenance : at the start of each iteration of the for loop, we suppose it is true that v doesn't exist in the subarray A[1..i - 1]. Then we check if A[i] == v, if it is true, then we are done, we found v. If it is not true, then it means v doesn't exist in the subarray A[1..i], which holds the loop invariant before the i + 1 iteration.  Termination : when the for loop is terminated, i might be some number between 1 and A.length, or i equals to A.length + 1. If i equals to A.length + 1, we have that the subarray A[1..A.length] doesn't contain v, and since the subarray A[1..A.length] is the entire array, we know v doesn't exist in the entir array and it returns NIL, the algorithm is correct. And if i is some number between 1 and A.length, then we know v is found during the for loop, the algorithm is also correct.", 
            "title": "2.1-3"
        }, 
        {
            "location": "/2-Getting-Started/2.1-Insertion-sort/#21-4", 
            "text": "The problem description:  Input : a sequence of n numbers $A = \\langle a_1, a_2, \\ldots, a_n \\rangle, a_i \\in \\langle0, 1\\rangle$, and a sequence of n numbers $B = \\langle b_1, b_2, \\ldots, b_n \\rangle, b_i \\in \\langle0, 1\\rangle$. A and B represent the binay form of two integers.  Output : a sequence of n + 1 numbers C, the binary integer represented by C is the sum of the binary integers represented by A and B.  Pseudocode:  BINARY-SUM(A, B)\n\ncarry = 0\nC = [0] * (A.length + 1)\n\nfor i = A.length to 1\n    C[i] = (A[i] + B[i] + carry) % 2\n    carry = (A[i] + B[i] + carry) / 2\n\nC[1] = carry\n\nreturn C", 
            "title": "2.1-4"
        }, 
        {
            "location": "/2-Getting-Started/2.2-Analyzing-algorithms/", 
            "text": "2.2 Analyzing algorithms\n\n\n2.2-1\n\n\n$\\Theta(n^3)$.\n\n\n2.2-2\n\n\nPseudocode:\n\n\nSELECTION-SORT (A)\n\n1 for i = 1 to A.length - 1\n2     min_index = i\n3\n4     for j = i + 1 to A.length\n5         if A[min_index] \n A[j]\n6             min_index = j\n7\n8     if i != min_index:\n9         temp = A[i]\n10        A[i] = A[min_index]\n11        A[min_index] = temp\n\n\n\n\nLoop invariant:\n\n\nAt the start of each iteration of the outer for loop, the subarray A[1..i - 1] consists of elements that are in sorted order\n.\n\n\nWhy does it need to run for only the first n - 1 elements, rather than for all n elements?\n\n\nWhen i equals to A.length - 1, only the last two elements may not be in sorted order, and we need to find the A.length - 1 smallest element, so we compare A[A.length - 1] and A[A.length]. We put the smaller element at the index of A.length - 1, and put the larger element at the index of A.length. So after this iteration, the last element of the array is already the largest number. So it's not necessary to run for the nth element.\n\n\nGive the best-case and worst-case running times of selection sort in $\\Theta$ notation.\n\n\nLet's first see the cost of each statement and the number of times each statement is executed. We let $t_i$ denote the number of times the operation in line 6 is executed for that value of i, and let $p_i$ denote the number of times the operation in line 9/10/11 is executed for that value of i.\n\n\n\n\n\n\n\n\nLine number\n\n\nCost\n\n\nTimes\n\n\n\n\n\n\n\n\n\n\n1\n\n\n$c_1$\n\n\n$n$\n\n\n\n\n\n\n2\n\n\n$c_2$\n\n\n$n - 1$\n\n\n\n\n\n\n4\n\n\n$c_4$\n\n\n$\\sum_{i = 1}^{n - 1} (n - i + 1) = \\frac{n(n + 1)}{2} - 1$\n\n\n\n\n\n\n5\n\n\n$c_5$\n\n\n$\\sum_{i = 1}^{n - 1} (n - i) = \\frac{n(n - 1)}{2}$\n\n\n\n\n\n\n6\n\n\n$c_6$\n\n\n$\\sum_{i = 1}^{n - 1} t_i$\n\n\n\n\n\n\n8\n\n\n$c_8$\n\n\n$n - 1$\n\n\n\n\n\n\n9\n\n\n$c_9$\n\n\n$\\sum_{i = 1}^{n - 1} p_i$\n\n\n\n\n\n\n10\n\n\n$c_{10}$\n\n\n$\\sum_{i = 1}^{n - 1} p_i$\n\n\n\n\n\n\n11\n\n\n$c_{11}$\n\n\n$\\sum_{i = 1}^{n - 1} p_i$\n\n\n\n\n\n\n\n\nSo the running time of selection sort is:\n\n\n$$T(n) = c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_6\\sum_{i = 1}^{n - 1} t_i + c_8(n - 1) + \\\\\\\n(c_9 + c_{10} + c_{11})\\sum_{i = 1}^{n - 1} p_i$$\n\n\nNow let's see the best-case. The best-case occurs if the array is already sorted. Thus $t_i$ and $p_i$ are both 0, and the best-case running time is:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_8(n - 1)       \\\\\\\n  \n=\n (\\frac{c_4}{2} + \\frac{c_5}{2})n^2 + (c_1 + c_2 + \\frac{c_4}{2} - \\frac{c_5}{2} + c_8)n - (c_2 + c_4 + c_8)  \\\\\\\n  \n=\n \\Theta(n^2)\n\\end{eqnarray}\n$$\n\n\nAnd the worst-case occurs if the array is in reverse sorted order. Thus in the iteration of j, line 6 is executed from i + 1 to A.length - (i - 1) (because after the first iteration of i, the largest element is at the index of A.length, after the second iteration of i, the second largest element is at the index of A.length - 1, so the last i - 1 elements are bigger than the previous elements), so $t_i = n - i - (i - 1) = n - 2i + 1$. So the times of line 6 is $c_6\\sum_{i = 1}^{n - 1} (n - 2i + 1)$, notice that $n - 2i + 1$ will be 0, when it's 0, it's not necessary to sum it, to make it simple, let's assume it stops when $i = \\frac{n}{2}$. So:\n\n\n$$\n\\begin{eqnarray}\n\\sum_{i = 1}^{n - 1} (n - 2i + 1) \n=\n (n - 1) + (n - 3) + \\ldots + 1 \\\\\\\n\n=\n n * \\frac{n}{2} - (1 + 3 + 5 + \\ldots + (2 * \\frac{n}{2} - 1)) \\\\\\\n\n=\n \\frac{n^2}{2} - \\frac{\\frac{n}{2}(1 + n - 1)}{2} \\\\\\\n\n=\n \\frac{n^2}{4}\n\\end{eqnarray}\n$$\n\n\nNow let's see $p_i$, we already assume that line 6 will not be executed when $i \\geq \\frac{n}{2}$, which also means line 9 to line 11 will not be executed. So $\\sum_{i = 1}^{n - 1} p_i$ should be $\\frac{n}{2}$. So the running time of worst-case is:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_6\\frac{n^2}{4} + c_8(n - 1) + \\frac{c_9 + c_{10} + c_{11}}{2}n \\\\\\\n  \n=\n (\\frac{c_4}{2} + \\frac{c_5}{2} + \\frac{c_6}{4})n^2 + (c_1 + c_2 + \\frac{c_4}{2} - \\frac{c_5}{2} + c_8 + \\frac{c_9 + c_{10} + c_{11}}{2})n - (c_2 + c_4 + c_8)  \\\\\\\n  \n=\n \\Theta(n^2)\n\\end{eqnarray}\n$$\n\n\n2.2-3\n\n\nHow many elements of the input sequence need to be checked on the average?\n\n\nSince the element being searched for is equally likely to be any element in the array, so the probability of finding the target value at index i is $1 / n$, and it will check i elements. So the average checks is:\n\n\n$$\\frac{1}{n}(1 + 2 + 3 + \\ldots + n - 1 + n) = \\frac{1}{n}\\frac{n(n + 1)}{2}=\\frac{n + 1}{2}$$\n\n\nAnd the worst case will check n elements. The running time of average case and worst case are both $\\Theta(n)$.\n\n\n2.2-4\n\n\nWe can check if the input is a best-case first, if it is true, then we can return a predefined solution for that best-case. For example, in a sorting algorithm, if the input is sorted, then we can return the array directly.", 
            "title": "2.2 Analyzing algorithms"
        }, 
        {
            "location": "/2-Getting-Started/2.2-Analyzing-algorithms/#22-analyzing-algorithms", 
            "text": "", 
            "title": "2.2 Analyzing algorithms"
        }, 
        {
            "location": "/2-Getting-Started/2.2-Analyzing-algorithms/#22-1", 
            "text": "$\\Theta(n^3)$.", 
            "title": "2.2-1"
        }, 
        {
            "location": "/2-Getting-Started/2.2-Analyzing-algorithms/#22-2", 
            "text": "Pseudocode:  SELECTION-SORT (A)\n\n1 for i = 1 to A.length - 1\n2     min_index = i\n3\n4     for j = i + 1 to A.length\n5         if A[min_index]   A[j]\n6             min_index = j\n7\n8     if i != min_index:\n9         temp = A[i]\n10        A[i] = A[min_index]\n11        A[min_index] = temp  Loop invariant:  At the start of each iteration of the outer for loop, the subarray A[1..i - 1] consists of elements that are in sorted order .  Why does it need to run for only the first n - 1 elements, rather than for all n elements?  When i equals to A.length - 1, only the last two elements may not be in sorted order, and we need to find the A.length - 1 smallest element, so we compare A[A.length - 1] and A[A.length]. We put the smaller element at the index of A.length - 1, and put the larger element at the index of A.length. So after this iteration, the last element of the array is already the largest number. So it's not necessary to run for the nth element.  Give the best-case and worst-case running times of selection sort in $\\Theta$ notation.  Let's first see the cost of each statement and the number of times each statement is executed. We let $t_i$ denote the number of times the operation in line 6 is executed for that value of i, and let $p_i$ denote the number of times the operation in line 9/10/11 is executed for that value of i.     Line number  Cost  Times      1  $c_1$  $n$    2  $c_2$  $n - 1$    4  $c_4$  $\\sum_{i = 1}^{n - 1} (n - i + 1) = \\frac{n(n + 1)}{2} - 1$    5  $c_5$  $\\sum_{i = 1}^{n - 1} (n - i) = \\frac{n(n - 1)}{2}$    6  $c_6$  $\\sum_{i = 1}^{n - 1} t_i$    8  $c_8$  $n - 1$    9  $c_9$  $\\sum_{i = 1}^{n - 1} p_i$    10  $c_{10}$  $\\sum_{i = 1}^{n - 1} p_i$    11  $c_{11}$  $\\sum_{i = 1}^{n - 1} p_i$     So the running time of selection sort is:  $$T(n) = c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_6\\sum_{i = 1}^{n - 1} t_i + c_8(n - 1) + \\\\\\\n(c_9 + c_{10} + c_{11})\\sum_{i = 1}^{n - 1} p_i$$  Now let's see the best-case. The best-case occurs if the array is already sorted. Thus $t_i$ and $p_i$ are both 0, and the best-case running time is:  $$\n\\begin{eqnarray}\nT(n)  =  c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_8(n - 1)       \\\\\\\n   =  (\\frac{c_4}{2} + \\frac{c_5}{2})n^2 + (c_1 + c_2 + \\frac{c_4}{2} - \\frac{c_5}{2} + c_8)n - (c_2 + c_4 + c_8)  \\\\\\\n   =  \\Theta(n^2)\n\\end{eqnarray}\n$$  And the worst-case occurs if the array is in reverse sorted order. Thus in the iteration of j, line 6 is executed from i + 1 to A.length - (i - 1) (because after the first iteration of i, the largest element is at the index of A.length, after the second iteration of i, the second largest element is at the index of A.length - 1, so the last i - 1 elements are bigger than the previous elements), so $t_i = n - i - (i - 1) = n - 2i + 1$. So the times of line 6 is $c_6\\sum_{i = 1}^{n - 1} (n - 2i + 1)$, notice that $n - 2i + 1$ will be 0, when it's 0, it's not necessary to sum it, to make it simple, let's assume it stops when $i = \\frac{n}{2}$. So:  $$\n\\begin{eqnarray}\n\\sum_{i = 1}^{n - 1} (n - 2i + 1)  =  (n - 1) + (n - 3) + \\ldots + 1 \\\\\\ =  n * \\frac{n}{2} - (1 + 3 + 5 + \\ldots + (2 * \\frac{n}{2} - 1)) \\\\\\ =  \\frac{n^2}{2} - \\frac{\\frac{n}{2}(1 + n - 1)}{2} \\\\\\ =  \\frac{n^2}{4}\n\\end{eqnarray}\n$$  Now let's see $p_i$, we already assume that line 6 will not be executed when $i \\geq \\frac{n}{2}$, which also means line 9 to line 11 will not be executed. So $\\sum_{i = 1}^{n - 1} p_i$ should be $\\frac{n}{2}$. So the running time of worst-case is:  $$\n\\begin{eqnarray}\nT(n)  =  c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_6\\frac{n^2}{4} + c_8(n - 1) + \\frac{c_9 + c_{10} + c_{11}}{2}n \\\\\\\n   =  (\\frac{c_4}{2} + \\frac{c_5}{2} + \\frac{c_6}{4})n^2 + (c_1 + c_2 + \\frac{c_4}{2} - \\frac{c_5}{2} + c_8 + \\frac{c_9 + c_{10} + c_{11}}{2})n - (c_2 + c_4 + c_8)  \\\\\\\n   =  \\Theta(n^2)\n\\end{eqnarray}\n$$", 
            "title": "2.2-2"
        }, 
        {
            "location": "/2-Getting-Started/2.2-Analyzing-algorithms/#22-3", 
            "text": "How many elements of the input sequence need to be checked on the average?  Since the element being searched for is equally likely to be any element in the array, so the probability of finding the target value at index i is $1 / n$, and it will check i elements. So the average checks is:  $$\\frac{1}{n}(1 + 2 + 3 + \\ldots + n - 1 + n) = \\frac{1}{n}\\frac{n(n + 1)}{2}=\\frac{n + 1}{2}$$  And the worst case will check n elements. The running time of average case and worst case are both $\\Theta(n)$.", 
            "title": "2.2-3"
        }, 
        {
            "location": "/2-Getting-Started/2.2-Analyzing-algorithms/#22-4", 
            "text": "We can check if the input is a best-case first, if it is true, then we can return a predefined solution for that best-case. For example, in a sorting algorithm, if the input is sorted, then we can return the array directly.", 
            "title": "2.2-4"
        }, 
        {
            "location": "/2-Getting-Started/2.3-Designing-algorithms/", 
            "text": "2.3 Designing algorithms\n\n\n2.3-1\n\n\n\n\n2.3-2\n\n\nMERGE(A, p, q, r)\n\nn1 = q - p + 1\nn2 = r - q\nlet L[1..n1] and R[1..n2] be new arrays\nfor i = 1 to n1\n    L[i] = A[p + i - 1]\nfor j = 1 to n2\n    R[j] = A[q + j]\ni = 1\nj = 1\nfor k = p to r\n    if i \n n1:\n        A[k] = R[j]\n        j = j + 1\n    else if j \n n2:\n        A[k] = L[i]\n        i = i + 1\n    else if L[i] \n= R[j]:\n        A[k] = L[i]\n        i = i + 1\n    else\n        A[k] = R[j]\n        j = j + 1\n\n\n\n\n2.3-3\n\n\nBecause n is an exact power of 2, so the sequence of n is $2, 4, 8, 16, \\ldots, 2^k$. First, let's see the base case, the base case is $n = 2$. And we have $T(n) = 2 = 2\\lg{2}$. So the statement holds true for $n = 2$.\n\n\nSecond, let's see the inductive step. We assume $T(n) = n\\lg{n}$ is true when $n = 2^k, k \n 1$, and we want to prove it also holds true for the $n + 1$ element, which is $2^{k + 1}$.\n\n\nWhen $n = 2^{k + 1}$, we have:\n\n\n$$\n\\begin{eqnarray}\nT(2^{k + 1}) \n=\n 2T(\\frac{2^{k + 1}}{2}) + 2^{k + 1} \\\\\\\n\n=\n 2T(2^k) + 2^{k + 1} \\\\\\\n\n=\n 2 * 2^k\\lg{2^k} + 2^{k + 1} \\\\\\\n\n=\n 2^{k + 1}\\lg{2^k} + 2^{k + 1} \\\\\\\n\n=\n 2^{k + 1}(\\lg{2^k} + 1) \\\\\\\n\n=\n 2^{k + 1}(\\lg{2^k} + \\lg{2}) \\\\\\\n\n=\n 2^{k + 1}\\lg{2^{k + 1}}\n\\end{eqnarray}\n$$\n\n\nSo $T(n) = n\\lg{n}$ is true for all $n = 2^k, k \\geq 1$.\n\n\n2.3-4\n\n\nThe pseudocode would be like this:\n\n\nINSERT(A, end)\n\nkey = A[end]\ni = end - 1\n\nwhile i \n 0 and A[i] \n key\n    A[i + 1] = A[i]\n    i = i - 1\n\nA[i + 1] = key\n\n\n\n\nRECURSIVE-INSERTION-SORT(A, end)\n\nif end \n 1:\n    RECURSIVE-INSERTION-SORT(A, end - 1)\n    INSERT(A, end)\n\n\n\n\nAnd here is the recurrence for the running time (let $I(n)$ denotes the running time of inserting $A[n]$ to $A[1..n - 1]$):\n\n\n$$\nT(n) =\n\\begin{cases}\n\\Theta(1) \n if\\ n = 1, \\\\\\\nT(n - 1) + I(n) \n if\\ n \\geq 2\n\\end{cases}\n$$\n\n\n2.3-5\n\n\nPseudocode:\n\n\nITERATIVE-BINARY-SEARCH(A, v)\n\nlow = 1\nhigh = n\n\nwhile low \n= high\n    middle = (low + high) / 2\n\n    if A[middle] \n v\n        low = middle + 1\n    else if A[middle] \n v\n        high = middle - 1\n    else\n        return middle\n\nreturn NIL\n\n\n\n\nRECURSIVE-BINARY-SEARCH(A, v, low, high)\n\nif low \n= high\n    middle = (low + high) / 2\n\n    if A[middle] \n v\n        return RECURSIVE-BINARY-SEARCH(A, v, middle + 1, high)\n    else if A[middle] \n v\n        return RECURSIVE-BINARY-SEARCH(A, v, low, middle - 1)\n    else\n        return middle\n\nreturn NIL\n\n\n\n\nFrom the pseudocode we can see either iterative or recursive binary search will halve the size of problem in each step when the middle element doesn't match the target value. So we have:\n\n\n$$\nT(n) =\n\\begin{cases}\nC \n if\\ n = 1, \\\\\\\nT(n / 2) + C \n if\\ n \\geq 2\n\\end{cases}\n$$\n\n\nAnd we can rewrite $T(n)$ like:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(n / 2) + C \\\\\\\n\n=\n T(n / 2^2) + C + C \\\\\\\n\n=\n T(n / 2^3) + C + C + C \\\\\\\n\n=\n ... \\\\\\\n\n=\n T(1) + C\\lg{n} \\\\\\\n\n=\n C(\\lg{n} + 1) \\\\\\\n\n=\n \\Theta(\\lg{n})\n\\end{eqnarray}\n$$\n\n\nSo the worst-case running time of binary search is $\\Theta(\\lg{n})$.\n\n\n2.3-6\n\n\nNo, we cannot. The running time of finding the position to insert the new element is $\\Theta(\\lg{n})$, but we still need $\\Theta(n)$ time to insert that element into the array. So the worst-case running time is still $\\Theta(n^2)$.\n\n\n2.3-7\n\n\nTWO-SUM(S, x)\n\nMERGE-SORT(S)\n\nfor i = 1 to S.length\n    index = BINARY-SEARCH(S, x - S[i])\n\n    if index != NIL and index != i\n        return true\n\nreturn false\n\n\n\n\nFirst we sort S by merge sort, and the running time is $\\Theta(n\\lg{n})$. Then we iterate S, for each element in S, if this element is one of the two elements whose sum is exactly x, then we know x minus this element should be also in S. We can use binary search to search it. If the search result is not NIL and not equal to i, then we return true.\n\n\nThe running time of binary search is $\\Theta(\\lg{n})$, and in the worst-case, we will run it for $n$ times, and the running time is $\\Theta(n\\lg{n})$. So the worst-case running time of this algorithm is $\\Theta(n\\lg{n}) + \\Theta(n\\lg{n})$, which is still $\\Theta(n\\lg{n})$.", 
            "title": "2.3 Designing algorithms"
        }, 
        {
            "location": "/2-Getting-Started/2.3-Designing-algorithms/#23-designing-algorithms", 
            "text": "", 
            "title": "2.3 Designing algorithms"
        }, 
        {
            "location": "/2-Getting-Started/2.3-Designing-algorithms/#23-1", 
            "text": "", 
            "title": "2.3-1"
        }, 
        {
            "location": "/2-Getting-Started/2.3-Designing-algorithms/#23-2", 
            "text": "MERGE(A, p, q, r)\n\nn1 = q - p + 1\nn2 = r - q\nlet L[1..n1] and R[1..n2] be new arrays\nfor i = 1 to n1\n    L[i] = A[p + i - 1]\nfor j = 1 to n2\n    R[j] = A[q + j]\ni = 1\nj = 1\nfor k = p to r\n    if i   n1:\n        A[k] = R[j]\n        j = j + 1\n    else if j   n2:\n        A[k] = L[i]\n        i = i + 1\n    else if L[i]  = R[j]:\n        A[k] = L[i]\n        i = i + 1\n    else\n        A[k] = R[j]\n        j = j + 1", 
            "title": "2.3-2"
        }, 
        {
            "location": "/2-Getting-Started/2.3-Designing-algorithms/#23-3", 
            "text": "Because n is an exact power of 2, so the sequence of n is $2, 4, 8, 16, \\ldots, 2^k$. First, let's see the base case, the base case is $n = 2$. And we have $T(n) = 2 = 2\\lg{2}$. So the statement holds true for $n = 2$.  Second, let's see the inductive step. We assume $T(n) = n\\lg{n}$ is true when $n = 2^k, k   1$, and we want to prove it also holds true for the $n + 1$ element, which is $2^{k + 1}$.  When $n = 2^{k + 1}$, we have:  $$\n\\begin{eqnarray}\nT(2^{k + 1})  =  2T(\\frac{2^{k + 1}}{2}) + 2^{k + 1} \\\\\\ =  2T(2^k) + 2^{k + 1} \\\\\\ =  2 * 2^k\\lg{2^k} + 2^{k + 1} \\\\\\ =  2^{k + 1}\\lg{2^k} + 2^{k + 1} \\\\\\ =  2^{k + 1}(\\lg{2^k} + 1) \\\\\\ =  2^{k + 1}(\\lg{2^k} + \\lg{2}) \\\\\\ =  2^{k + 1}\\lg{2^{k + 1}}\n\\end{eqnarray}\n$$  So $T(n) = n\\lg{n}$ is true for all $n = 2^k, k \\geq 1$.", 
            "title": "2.3-3"
        }, 
        {
            "location": "/2-Getting-Started/2.3-Designing-algorithms/#23-4", 
            "text": "The pseudocode would be like this:  INSERT(A, end)\n\nkey = A[end]\ni = end - 1\n\nwhile i   0 and A[i]   key\n    A[i + 1] = A[i]\n    i = i - 1\n\nA[i + 1] = key  RECURSIVE-INSERTION-SORT(A, end)\n\nif end   1:\n    RECURSIVE-INSERTION-SORT(A, end - 1)\n    INSERT(A, end)  And here is the recurrence for the running time (let $I(n)$ denotes the running time of inserting $A[n]$ to $A[1..n - 1]$):  $$\nT(n) =\n\\begin{cases}\n\\Theta(1)   if\\ n = 1, \\\\\\\nT(n - 1) + I(n)   if\\ n \\geq 2\n\\end{cases}\n$$", 
            "title": "2.3-4"
        }, 
        {
            "location": "/2-Getting-Started/2.3-Designing-algorithms/#23-5", 
            "text": "Pseudocode:  ITERATIVE-BINARY-SEARCH(A, v)\n\nlow = 1\nhigh = n\n\nwhile low  = high\n    middle = (low + high) / 2\n\n    if A[middle]   v\n        low = middle + 1\n    else if A[middle]   v\n        high = middle - 1\n    else\n        return middle\n\nreturn NIL  RECURSIVE-BINARY-SEARCH(A, v, low, high)\n\nif low  = high\n    middle = (low + high) / 2\n\n    if A[middle]   v\n        return RECURSIVE-BINARY-SEARCH(A, v, middle + 1, high)\n    else if A[middle]   v\n        return RECURSIVE-BINARY-SEARCH(A, v, low, middle - 1)\n    else\n        return middle\n\nreturn NIL  From the pseudocode we can see either iterative or recursive binary search will halve the size of problem in each step when the middle element doesn't match the target value. So we have:  $$\nT(n) =\n\\begin{cases}\nC   if\\ n = 1, \\\\\\\nT(n / 2) + C   if\\ n \\geq 2\n\\end{cases}\n$$  And we can rewrite $T(n)$ like:  $$\n\\begin{eqnarray}\nT(n)  =  T(n / 2) + C \\\\\\ =  T(n / 2^2) + C + C \\\\\\ =  T(n / 2^3) + C + C + C \\\\\\ =  ... \\\\\\ =  T(1) + C\\lg{n} \\\\\\ =  C(\\lg{n} + 1) \\\\\\ =  \\Theta(\\lg{n})\n\\end{eqnarray}\n$$  So the worst-case running time of binary search is $\\Theta(\\lg{n})$.", 
            "title": "2.3-5"
        }, 
        {
            "location": "/2-Getting-Started/2.3-Designing-algorithms/#23-6", 
            "text": "No, we cannot. The running time of finding the position to insert the new element is $\\Theta(\\lg{n})$, but we still need $\\Theta(n)$ time to insert that element into the array. So the worst-case running time is still $\\Theta(n^2)$.", 
            "title": "2.3-6"
        }, 
        {
            "location": "/2-Getting-Started/2.3-Designing-algorithms/#23-7", 
            "text": "TWO-SUM(S, x)\n\nMERGE-SORT(S)\n\nfor i = 1 to S.length\n    index = BINARY-SEARCH(S, x - S[i])\n\n    if index != NIL and index != i\n        return true\n\nreturn false  First we sort S by merge sort, and the running time is $\\Theta(n\\lg{n})$. Then we iterate S, for each element in S, if this element is one of the two elements whose sum is exactly x, then we know x minus this element should be also in S. We can use binary search to search it. If the search result is not NIL and not equal to i, then we return true.  The running time of binary search is $\\Theta(\\lg{n})$, and in the worst-case, we will run it for $n$ times, and the running time is $\\Theta(n\\lg{n})$. So the worst-case running time of this algorithm is $\\Theta(n\\lg{n}) + \\Theta(n\\lg{n})$, which is still $\\Theta(n\\lg{n})$.", 
            "title": "2.3-7"
        }, 
        {
            "location": "/2-Getting-Started/Problems/", 
            "text": "Problems\n\n\n2-1\n\n\na\n\n\nWe know that the worst-case running time of sorting $n$ elements with insertion sort is $\\Theta(n^2)$. So the running time of sorting $k$ elements is $\\Theta(k^2)$. And there are $\\frac{n}{k}$ sublists of length $k$, so the whole running time is $\\frac{n}{k} * \\Theta(k^2) = \\Theta(nk)$.\n\n\nb\n\n\nWe can go back to look at figure 2.5 in the book. In original merge sort, there are $n$ elements at the bottom in recursion tree. In each step, current array will be divided into two subarrays, until the array only contains one element. But in the modified algorithm, this operation stops when current array contains less than $k$ elements. Thus, it stops after $\\lg{\\frac{n}{k}}$ recursions. So the height of recursion tree is $\\lg{\\frac{n}{k}} + 1$. Because each level contributes a cost of $\\Theta(n)$ merge operation, the total merge running time is $\\Theta(n)(\\lg{\\frac{n}{k}} + 1) = \\Theta(n\\lg{\\frac{n}{k}})$.\n\n\nc\n\n\nGiven the modified algorithm runs in $\\Theta(nk + n\\lg{\\frac{n}{k}})$ worst-case time, we know $n\\lg{\\frac{n}{k}} \\leq n\\lg{n}$, so this part will not influence the upper bound running time. Thus $nk$ is responsible for the worst-case running time of the modified algorithm. And we can see if $k$ is larger than $\\lg{n}$, the modified algorithm will not have the same running time as the standard merge sort algorithm. So the largest value of $k$ is $\\lg{n}$.\n\n\nd\n\n\nThe optimum value of $k$ is system dependent. A optimum value on one machine may not be optimum on another machine.\n\n\n2-2\n\n\na\n\n\nWe need to prove the array A' consists of the elements originally in A.\n\n\nb\n\n\nLoop invariant:\n\n\nAt the start of each iteration of the for loop of lines 2-4, A[j] is the smallest in the subarray A[j..n]\n.\n\n\nInitialization\n: at the start of the iteration, j is n, and the subarray A[j..n] only contains one element, which is of course the smallest.\n\n\nMaintenance\n: if it is true before an iteration of the loop, we have A[j] is the smallest one in the subarray A[j..n]. Then we compare A[j - 1] and A[j], if A[j - 1] is smaller, we exchange A[j] with A[j - 1], thus A[j - 1] is the smallest one in the subarray A[j - 1..n]. So it remains true before the next iteration.\n\n\nTermination\n: it terminates when j equals to i. And when it terminates, we know A[i] is the smallest element in the subarray A[i..n]. So the loop invariant is correct.\n\n\nc\n\n\nLoop invariant:\n\n\nAt the start of each iteration of the for loop of lines 1-4, the subarray A[1..i - 1] is in sorted order, and any element in the subarray A[1..i - 1] is not larger than any element in the subarray A[i..n]\n.\n\n\nInitialization\n: at the start of the iteration, i is 1, the subarray A[1..i - 1] is empty, the loop invariant is true.\n\n\nMaintenance\n: if it is true before an iteration of the loop, we have the subarray A[1..i - 1] is in sorted order. After the for loop in lines 2-4, A[i] is the smallest in the subarray A[i..n]. Because the elements in the subarray A[1..i - 1] is not larget than the elements in the subarray A[i..n], we know the subarray A[1..i] is still sorted. It remains true before the next iteration.\n\n\nTermination\n: it terminates when i is n. Substituting n for i in the wording of loop invariant, we have that the subarray A[1..n - 1] is in sorted order, and the elements in the subarray A[1..n - 1] is not larger than the elements in the subarray A[n..n], so the subarray A[1..n] is in sorted order. And the subarray A[1..n] is the entire array, we conclude that the entire array is sorted. Hence, the algorithm is correct.\n\n\nd\n\n\nThe worst-case running time of bubblesort is approximately $n + (n - 1) + \\ldots + 3 = \\Theta(n^2)$.\n\n\nThe worst-case running time of insertion sort is also $\\Theta(n^2)$, but the best-case running time of insertion sort is $\\Theta(n)$. For bubblesort, the best-case running time is still $\\Theta(n^2)$, because a best-case input could not reduce the cost of the for loop of lines 2-4.\n\n\n2-3\n\n\na\n\n\nIt's $\\Theta(n)$.\n\n\nb\n\n\nThe pseudocode of the naive polynomial-evaluation algorithm:\n\n\nPOLYNOMIAL-EVALUATION(A, x)\n\ny = 0\n\nfor i = 0 to n\n    a = A[i]\n    x_product = 1\n\n    for j = 1 to i\n        x_product = x_product * x\n\n    y = y + a * x_product\n\n\n\n\nThe running time of this algorithm is $\\Theta(n^2)$. It's slower than the Horner's rule.\n\n\nc\n\n\nInitialization\n: at the start of the iteration of the for loop, i is n, and $y = \\sum_{k = 0}^{-1} a_{k + n + 1}{x^k}$, this is not a valid summation, so y is still 0. Thus the loop invariant is correct.\n\n\nMaintenance\n: at the start of the ith iteration, we have:\n$$y = \\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1}{x^k}$$\n\n\nAnd after the ith iteration, we have:\n\n\n$$\n\\begin{eqnarray}\ny \n=\n a_i + xy = a_i + x\\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1}{x^k} \\\\\\\n\n=\n a_i + x(a_{i + 1}x^0 + a_{i + 2}x^1 + \\ldots + a_nx^{n - i - 1}) \\\\\\\n\n=\n a_i + a_{i + 1}x^1 + a_{i + 2}x^2 + \\ldots + a_nx^{n - i} \\\\\\\n\n=\n a_ix^0 + a_{i + 1}x^1 + a_{i + 2}x^2 + \\ldots + a_nx^{n - i} \\\\\\\n\n=\n \\sum_{k = 0}^{n - i} a_{k + i}{x^k} \\\\\\\n\n=\n \\sum_{k = 0}^{n - ((i - 1) + 1)} a_{k + (i - 1) + 1}{x^k}\n\\end{eqnarray}\n$$\n\n\nSo the loop invariant is still true after the end of ith iteration.\n\n\nTermination\n: when the for loop terminates, i is -1, and we replace i with -1 in the summation:\n\n\n$$y = \\sum_{k = 0}^{n - (-1 + 1)} a_{k + -1 + 1}{x^k} = \\sum_{k = 0}^{n} a_{k}{x^k}$$\n\n\nSo the loop invariant is correct.\n\n\nd\n\n\nAt the end of the iteration of the for loop of lines 2-3, we have $y = \\sum_{k = 0}^{n} a_{k}{x^k}$, so it's correct.\n\n\n2-4\n\n\na\n\n\n$\\langle 0, 4\\rangle, \\langle 1, 4\\rangle, \\langle 2, 3\\rangle, \\langle 2, 4\\rangle, \\langle 6, 1\\rangle$.\n\n\nb\n\n\nThe array $\\lbrace n, \\ldots, 2, 1\\rbrace$ has the most inversions. It has:\n\n\n$$(n - 1) + (n - 2) + \\ldots + 1 = \\frac{n(n - 1)}{2}$$\n\n\nc\n\n\nLet's go back to the pseudocode of insertion sort. We know the running time of insertion sort is:\n\n\n$$T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\sum_{j = 2}^{n} t_j + c_6\\sum_{j = 2}^{n} (t_j - 1) + c_7\\sum_{j = 2}^{n} (t_j - 1) + c_8(n - 1)$$\n\n\nwhere $t_j$ is the number of times the while loop test in line 5 is executed for the jth outer for loop.\n\n\nWe can see lines 6 and 7 will be executed when there is an inversion, let's denote $p_j$ the number of inversions in the subarray A[1..j]. So we have $p_j = t_j - 1$. When j is increasing, the number of inversions in the whole array A[1..n] is decreasing, but it doesn't create new inversions in eath iteration, since the relative order in subarray A[1..j] and A[j + 1..n] are not changed. So if we denote $m$ the number of inversions in the whole array A, we have $\\sum_{j = 2}^{n} p_j = \\sum_{j = 2}^{n} (t_j - 1) = m$. So:\n\n\n$$T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5(m + n - 1) + c_6m + c_7m + c_8(n - 1)$$\n\n\nd\n\n\nNUMBER-OF-INVERSIONS(A)\n    n = A.length\n\n    return MERGE-SORT(A, 1, n)\n\nMERGE-SORT(A, p, r)\n    inversions = 0\n\n    if p \n r\n        q = [(p + q) / 2]\n\n        inversions += MERGE-SORT(A, p, q)\n        inversions += MERGE-SORT(A, q + 1, r)\n        inversions += MERGE(A, p, q, r)\n\n    return inversions\n\nMERGE(A, p, q, r)\n\nn1 = q - p + 1\nn2 = r - q\nlet L[1..n1] and R[1..n2] be new arrays\nfor i = 1 to n1\n    L[i] = A[p + i - 1]\nfor j = 1 to n2\n    R[j] = A[q + j]\ni = 1\nj = 1\ninversions = 0\nfor k = p to r\n    if i \n n1:\n        A[k] = R[j]\n        j = j + 1\n    else if j \n n2:\n        A[k] = L[i]\n        i = i + 1\n    else if L[i] \n= R[j]:\n        A[k] = L[i]\n        i = i + 1\n    else\n        A[k] = R[j]\n        j = j + 1\n        inversions += n1 - i + 1", 
            "title": "Problems"
        }, 
        {
            "location": "/2-Getting-Started/Problems/#problems", 
            "text": "", 
            "title": "Problems"
        }, 
        {
            "location": "/2-Getting-Started/Problems/#2-1", 
            "text": "a  We know that the worst-case running time of sorting $n$ elements with insertion sort is $\\Theta(n^2)$. So the running time of sorting $k$ elements is $\\Theta(k^2)$. And there are $\\frac{n}{k}$ sublists of length $k$, so the whole running time is $\\frac{n}{k} * \\Theta(k^2) = \\Theta(nk)$.  b  We can go back to look at figure 2.5 in the book. In original merge sort, there are $n$ elements at the bottom in recursion tree. In each step, current array will be divided into two subarrays, until the array only contains one element. But in the modified algorithm, this operation stops when current array contains less than $k$ elements. Thus, it stops after $\\lg{\\frac{n}{k}}$ recursions. So the height of recursion tree is $\\lg{\\frac{n}{k}} + 1$. Because each level contributes a cost of $\\Theta(n)$ merge operation, the total merge running time is $\\Theta(n)(\\lg{\\frac{n}{k}} + 1) = \\Theta(n\\lg{\\frac{n}{k}})$.  c  Given the modified algorithm runs in $\\Theta(nk + n\\lg{\\frac{n}{k}})$ worst-case time, we know $n\\lg{\\frac{n}{k}} \\leq n\\lg{n}$, so this part will not influence the upper bound running time. Thus $nk$ is responsible for the worst-case running time of the modified algorithm. And we can see if $k$ is larger than $\\lg{n}$, the modified algorithm will not have the same running time as the standard merge sort algorithm. So the largest value of $k$ is $\\lg{n}$.  d  The optimum value of $k$ is system dependent. A optimum value on one machine may not be optimum on another machine.", 
            "title": "2-1"
        }, 
        {
            "location": "/2-Getting-Started/Problems/#2-2", 
            "text": "a  We need to prove the array A' consists of the elements originally in A.  b  Loop invariant:  At the start of each iteration of the for loop of lines 2-4, A[j] is the smallest in the subarray A[j..n] .  Initialization : at the start of the iteration, j is n, and the subarray A[j..n] only contains one element, which is of course the smallest.  Maintenance : if it is true before an iteration of the loop, we have A[j] is the smallest one in the subarray A[j..n]. Then we compare A[j - 1] and A[j], if A[j - 1] is smaller, we exchange A[j] with A[j - 1], thus A[j - 1] is the smallest one in the subarray A[j - 1..n]. So it remains true before the next iteration.  Termination : it terminates when j equals to i. And when it terminates, we know A[i] is the smallest element in the subarray A[i..n]. So the loop invariant is correct.  c  Loop invariant:  At the start of each iteration of the for loop of lines 1-4, the subarray A[1..i - 1] is in sorted order, and any element in the subarray A[1..i - 1] is not larger than any element in the subarray A[i..n] .  Initialization : at the start of the iteration, i is 1, the subarray A[1..i - 1] is empty, the loop invariant is true.  Maintenance : if it is true before an iteration of the loop, we have the subarray A[1..i - 1] is in sorted order. After the for loop in lines 2-4, A[i] is the smallest in the subarray A[i..n]. Because the elements in the subarray A[1..i - 1] is not larget than the elements in the subarray A[i..n], we know the subarray A[1..i] is still sorted. It remains true before the next iteration.  Termination : it terminates when i is n. Substituting n for i in the wording of loop invariant, we have that the subarray A[1..n - 1] is in sorted order, and the elements in the subarray A[1..n - 1] is not larger than the elements in the subarray A[n..n], so the subarray A[1..n] is in sorted order. And the subarray A[1..n] is the entire array, we conclude that the entire array is sorted. Hence, the algorithm is correct.  d  The worst-case running time of bubblesort is approximately $n + (n - 1) + \\ldots + 3 = \\Theta(n^2)$.  The worst-case running time of insertion sort is also $\\Theta(n^2)$, but the best-case running time of insertion sort is $\\Theta(n)$. For bubblesort, the best-case running time is still $\\Theta(n^2)$, because a best-case input could not reduce the cost of the for loop of lines 2-4.", 
            "title": "2-2"
        }, 
        {
            "location": "/2-Getting-Started/Problems/#2-3", 
            "text": "a  It's $\\Theta(n)$.  b  The pseudocode of the naive polynomial-evaluation algorithm:  POLYNOMIAL-EVALUATION(A, x)\n\ny = 0\n\nfor i = 0 to n\n    a = A[i]\n    x_product = 1\n\n    for j = 1 to i\n        x_product = x_product * x\n\n    y = y + a * x_product  The running time of this algorithm is $\\Theta(n^2)$. It's slower than the Horner's rule.  c  Initialization : at the start of the iteration of the for loop, i is n, and $y = \\sum_{k = 0}^{-1} a_{k + n + 1}{x^k}$, this is not a valid summation, so y is still 0. Thus the loop invariant is correct.  Maintenance : at the start of the ith iteration, we have:\n$$y = \\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1}{x^k}$$  And after the ith iteration, we have:  $$\n\\begin{eqnarray}\ny  =  a_i + xy = a_i + x\\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1}{x^k} \\\\\\ =  a_i + x(a_{i + 1}x^0 + a_{i + 2}x^1 + \\ldots + a_nx^{n - i - 1}) \\\\\\ =  a_i + a_{i + 1}x^1 + a_{i + 2}x^2 + \\ldots + a_nx^{n - i} \\\\\\ =  a_ix^0 + a_{i + 1}x^1 + a_{i + 2}x^2 + \\ldots + a_nx^{n - i} \\\\\\ =  \\sum_{k = 0}^{n - i} a_{k + i}{x^k} \\\\\\ =  \\sum_{k = 0}^{n - ((i - 1) + 1)} a_{k + (i - 1) + 1}{x^k}\n\\end{eqnarray}\n$$  So the loop invariant is still true after the end of ith iteration.  Termination : when the for loop terminates, i is -1, and we replace i with -1 in the summation:  $$y = \\sum_{k = 0}^{n - (-1 + 1)} a_{k + -1 + 1}{x^k} = \\sum_{k = 0}^{n} a_{k}{x^k}$$  So the loop invariant is correct.  d  At the end of the iteration of the for loop of lines 2-3, we have $y = \\sum_{k = 0}^{n} a_{k}{x^k}$, so it's correct.", 
            "title": "2-3"
        }, 
        {
            "location": "/2-Getting-Started/Problems/#2-4", 
            "text": "a  $\\langle 0, 4\\rangle, \\langle 1, 4\\rangle, \\langle 2, 3\\rangle, \\langle 2, 4\\rangle, \\langle 6, 1\\rangle$.  b  The array $\\lbrace n, \\ldots, 2, 1\\rbrace$ has the most inversions. It has:  $$(n - 1) + (n - 2) + \\ldots + 1 = \\frac{n(n - 1)}{2}$$  c  Let's go back to the pseudocode of insertion sort. We know the running time of insertion sort is:  $$T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\sum_{j = 2}^{n} t_j + c_6\\sum_{j = 2}^{n} (t_j - 1) + c_7\\sum_{j = 2}^{n} (t_j - 1) + c_8(n - 1)$$  where $t_j$ is the number of times the while loop test in line 5 is executed for the jth outer for loop.  We can see lines 6 and 7 will be executed when there is an inversion, let's denote $p_j$ the number of inversions in the subarray A[1..j]. So we have $p_j = t_j - 1$. When j is increasing, the number of inversions in the whole array A[1..n] is decreasing, but it doesn't create new inversions in eath iteration, since the relative order in subarray A[1..j] and A[j + 1..n] are not changed. So if we denote $m$ the number of inversions in the whole array A, we have $\\sum_{j = 2}^{n} p_j = \\sum_{j = 2}^{n} (t_j - 1) = m$. So:  $$T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5(m + n - 1) + c_6m + c_7m + c_8(n - 1)$$  d  NUMBER-OF-INVERSIONS(A)\n    n = A.length\n\n    return MERGE-SORT(A, 1, n)\n\nMERGE-SORT(A, p, r)\n    inversions = 0\n\n    if p   r\n        q = [(p + q) / 2]\n\n        inversions += MERGE-SORT(A, p, q)\n        inversions += MERGE-SORT(A, q + 1, r)\n        inversions += MERGE(A, p, q, r)\n\n    return inversions\n\nMERGE(A, p, q, r)\n\nn1 = q - p + 1\nn2 = r - q\nlet L[1..n1] and R[1..n2] be new arrays\nfor i = 1 to n1\n    L[i] = A[p + i - 1]\nfor j = 1 to n2\n    R[j] = A[q + j]\ni = 1\nj = 1\ninversions = 0\nfor k = p to r\n    if i   n1:\n        A[k] = R[j]\n        j = j + 1\n    else if j   n2:\n        A[k] = L[i]\n        i = i + 1\n    else if L[i]  = R[j]:\n        A[k] = L[i]\n        i = i + 1\n    else\n        A[k] = R[j]\n        j = j + 1\n        inversions += n1 - i + 1", 
            "title": "2-4"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/", 
            "text": "3.1 Asymptotic notation\n\n\n3.1-1\n\n\nAccording to the basic definition of $\\Theta$-notation, if $max(f(n), g(n)) = \\Theta(f(n) + g(n))$, then there exist positive constants $c_1$, $c_2$ and $n_0$ such that:\n\n\n$$0 \\leq c_1(f(n) + g(n)) \\leq max(f(n), g(n)) \\leq c_2(f(n) + g(n)),\\ for \\ all \\ n \\geq n_0$$\n\n\nBecause f(n) and g(n) are nongenative, so it's obvious that $max(f(n), g(n)) \\leq f(n) + g(n)$, so we can let $c_2$ be 1.\n\n\nAnd we also know that $max(f(n), g(n)) \\geq f(n)$ and $max(f(n), g(n)) \\geq g(n)$, so $2max(f(n), g(n)) \\geq f(n) + g(n)$, thus $max(f(n), g(n)) \\geq \\frac{f(n) + g(n)}{2}$. So we can let $c_1$ be $\\frac{1}{2}$.\n\n\nBecause $0 \\leq \\frac{f(n) + g(n)}{2} \\leq max(f(n), g(n)) \\leq f(n) + g(n)$ is true for all n, so we can let $n_0$ be 1.\n\n\nSo we've found positive constants $c_1$, $c_2$ and $n_0$, thus $max(f(n), g(n)) = \\Theta(f(n) + g(n))$.\n\n\n3.1-2\n\n\nAccording to \nNewton's generalised binomial theorem\n, we have:\n\n\n$$(n + a)^b = \\sum_{k = 0}^{\\infty} \\binom{b}{k}n^{b - k}a^k = n^b + bn^{b - 1}a + \\frac{b(b - 1)}{2!}n^{b - 2}a^2 + \\dots$$\n\n\nSince $n^b$ grows faster than others, so $(n + a)^b = \\Theta(n^b)$.\n\n\n3.1-3\n\n\nThe O-notation denotes a upper bound, so we can not say \"at least\".\n\n\n3.1-4\n\n\n$2^{n + 1} = O(2^n)$, because there exist positive constants c = 2 and $n_0 = 1$ such that $0 \\leq 2^{n + 1} \\leq 2 * 2^n\\ for\\ all\\ n \\geq n_0$.\n\n\n$2^{2n} \\neq O(2^n)$. Suppose $2^{2n} = O(2^n)$, then there exist positive constants c and $n_0$ such that $0 \\leq 2^{2n} \\leq c2^n\\ for\\ all\\ n \\geq n_0$, which means $0 \\leq 2^n \\leq c\\ for\\ all\\ n \\geq n_0$. No matter how big c is, $2^n$ will be bigger than c after a specific $n_1$, so you cannot find a $n_0$ like that.\n\n\n3.1-5\n\n\nIf $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq f(n) \\leq c_1g(n)\\ for\\ all\\ n \\geq n_1$, and there exist positive constants $c_2$ and $n_2$ such that $0 \\leq c_2g(n) \\leq f(n)\\ for\\ all\\ n \\geq n_2$. We can choose $n_0 = max(n_1, n_2)$, and combine the two inequations we have $0 \\leq c_2g(n) \\leq f(n) \\leq c_1g(n)\\ for\\ all\\ n \\geq n_0$, which is the definition for $f(n) = \\Theta(g(n))$. So we proved if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$, then $f(n) = \\Theta(g(n))$.\n\n\nNow lets' suppose $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$ is not true (either one of them is wrong or both are wrong), but $f(n) = \\Theta(g(n))$ is ture. And let's prove this hypothesis is not correct.\n\n\nIf we have $f(n) = \\Theta(g(n))$, we know there exist positive constants c and $n_0$ such that $0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n)\\ for\\ all\\ n \\geq n_0$. So we can separate it into two inequations:\n\n\n$$0 \\leq f(n) \\leq c_2g(n)\\ for\\ all\\ n \\geq n_0$$\n$$0 \\leq c_1g(n) \\leq f(n)\\ for\\ all\\ n \\geq n_0$$\n\n\nwhich are the definitions of $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$. So we proved the hypothesis is wrong, thus we proved the \"only if\" part.\n\n\n3.1-6\n\n\nIt's similiar to 3.1-5, the worst-case and best-case running time denote the upper bound and lower bound, which are $O(g(n))$ and $\\Omega(g(n))$. So the proof is similar.\n\n\n3.1-7\n\n\nLet's let $f(n) = o(g(n))$ and $h(n) = \\omega(g(n))$. According to the definition, for any positive constants $c_1$, $c_2$, there exist positive constants $n_1$, $n_2$ such that:\n\n\n$$0 \\leq f(n) \n c_1g(n)\\ for\\ all\\ n \\geq n_1$$\n$$0 \\leq c_2g(n) \n h(n)\\ for\\ all\\ n \\geq n_2$$\n\n\nSuppose $o(g(n)) \\cap \\omega(g(n))$ is not empty, so at least we have one f(n) and one h(n) such that f(n) = h(n), because $c_1$ and $c_2$ are any positive constants, so we let $c_1 = c_2 = c$,\nand let $n_0 = max(n_1, n_2)$, so we have:\n\n\n$$0 \\leq f(n) \n cg(n)\\ for\\ all\\ n \\geq n_0$$\n$$0 \\leq cg(n) \n f(n)\\ for\\ all\\ n \\geq n_0$$\n\n\nwhich also means:\n\n\n$$0 \\leq cg(n) \n f(n)\\ \n cg(n)\\ for\\ all\\ n \\geq n_0$$\n\n\nAnd we know the above inequation is impossible. So the hypothesis is wrong, thus $o(g(n)) \\cap \\omega(g(n))$ is empty.\n\n\n3.1-8\n\n\n$$\\Omega(g(n, m)) = \\lbrace f(n, m): \\text {there exist positive constants } c, n_0, \\text {and } m_0 \\text{ such that } 0 \\leq cg(n, m) \\leq f(n, m) \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0 \\rbrace$$\n\n\n$$\\Theta(g(n, m)) = \\lbrace f(n, m): \\text {there exist positive constants } c_1, c_2, n_0, \\text {and } m_0 \\text{ such that } 0 \\leq c_1g(n, m) \\leq f(n, m) \\leq c_2g(n, m) \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0 \\rbrace$$", 
            "title": "3.1 Asymptotic notation"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/#31-asymptotic-notation", 
            "text": "", 
            "title": "3.1 Asymptotic notation"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/#31-1", 
            "text": "According to the basic definition of $\\Theta$-notation, if $max(f(n), g(n)) = \\Theta(f(n) + g(n))$, then there exist positive constants $c_1$, $c_2$ and $n_0$ such that:  $$0 \\leq c_1(f(n) + g(n)) \\leq max(f(n), g(n)) \\leq c_2(f(n) + g(n)),\\ for \\ all \\ n \\geq n_0$$  Because f(n) and g(n) are nongenative, so it's obvious that $max(f(n), g(n)) \\leq f(n) + g(n)$, so we can let $c_2$ be 1.  And we also know that $max(f(n), g(n)) \\geq f(n)$ and $max(f(n), g(n)) \\geq g(n)$, so $2max(f(n), g(n)) \\geq f(n) + g(n)$, thus $max(f(n), g(n)) \\geq \\frac{f(n) + g(n)}{2}$. So we can let $c_1$ be $\\frac{1}{2}$.  Because $0 \\leq \\frac{f(n) + g(n)}{2} \\leq max(f(n), g(n)) \\leq f(n) + g(n)$ is true for all n, so we can let $n_0$ be 1.  So we've found positive constants $c_1$, $c_2$ and $n_0$, thus $max(f(n), g(n)) = \\Theta(f(n) + g(n))$.", 
            "title": "3.1-1"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/#31-2", 
            "text": "According to  Newton's generalised binomial theorem , we have:  $$(n + a)^b = \\sum_{k = 0}^{\\infty} \\binom{b}{k}n^{b - k}a^k = n^b + bn^{b - 1}a + \\frac{b(b - 1)}{2!}n^{b - 2}a^2 + \\dots$$  Since $n^b$ grows faster than others, so $(n + a)^b = \\Theta(n^b)$.", 
            "title": "3.1-2"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/#31-3", 
            "text": "The O-notation denotes a upper bound, so we can not say \"at least\".", 
            "title": "3.1-3"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/#31-4", 
            "text": "$2^{n + 1} = O(2^n)$, because there exist positive constants c = 2 and $n_0 = 1$ such that $0 \\leq 2^{n + 1} \\leq 2 * 2^n\\ for\\ all\\ n \\geq n_0$.  $2^{2n} \\neq O(2^n)$. Suppose $2^{2n} = O(2^n)$, then there exist positive constants c and $n_0$ such that $0 \\leq 2^{2n} \\leq c2^n\\ for\\ all\\ n \\geq n_0$, which means $0 \\leq 2^n \\leq c\\ for\\ all\\ n \\geq n_0$. No matter how big c is, $2^n$ will be bigger than c after a specific $n_1$, so you cannot find a $n_0$ like that.", 
            "title": "3.1-4"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/#31-5", 
            "text": "If $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq f(n) \\leq c_1g(n)\\ for\\ all\\ n \\geq n_1$, and there exist positive constants $c_2$ and $n_2$ such that $0 \\leq c_2g(n) \\leq f(n)\\ for\\ all\\ n \\geq n_2$. We can choose $n_0 = max(n_1, n_2)$, and combine the two inequations we have $0 \\leq c_2g(n) \\leq f(n) \\leq c_1g(n)\\ for\\ all\\ n \\geq n_0$, which is the definition for $f(n) = \\Theta(g(n))$. So we proved if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$, then $f(n) = \\Theta(g(n))$.  Now lets' suppose $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$ is not true (either one of them is wrong or both are wrong), but $f(n) = \\Theta(g(n))$ is ture. And let's prove this hypothesis is not correct.  If we have $f(n) = \\Theta(g(n))$, we know there exist positive constants c and $n_0$ such that $0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n)\\ for\\ all\\ n \\geq n_0$. So we can separate it into two inequations:  $$0 \\leq f(n) \\leq c_2g(n)\\ for\\ all\\ n \\geq n_0$$\n$$0 \\leq c_1g(n) \\leq f(n)\\ for\\ all\\ n \\geq n_0$$  which are the definitions of $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$. So we proved the hypothesis is wrong, thus we proved the \"only if\" part.", 
            "title": "3.1-5"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/#31-6", 
            "text": "It's similiar to 3.1-5, the worst-case and best-case running time denote the upper bound and lower bound, which are $O(g(n))$ and $\\Omega(g(n))$. So the proof is similar.", 
            "title": "3.1-6"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/#31-7", 
            "text": "Let's let $f(n) = o(g(n))$ and $h(n) = \\omega(g(n))$. According to the definition, for any positive constants $c_1$, $c_2$, there exist positive constants $n_1$, $n_2$ such that:  $$0 \\leq f(n)   c_1g(n)\\ for\\ all\\ n \\geq n_1$$\n$$0 \\leq c_2g(n)   h(n)\\ for\\ all\\ n \\geq n_2$$  Suppose $o(g(n)) \\cap \\omega(g(n))$ is not empty, so at least we have one f(n) and one h(n) such that f(n) = h(n), because $c_1$ and $c_2$ are any positive constants, so we let $c_1 = c_2 = c$,\nand let $n_0 = max(n_1, n_2)$, so we have:  $$0 \\leq f(n)   cg(n)\\ for\\ all\\ n \\geq n_0$$\n$$0 \\leq cg(n)   f(n)\\ for\\ all\\ n \\geq n_0$$  which also means:  $$0 \\leq cg(n)   f(n)\\   cg(n)\\ for\\ all\\ n \\geq n_0$$  And we know the above inequation is impossible. So the hypothesis is wrong, thus $o(g(n)) \\cap \\omega(g(n))$ is empty.", 
            "title": "3.1-7"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.1-Asymptotic-notation/#31-8", 
            "text": "$$\\Omega(g(n, m)) = \\lbrace f(n, m): \\text {there exist positive constants } c, n_0, \\text {and } m_0 \\text{ such that } 0 \\leq cg(n, m) \\leq f(n, m) \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0 \\rbrace$$  $$\\Theta(g(n, m)) = \\lbrace f(n, m): \\text {there exist positive constants } c_1, c_2, n_0, \\text {and } m_0 \\text{ such that } 0 \\leq c_1g(n, m) \\leq f(n, m) \\leq c_2g(n, m) \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0 \\rbrace$$", 
            "title": "3.1-8"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/", 
            "text": "3.2 Standard notations and common functions\n\n\n3.2-1\n\n\nIf f(n) and g(n) are monotonically increasing functions, then if $m \\leq n$, we have $f(m) \\leq f(n)$ and $g(m) \\leq g(n)$. So we get $f(m) + g(m) \\leq f(n) + g(n)$ when $m \\leq n$. So the function f(n) + g(n) is monotonically increasing.\n\n\nLet $m_1 = g(m)$ and $n_1 = g(n)$. Because $m_1 \\leq n_1$, so $f(m_1) \\leq f(n_1)$, so f(g(n)) is monotonically increasing.\n\n\nif f(n) and g(n) are nonnegative, because $f(m) \\leq f(n)$ and $g(m) \\leq g(n)$, so we can multiply the inequations and have $f(m) \\cdot g(m) \\leq f(n) \\cdot g(n)$. Thus $f(n) \\cdot g(n)$ is monotonically increasing.\n\n\n3.2-2\n\n\n$a^{\\log_bc} = a^{\\frac{\\log_ac}{\\log_ab}} = (a^{\\log_ac})^\\frac{1}{\\log_ab} = c^\\frac{1}{\\log_ab} = c^{\\log_ba}$.\n\n\n3.2-3\n\n\nAccording to \nStirling's approximation\n, we have:\n\n\n$$\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n} \\leq n! \\leq en^{n + \\frac{1}{2}}e^{-n}$$\n\n\nSo:\n\n\n$$\\lg{(\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n})} \\leq \\lg{(n!)} \\leq \\lg{(en^{n + \\frac{1}{2}}e^{-n})}$$\n\n\nNotice that:\n\n\n$$\n\\begin{eqnarray}\n\\lg{(\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n})} \n=\n \\lg{(\\sqrt{2\\pi})} + \\lg{n^{n + \\frac{1}{2}}} + \\lg{e^{-n}} \\\\\n\n=\n \\lg{(\\sqrt{2\\pi})} + (n + \\frac{1}{2})\\lg{n} - n\\lg{e} \\\\\n\n=\n n(\\lg{n} - \\lg{e}) + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\\n\n\\geq\n n(\\lg{n} - \\lg{\\sqrt{n}}) + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\text{ (when } n \\geq e^2 \\text{)} \\\\\n\n=\n n\\lg{\\frac{n}{\\sqrt{n}}} + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\\n\n=\n \\frac{1}{2}n\\lg{n} + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\\n\n\\geq\n \\frac{1}{2}n\\lg{n}\n\\end{eqnarray}\n$$\n\n\nAnd:\n\n\n$$\n\\begin{eqnarray}\n\\lg{(en^{n + \\frac{1}{2}}e^{-n})} \n=\n \\lg{e} + \\lg{n^{n + \\frac{1}{2}}} + \\lg{e^{-n}} \\\\\n\n=\n \\lg{e} + (n + \\frac{1}{2})\\lg{n} + \\lg{e^{-n}} \\\\\n\n=\n n\\lg{n} + \\lg{e} + \\frac{1}{2}\\lg{n} + \\lg{e^{-n}} \\\\\n\n=\n n\\lg{n} + \\lg{\\frac{e\\sqrt{n}}{e^n}} \\\\\n\n\\leq\n n\\lg{n} + \\lg{1} \\text{ (when } n \\geq 1 \\text{)} \\\\\n\n=\n n\\lg{n}\n\\end{eqnarray}\n$$\n\n\nSo now we get:\n\n\n$$\\frac{1}{2}n\\lg{n} \\leq \\lg{(n!)} \\leq n\\lg{n} \\text{ (when } n \\geq e^2 \\text{)}$$\n\n\nwhich means: there exist positive constants $c_1 = \\frac{1}{2}$, $c_2 = 1$ and $n_0 = 8$ such that:\n\n\n$$0 \\leq c_1n\\lg{n} \\leq \\lg(n!) \\leq c_2n\\lg{n},\\ for \\ all \\ n \\geq n_0$$\n\n\nSo $\\lg{(n!)} = \\Theta(n\\lg{n})$.\n\n\nWhen $n \\geq 4$, we have:\n\n\n$$\n\\begin{eqnarray}\nn! \n=\n n * (n - 1) * \\ldots * 4 * 3 * 2 * 1 \\\\\n\n=\n n * (n - 1) * \\ldots * 2 * 2 * 3 * 2 * 1 \\\\\n\n 2 * 2 * \\ldots * 2 * 2 * 3 * 2 * 1 \\\\\n\n=\n  2^n * 1 \\\\\n\n=\n 2^n\n\\end{eqnarray}\n$$\n\n\nSo for any positive constant $c_1$, there exist positive constant $n_1 = 4$, such that $0 \\leq c_1(2^n) \n n! \\ for\\ all\\ n \\geq n_1$.\n\n\nSo $n! = w(2^n)$.\n\n\nWhen $n \\geq 2$, we have:\n\n\n$$\n\\begin{eqnarray}\nn! \n=\n n * (n - 1) * \\ldots * 2 * 1 \\\\\n\n n * n * \\ldots * n * n \\\\\n\n=\n  n^n\n\\end{eqnarray}\n$$\n\n\nSo for any positive constant $c_2$, there exist positive constant $n_2 = 2$, such that $0 \\leq n! \n c_2n^n \\ for\\ all\\ n \\geq n_2$.\n\n\nSo $n! = o(n^n)$.\n\n\n3.2-4\n\n\nSuppose $\\lceil \\lg{n} \\rceil!$ is polynomially bounded, then there exist positive constants c, k and $n_0$ such that $0 \\leq \\lceil \\lg{n} \\rceil! \\leq cn^k$ for all $n \\geq n_0$. So $\\lg{(\\lceil \\lg{n} \\rceil!)} \\leq \\lg{(cn^k)}$.\n\n\nBut:\n\n\n$$\n\\begin{eqnarray}\n\\lg{(\\lceil \\lg{n} \\rceil!)} - \\lg{(cn^k)} \n\\geq\n \\lg{((\\lg{n})!)} - \\lg{(cn^k)} \\\\\n\n\\geq\n \\frac{1}{2}\\lg{n} * \\lg{(\\lg{n})} - \\lg{(cn^k)} \\text{ (reuse the inequation in 3.2-3)} \\\\\n\n=\n \\frac{1}{2}\\lg{n} * \\lg{(\\lg{n})} - \\lg{c} - k\\lg{n} \\\\\n\n=\n \\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c}\n\\end{eqnarray}\n$$\n\n\nSince both k and c are constants and $\\lg{n}$ is a monotonically increasing function, so $\\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c}$ is monotonically increasing. First we let $(\\frac{1}{2}\\lg{(\\lg{n})} - k) \\geq 1$ and we get $n \\geq 2^{2^{2k + 2}}$. So $\\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c} \\geq \\lg{n} - \\lg{c}$ when $n \\geq 2^{2^{2k + 2}}$. Then let $\\lg{n} - \\lg{c} \\geq 0$ and we get $n \\geq c$. So we find a $n_0 = max(2^{2^{2k + 2}}, c)$ such that $\\lg{(\\lceil \\lg{n} \\rceil!)} - \\lg{(cn^k)} \\geq 0 \\text{ for all } n \\geq n_0$.\n\n\nSo we know that the hypothesis is not correct, thus $\\lceil \\lg{n} \\rceil!$ is not polynomially bounded.\n\n\nNow let's prove that $\\lceil \\lg{\\lg{n}} \\rceil!$ is polynomially bounded.\n\n\n$$\n\\begin{eqnarray}\n\\lg{(\\lceil \\lg{\\lg{n}} \\rceil!)} - \\lg{(cn^k)} \n\\leq\n \\lceil \\lg{\\lg{n}} \\rceil\\lg{(\\lceil \\lg{\\lg{n}} \\rceil)} - \\lg{(cn^k)} \\text{ (reuse the inequation in 3.2-3)} \\\\\n\n=\n \\lceil \\lg{\\lg{n}} \\rceil\\lg{(\\lceil \\lg{\\lg{n}} \\rceil)} - \\lg{c} - k\\lg{n} \\\\\n\n (\\lg{\\lg{n}} + 1)\\lg{(\\lg{\\lg{n}} + 1)} - \\lg{c} - k\\lg{n} \\\\\n\\end{eqnarray}\n$$\n\n\nLet $\\lg{n} = x, x \n 0$, so $(\\lg{\\lg{n}} + 1)\\lg{(\\lg{\\lg{n}} + 1)} - \\lg{c} - k\\lg{n} = (\\lg{x} + 1)\\lg{(\\lg{x} + 1)} - \\lg{c} - kx = f(x)$.\n\n\nSo $f'(x) = \\frac{\\lg{(\\lg{x} + 1)} + 1}{x} - k$.\n\n\nLet $g(x) = \\frac{\\lg{(\\lg{x} + 1)} + 1}{x}$, so $g'(x) = \\frac{\\frac{1}{\\lg{x} + 1} - (\\lg{(\\lg{x} + 1)} + 1)}{x^2}$. It's easy to see that g'(x) is a monotonically decreasing function and g'(1) = 0, so $g(x) \\leq 0 \\text{ on } [1, +\\infty)$. So g(x) is monotonically decreasing on $[1, +\\infty)$ and g(1) = 1.\n\n\nSo $f'(x) \\leq 1 - k \\text{ on } [1, +\\infty)$. And for some constants k, $f'(x) \\leq 0$. Thus f(x) is also a monotonically decreasing function on $[1, +\\infty)$ for some constants k. And $f(1) = -\\lg{c} - k \n 0$, so f(x) \n 0 on $[1, +\\infty)$.\n\n\nSo there exist positive constants c, k and $n_0 = 2$ such that $\\lg{(\\lceil \\lg{\\lg{n}} \\rceil!)} - \\lg{(cn^k)} \n f(x) \n 0 \\text{ for all } n \\geq n_0$. So $\\lceil \\lg{\\lg{n}} \\rceil! \\leq cn^k$. So $\\lceil \\lg{\\lg{n}} \\rceil!$ is polynomially bounded.\n\n\n3.2-5\n\n\nHere\n is the definition of $\\lg^*{n}$. So $\\lg{(\\lg^*{n})} = \\lg{(1 + \\lg^*{(\\lg{n})})}$. Let $\\lg^*{(\\lg{n})} = x$, so it's obvious $f(x) = \\lg{(1 + x)}$ grows slower than $g(x) = x$, so $\\lg^*{(\\lg{n})}$ is asymptotically larger.\n\n\n3.2-6\n\n\n$$\n\\begin{eqnarray}\n\\phi^2 - \\phi - 1 \n=\n (\\frac{1 + \\sqrt{5}}{2})^2 - \\frac{1 + \\sqrt{5}}{2} - 1 \\\\\n\n=\n \\frac{1 + 2\\sqrt{5} + 5}{4} - \\frac{2 + 2\\sqrt{5}}{4} - \\frac{4}{4} \\\\\n\n=\n \\frac{6 + 2\\sqrt{5} - 2 - 2\\sqrt{5} - 4}{4} \\\\\n\n=\n 0\n\\end{eqnarray}\n$$\n\n\n$$\n\\begin{eqnarray}\n\\hat\\phi^2 - \\hat\\phi - 1 \n=\n (\\frac{1 - \\sqrt{5}}{2})^2 - \\frac{1 - \\sqrt{5}}{2} - 1 \\\\\n\n=\n \\frac{1 - 2\\sqrt{5} + 5}{4} - \\frac{2 - 2\\sqrt{5}}{4} - \\frac{4}{4} \\\\\n\n=\n \\frac{6 - 2\\sqrt{5} - 2 + 2\\sqrt{5} - 4}{4} \\\\\n\n=\n 0\n\\end{eqnarray}\n$$\n\n\nSo the golden ratio $\\phi$ and its conjugate $\\hat\\phi$ both satisfy the equation $x^2 = x + 1$.\n\n\n3.2-7\n\n\nBasis\n: let's show that the statement holds for i = 1 and i = 2.\n\n\n$\\frac{\\phi - \\hat\\phi}{\\sqrt{5}} = \\frac{\\frac{1 + \\sqrt{5}}{2} - \\frac{1 - \\sqrt{5}}{2}}{\\sqrt{5}} = \\frac{\\frac{1 + \\sqrt{5} - 1 + \\sqrt{5}}{2}}{\\sqrt{5}} = \\frac{\\frac{2\\sqrt{5}}{2}}{\\sqrt{5}} = 1 = F_1$\n\n\n$\\frac{\\phi^2 - \\hat\\phi^2}{\\sqrt{5}} = \\frac{(\\frac{1 + \\sqrt{5}}{2})^2 - (\\frac{1 - \\sqrt{5}}{2})^2}{\\sqrt{5}} = \\frac{\\frac{1 + 2\\sqrt{5} + 5 - 1 + 2\\sqrt{5} - 5}{4}}{\\sqrt{5}} = \\frac{\\frac{4\\sqrt{5}}{4}}{\\sqrt{5}} = 1 = F_2$\n\n\nInductive step\n: if $F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}}$ holds, let's show $F_{i + 1} = \\frac{\\phi^{i + 1} - \\hat\\phi^{i + 1}}{\\sqrt{5}}$ holds.\n\n\n$$\n\\begin{eqnarray}\nF_{i + 1} \n=\n F_i + F_{i - 1} \\\\\n\n=\n \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}} + \\frac{\\phi^{i - 1} - \\hat\\phi^{i - 1}}{\\sqrt{5}} \\\\\n\n=\n \\frac{\\phi^{i - 1}(\\phi + 1) - \\hat\\phi^{i - 1}(\\hat\\phi + 1)}{\\sqrt{5}} \\\\\n\n=\n \\frac{\\phi^{i - 1}\\phi^2 - \\hat\\phi^{i - 1}\\hat\\phi^2}{\\sqrt{5}} \\text{( reuse the equation in 3.2-7)} \\\\\n\n=\n \\frac{\\phi^{i + 1} - \\hat\\phi^{i + 1}}{\\sqrt{5}}\n\\end{eqnarray}\n$$\n\n\nSo $F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}}$ holds for all i.\n\n\n3.2-8\n\n\nAccording to the symmetry property, if $k\\ln{k} = \\Theta(n)$, then $n = \\Theta(k\\ln{k})$. So there exist positive constants $c_1$, $c_2$ and $n_0$ such that:\n\n\n$$0 \\leq c_1k\\ln{k} \\leq n \\leq c_2k\\ln{k} \\text{, for all } n \\geq n_0 \\text{ (1)}$$\n\n\nSo:\n\n\n$$0 \\leq \\ln{(c_1k\\ln{k})} \\leq \\ln{n} \\leq \\ln{(c_2k\\ln{k})}$$\n\n\n$$0 \\leq \\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})} \\leq \\ln{n} \\leq \\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})} \\text{ (2)}$$\n\n\nLet's divide equation 1 by equation 2, so we get:\n\n\n$$0 \\leq \\frac{c_1k\\ln{k}}{\\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})}} \\leq \\frac{n}{\\ln{n}} \\leq \\frac{c_2k\\ln{k}}{\\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})}}$$\n\n\nBecause:\n\n\n$$\\frac{n}{\\ln{n}} \\geq \\frac{c_1k\\ln{k}}{\\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})}} \\geq \\frac{c_1k\\ln{k}}{\\ln{k} + \\ln{k} + \\ln{k}} = \\frac{c_1}{3}k$$\n\n\n$$\\frac{n}{\\ln{n}} \\leq \\frac{c_2k\\ln{k}}{\\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})}} \\leq \\frac{c_2k\\ln{k}}{\\ln{k}} = c_2k$$\n\n\nSo we have $\\frac{c_1}{3}k \\leq \\frac{n}{\\ln{n}} \\leq c_2k$.\n\n\nSo there exist positive constants $c_3 = \\frac{c_1}{3}$, $c_4 = c_2$ and $n_1 = n_0$ such that:\n\n\n$$c_3k \\leq \\frac{n}{\\ln{n}} \\leq c_4k \\text{ for all } n \\geq n_1$$\n\n\nSo $\\frac{n}{\\ln{n}} = \\Theta(k)$, which also means $k = \\Theta(\\frac{n}{\\ln{n}})$.", 
            "title": "3.2 Standard notations and common functions"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-standard-notations-and-common-functions", 
            "text": "", 
            "title": "3.2 Standard notations and common functions"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-1", 
            "text": "If f(n) and g(n) are monotonically increasing functions, then if $m \\leq n$, we have $f(m) \\leq f(n)$ and $g(m) \\leq g(n)$. So we get $f(m) + g(m) \\leq f(n) + g(n)$ when $m \\leq n$. So the function f(n) + g(n) is monotonically increasing.  Let $m_1 = g(m)$ and $n_1 = g(n)$. Because $m_1 \\leq n_1$, so $f(m_1) \\leq f(n_1)$, so f(g(n)) is monotonically increasing.  if f(n) and g(n) are nonnegative, because $f(m) \\leq f(n)$ and $g(m) \\leq g(n)$, so we can multiply the inequations and have $f(m) \\cdot g(m) \\leq f(n) \\cdot g(n)$. Thus $f(n) \\cdot g(n)$ is monotonically increasing.", 
            "title": "3.2-1"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-2", 
            "text": "$a^{\\log_bc} = a^{\\frac{\\log_ac}{\\log_ab}} = (a^{\\log_ac})^\\frac{1}{\\log_ab} = c^\\frac{1}{\\log_ab} = c^{\\log_ba}$.", 
            "title": "3.2-2"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-3", 
            "text": "According to  Stirling's approximation , we have:  $$\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n} \\leq n! \\leq en^{n + \\frac{1}{2}}e^{-n}$$  So:  $$\\lg{(\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n})} \\leq \\lg{(n!)} \\leq \\lg{(en^{n + \\frac{1}{2}}e^{-n})}$$  Notice that:  $$\n\\begin{eqnarray}\n\\lg{(\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n})}  =  \\lg{(\\sqrt{2\\pi})} + \\lg{n^{n + \\frac{1}{2}}} + \\lg{e^{-n}} \\\\ =  \\lg{(\\sqrt{2\\pi})} + (n + \\frac{1}{2})\\lg{n} - n\\lg{e} \\\\ =  n(\\lg{n} - \\lg{e}) + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\ \\geq  n(\\lg{n} - \\lg{\\sqrt{n}}) + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\text{ (when } n \\geq e^2 \\text{)} \\\\ =  n\\lg{\\frac{n}{\\sqrt{n}}} + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\ =  \\frac{1}{2}n\\lg{n} + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\ \\geq  \\frac{1}{2}n\\lg{n}\n\\end{eqnarray}\n$$  And:  $$\n\\begin{eqnarray}\n\\lg{(en^{n + \\frac{1}{2}}e^{-n})}  =  \\lg{e} + \\lg{n^{n + \\frac{1}{2}}} + \\lg{e^{-n}} \\\\ =  \\lg{e} + (n + \\frac{1}{2})\\lg{n} + \\lg{e^{-n}} \\\\ =  n\\lg{n} + \\lg{e} + \\frac{1}{2}\\lg{n} + \\lg{e^{-n}} \\\\ =  n\\lg{n} + \\lg{\\frac{e\\sqrt{n}}{e^n}} \\\\ \\leq  n\\lg{n} + \\lg{1} \\text{ (when } n \\geq 1 \\text{)} \\\\ =  n\\lg{n}\n\\end{eqnarray}\n$$  So now we get:  $$\\frac{1}{2}n\\lg{n} \\leq \\lg{(n!)} \\leq n\\lg{n} \\text{ (when } n \\geq e^2 \\text{)}$$  which means: there exist positive constants $c_1 = \\frac{1}{2}$, $c_2 = 1$ and $n_0 = 8$ such that:  $$0 \\leq c_1n\\lg{n} \\leq \\lg(n!) \\leq c_2n\\lg{n},\\ for \\ all \\ n \\geq n_0$$  So $\\lg{(n!)} = \\Theta(n\\lg{n})$.  When $n \\geq 4$, we have:  $$\n\\begin{eqnarray}\nn!  =  n * (n - 1) * \\ldots * 4 * 3 * 2 * 1 \\\\ =  n * (n - 1) * \\ldots * 2 * 2 * 3 * 2 * 1 \\\\  2 * 2 * \\ldots * 2 * 2 * 3 * 2 * 1 \\\\ =   2^n * 1 \\\\ =  2^n\n\\end{eqnarray}\n$$  So for any positive constant $c_1$, there exist positive constant $n_1 = 4$, such that $0 \\leq c_1(2^n)   n! \\ for\\ all\\ n \\geq n_1$.  So $n! = w(2^n)$.  When $n \\geq 2$, we have:  $$\n\\begin{eqnarray}\nn!  =  n * (n - 1) * \\ldots * 2 * 1 \\\\  n * n * \\ldots * n * n \\\\ =   n^n\n\\end{eqnarray}\n$$  So for any positive constant $c_2$, there exist positive constant $n_2 = 2$, such that $0 \\leq n!   c_2n^n \\ for\\ all\\ n \\geq n_2$.  So $n! = o(n^n)$.", 
            "title": "3.2-3"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-4", 
            "text": "Suppose $\\lceil \\lg{n} \\rceil!$ is polynomially bounded, then there exist positive constants c, k and $n_0$ such that $0 \\leq \\lceil \\lg{n} \\rceil! \\leq cn^k$ for all $n \\geq n_0$. So $\\lg{(\\lceil \\lg{n} \\rceil!)} \\leq \\lg{(cn^k)}$.  But:  $$\n\\begin{eqnarray}\n\\lg{(\\lceil \\lg{n} \\rceil!)} - \\lg{(cn^k)}  \\geq  \\lg{((\\lg{n})!)} - \\lg{(cn^k)} \\\\ \\geq  \\frac{1}{2}\\lg{n} * \\lg{(\\lg{n})} - \\lg{(cn^k)} \\text{ (reuse the inequation in 3.2-3)} \\\\ =  \\frac{1}{2}\\lg{n} * \\lg{(\\lg{n})} - \\lg{c} - k\\lg{n} \\\\ =  \\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c}\n\\end{eqnarray}\n$$  Since both k and c are constants and $\\lg{n}$ is a monotonically increasing function, so $\\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c}$ is monotonically increasing. First we let $(\\frac{1}{2}\\lg{(\\lg{n})} - k) \\geq 1$ and we get $n \\geq 2^{2^{2k + 2}}$. So $\\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c} \\geq \\lg{n} - \\lg{c}$ when $n \\geq 2^{2^{2k + 2}}$. Then let $\\lg{n} - \\lg{c} \\geq 0$ and we get $n \\geq c$. So we find a $n_0 = max(2^{2^{2k + 2}}, c)$ such that $\\lg{(\\lceil \\lg{n} \\rceil!)} - \\lg{(cn^k)} \\geq 0 \\text{ for all } n \\geq n_0$.  So we know that the hypothesis is not correct, thus $\\lceil \\lg{n} \\rceil!$ is not polynomially bounded.  Now let's prove that $\\lceil \\lg{\\lg{n}} \\rceil!$ is polynomially bounded.  $$\n\\begin{eqnarray}\n\\lg{(\\lceil \\lg{\\lg{n}} \\rceil!)} - \\lg{(cn^k)}  \\leq  \\lceil \\lg{\\lg{n}} \\rceil\\lg{(\\lceil \\lg{\\lg{n}} \\rceil)} - \\lg{(cn^k)} \\text{ (reuse the inequation in 3.2-3)} \\\\ =  \\lceil \\lg{\\lg{n}} \\rceil\\lg{(\\lceil \\lg{\\lg{n}} \\rceil)} - \\lg{c} - k\\lg{n} \\\\  (\\lg{\\lg{n}} + 1)\\lg{(\\lg{\\lg{n}} + 1)} - \\lg{c} - k\\lg{n} \\\\\n\\end{eqnarray}\n$$  Let $\\lg{n} = x, x   0$, so $(\\lg{\\lg{n}} + 1)\\lg{(\\lg{\\lg{n}} + 1)} - \\lg{c} - k\\lg{n} = (\\lg{x} + 1)\\lg{(\\lg{x} + 1)} - \\lg{c} - kx = f(x)$.  So $f'(x) = \\frac{\\lg{(\\lg{x} + 1)} + 1}{x} - k$.  Let $g(x) = \\frac{\\lg{(\\lg{x} + 1)} + 1}{x}$, so $g'(x) = \\frac{\\frac{1}{\\lg{x} + 1} - (\\lg{(\\lg{x} + 1)} + 1)}{x^2}$. It's easy to see that g'(x) is a monotonically decreasing function and g'(1) = 0, so $g(x) \\leq 0 \\text{ on } [1, +\\infty)$. So g(x) is monotonically decreasing on $[1, +\\infty)$ and g(1) = 1.  So $f'(x) \\leq 1 - k \\text{ on } [1, +\\infty)$. And for some constants k, $f'(x) \\leq 0$. Thus f(x) is also a monotonically decreasing function on $[1, +\\infty)$ for some constants k. And $f(1) = -\\lg{c} - k   0$, so f(x)   0 on $[1, +\\infty)$.  So there exist positive constants c, k and $n_0 = 2$ such that $\\lg{(\\lceil \\lg{\\lg{n}} \\rceil!)} - \\lg{(cn^k)}   f(x)   0 \\text{ for all } n \\geq n_0$. So $\\lceil \\lg{\\lg{n}} \\rceil! \\leq cn^k$. So $\\lceil \\lg{\\lg{n}} \\rceil!$ is polynomially bounded.", 
            "title": "3.2-4"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-5", 
            "text": "Here  is the definition of $\\lg^*{n}$. So $\\lg{(\\lg^*{n})} = \\lg{(1 + \\lg^*{(\\lg{n})})}$. Let $\\lg^*{(\\lg{n})} = x$, so it's obvious $f(x) = \\lg{(1 + x)}$ grows slower than $g(x) = x$, so $\\lg^*{(\\lg{n})}$ is asymptotically larger.", 
            "title": "3.2-5"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-6", 
            "text": "$$\n\\begin{eqnarray}\n\\phi^2 - \\phi - 1  =  (\\frac{1 + \\sqrt{5}}{2})^2 - \\frac{1 + \\sqrt{5}}{2} - 1 \\\\ =  \\frac{1 + 2\\sqrt{5} + 5}{4} - \\frac{2 + 2\\sqrt{5}}{4} - \\frac{4}{4} \\\\ =  \\frac{6 + 2\\sqrt{5} - 2 - 2\\sqrt{5} - 4}{4} \\\\ =  0\n\\end{eqnarray}\n$$  $$\n\\begin{eqnarray}\n\\hat\\phi^2 - \\hat\\phi - 1  =  (\\frac{1 - \\sqrt{5}}{2})^2 - \\frac{1 - \\sqrt{5}}{2} - 1 \\\\ =  \\frac{1 - 2\\sqrt{5} + 5}{4} - \\frac{2 - 2\\sqrt{5}}{4} - \\frac{4}{4} \\\\ =  \\frac{6 - 2\\sqrt{5} - 2 + 2\\sqrt{5} - 4}{4} \\\\ =  0\n\\end{eqnarray}\n$$  So the golden ratio $\\phi$ and its conjugate $\\hat\\phi$ both satisfy the equation $x^2 = x + 1$.", 
            "title": "3.2-6"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-7", 
            "text": "Basis : let's show that the statement holds for i = 1 and i = 2.  $\\frac{\\phi - \\hat\\phi}{\\sqrt{5}} = \\frac{\\frac{1 + \\sqrt{5}}{2} - \\frac{1 - \\sqrt{5}}{2}}{\\sqrt{5}} = \\frac{\\frac{1 + \\sqrt{5} - 1 + \\sqrt{5}}{2}}{\\sqrt{5}} = \\frac{\\frac{2\\sqrt{5}}{2}}{\\sqrt{5}} = 1 = F_1$  $\\frac{\\phi^2 - \\hat\\phi^2}{\\sqrt{5}} = \\frac{(\\frac{1 + \\sqrt{5}}{2})^2 - (\\frac{1 - \\sqrt{5}}{2})^2}{\\sqrt{5}} = \\frac{\\frac{1 + 2\\sqrt{5} + 5 - 1 + 2\\sqrt{5} - 5}{4}}{\\sqrt{5}} = \\frac{\\frac{4\\sqrt{5}}{4}}{\\sqrt{5}} = 1 = F_2$  Inductive step : if $F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}}$ holds, let's show $F_{i + 1} = \\frac{\\phi^{i + 1} - \\hat\\phi^{i + 1}}{\\sqrt{5}}$ holds.  $$\n\\begin{eqnarray}\nF_{i + 1}  =  F_i + F_{i - 1} \\\\ =  \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}} + \\frac{\\phi^{i - 1} - \\hat\\phi^{i - 1}}{\\sqrt{5}} \\\\ =  \\frac{\\phi^{i - 1}(\\phi + 1) - \\hat\\phi^{i - 1}(\\hat\\phi + 1)}{\\sqrt{5}} \\\\ =  \\frac{\\phi^{i - 1}\\phi^2 - \\hat\\phi^{i - 1}\\hat\\phi^2}{\\sqrt{5}} \\text{( reuse the equation in 3.2-7)} \\\\ =  \\frac{\\phi^{i + 1} - \\hat\\phi^{i + 1}}{\\sqrt{5}}\n\\end{eqnarray}\n$$  So $F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}}$ holds for all i.", 
            "title": "3.2-7"
        }, 
        {
            "location": "/3-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-8", 
            "text": "According to the symmetry property, if $k\\ln{k} = \\Theta(n)$, then $n = \\Theta(k\\ln{k})$. So there exist positive constants $c_1$, $c_2$ and $n_0$ such that:  $$0 \\leq c_1k\\ln{k} \\leq n \\leq c_2k\\ln{k} \\text{, for all } n \\geq n_0 \\text{ (1)}$$  So:  $$0 \\leq \\ln{(c_1k\\ln{k})} \\leq \\ln{n} \\leq \\ln{(c_2k\\ln{k})}$$  $$0 \\leq \\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})} \\leq \\ln{n} \\leq \\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})} \\text{ (2)}$$  Let's divide equation 1 by equation 2, so we get:  $$0 \\leq \\frac{c_1k\\ln{k}}{\\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})}} \\leq \\frac{n}{\\ln{n}} \\leq \\frac{c_2k\\ln{k}}{\\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})}}$$  Because:  $$\\frac{n}{\\ln{n}} \\geq \\frac{c_1k\\ln{k}}{\\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})}} \\geq \\frac{c_1k\\ln{k}}{\\ln{k} + \\ln{k} + \\ln{k}} = \\frac{c_1}{3}k$$  $$\\frac{n}{\\ln{n}} \\leq \\frac{c_2k\\ln{k}}{\\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})}} \\leq \\frac{c_2k\\ln{k}}{\\ln{k}} = c_2k$$  So we have $\\frac{c_1}{3}k \\leq \\frac{n}{\\ln{n}} \\leq c_2k$.  So there exist positive constants $c_3 = \\frac{c_1}{3}$, $c_4 = c_2$ and $n_1 = n_0$ such that:  $$c_3k \\leq \\frac{n}{\\ln{n}} \\leq c_4k \\text{ for all } n \\geq n_1$$  So $\\frac{n}{\\ln{n}} = \\Theta(k)$, which also means $k = \\Theta(\\frac{n}{\\ln{n}})$.", 
            "title": "3.2-8"
        }, 
        {
            "location": "/3-Growth-of-Functions/Problems/", 
            "text": "Problems\n\n\n3-1\n\n\nLet $a_{max} = max(a_0, a_1, \\ldots, a_d)$. Because $a_d \n 0$, so $a_{max} \n 0$. Now let's prove there exists a constant $n_1$ such that $p(n) \\geq 0 \\text{ for all } n \\geq n_1$.\n\n\nLet $a_{absmax} = max(abs(a_0), abs(a_1), \\ldots, abs(a_{d - 1}))$. So $p(n) \\geq a_dn^d + \\sum_{i = 0}^{d - 1} (-a_{absmax}n^{d - 1}) = a_dn^d -da_{absmax}n^{d - 1} = n^{d - 1}(a_dn - da_{absmax})$. So $p(n) \\geq 0$ when $n \\geq \\lceil\\frac{da_{absmax}}{a_d}\\rceil$, $n_1 = \\lceil\\frac{da_{absmax}}{a_d}\\rceil$.\n\n\na\n\n\nIf $k \\geq d$, then $p(n) \\leq \\sum_{i = 0}^d a_{max}n^d = (d + 1)a_{max}n^d \\leq (d + 1)a_{max}n^k$.\n\n\nSo there exist positive constants $c = (d + 1)a_{max}$ and $n_0 = max(1, n_1)$ such that $0 \\leq p(n) \\leq cn^k \\text{ for all } n \\geq n_0$. Thus $p(n) = O(n^k)$.\n\n\nb\n\n\nWe know $p(n) \\geq n^{d - 1}(a_dn - da_{absmax})$. So $n^{d - 1}(a_dn - da_{absmax}) \\geq n^d$ when $n \\geq \\frac{da_{absmax}}{a_d - 1}$. Thus $p(n) \\geq n^d \\geq n^k$ when $n \\geq max(1, \\lceil \\frac{da_{absmax}}{a_d - 1} \\rceil)$.\n\n\nLet $n_2 = max(1, \\lceil \\frac{da_{absmax}}{a_d - 1} \\rceil)$, so there exist positive constants c = 1 and $n_0 = max(n_1, n_2)$ such that $0 \\leq cn^k \\leq p(n) \\text{ for all } n \\geq n_0$. So $p(n) = \\Omega(n^k)$.\n\n\nc\n\n\nLet $n_3 = (n_0 \\text{ in question a})$ and $n_4 = (n_0 \\text{ in question b})$. From the questions a and b we know there exist positive constants $c_1 = 1$, $c_2 = (d + 1)a_{max}$ and $n_0 = max(n_3, n_4)$ such that $0 \\leq c_1n^d \\leq p(n) \\leq c_2n^d \\text{ for all } n \\geq n_0$. Because k = d, so this also holds true for k, so $p(n) = \\Theta(n^k)$.\n\n\nd\n\n\nFrom question a we know $p(n) \\leq (d + 1)a_{max}n^d$, because k \n d, let $(d + 1)a_{max}n^d \n cn^k$, then we have $n \n (\\frac{(d + 1)a_{max}}{c})^{\\frac{1}{k - d}}$. So for any positive constant c, we can find a positive constant $n_0 = \\lceil (\\frac{(d + 1)a_{max}}{c})^{\\frac{1}{k - d}} \\rceil + 1$ such that $0 \\leq p(n) \n cn^k \\text{ for all } n \\geq n_0$. So $p(n) = o(n^k)$.\n\n\ne\n\n\nFrom question b we know $p(n) \\geq n^{d - 1}(a_dn - da_{absmax})$, let $f(n) = n^{d - 1}(a_dn - da_{absmax}) - cn^k = n^k(n^{d - k}(a_d - \\frac{da_{absmax}}{n}) - c)$, because k \n d, so it's obvious that f(n) is a  monotonically increasing function. First let $a_d - \\frac{da_{absmax}}{n} \n \\frac{a_d}{2}$ and we get $n \n \\frac{2da_{absmax}}{a_d}$. So $f(n) \n n^k(n^{d - k}\\frac{a_d}{2} - c) \\text{ for all } n \n= \\lceil \\frac{2da_{absmax}}{a_d} \\rceil + 1$. Then let $n^{d - k}\\frac{a_d}{2} - c \n 0$ and we have $n \n (\\frac{2c}{a_d})^{\\frac{1}{d - k}}$. So for any given positive constant c we can find a positive constant $n_0 = max(\\lceil \\frac{2da_{absmax}}{a_d} \\rceil + 1, \\lceil (\\frac{2c}{a_d})^{\\frac{1}{d - k}} \\rceil + 1)$ such that $0 \\leq cn^k \n p(n) \\text{ for all } n \\geq n_0$. So $p(n) = w(n^k)$.\n\n\n3-2\n\n\na\n\n\nNote that $n = 2^{\\lg{n}}$. So $\\lg^k{n} = (2^{\\lg{\\lg{n}}})^k = 2^{k\\lg{\\lg{n}}}$, $n^\\epsilon = (2^{\\lg{n}})^{\\epsilon} = 2^{\\epsilon\\lg{n}}$. It's obvious that $\\epsilon\\lg{n}$ grows faster than\n$k\\lg{\\lg{n}}$. Let $\\lg{n} = x, x \n 0$, so $k\\lg{\\lg{n}} = k\\lg{x}$, $\\epsilon\\lg{n} = \\epsilon{x}$. Let $f(x) = \\epsilon{x} - k\\lg{x}$, so $f'(x) = \\epsilon - \\frac{k}{x}$. Because $\\epsilon \n 0$ and $k \\geq 1$, we\nhave $f'(x) \n= 0 \\text{ when } x \\geq \\frac{k}{\\epsilon}$. So f(x) is a monotonically increasing function when $x \\geq \\frac{k}{\\epsilon}$.\n\n\nIn order to solve $\\epsilon{x} - k\\lg{x} \n 0$, we only have to solve $\\frac{x}{\\lg{x}} \n \\frac{k}{\\epsilon}$, since $\\lim_{x \\to +\\infty} \\frac{x}{\\lg{x}} = +\\infty$, so there exists a constant $x_0$ such that $\\frac{x_0}{\\lg{x_0}} \n \\frac{k}{\\epsilon}$, so $f(x_0) \n 0$.\n\n\nSo we can find a constant $x_0$ such that $f(x) \\geq 0 \\text{ for all } x \\geq x_0$. Thus $\\epsilon\\lg{n} \\geq k\\lg{\\lg{n}} \\text{ for all } n \\geq 2^{x_0}$. Therefore we proved there exist positive constants c = 1 and $n_0 = 2^{x_0}$\nsuch that $0 \\leq \\lg^k{n} \\leq n^{\\epsilon} \\text{ for all } n \\geq n_0$. So $\\lg^k{n} = O(n^{\\epsilon})$.\n\n\nNow let's compare $\\lg^k{n}$ and $cn^{\\epsilon}$. Similarly, $cn^{\\epsilon} = 2^{\\lg{c}}(2^{\\lg{n}})^{\\epsilon} = 2^{\\lg{c} + \\epsilon\\lg{n}}$. So let $\\lg{n} = x$, so $\\lg{c} + \\epsilon\\lg{n} - k\\lg{\\lg{n}} = \\lg{c} + \\epsilon{x} - k\\lg{x}$.\nLet $g(x) = \\epsilon{x} - k\\lg{x} + \\lg{c}$. And $g'(x) = \\epsilon - \\frac{k}{x}$, $g'(x) \n= 0 \\text{ when } x \\geq \\frac{k}{\\epsilon}$. So g(x) is a monotonically increasing function when $x \\geq \\frac{k}{\\epsilon}$.\n\n\nIn order to solve $g(x) \n 0$, we need to solve $\\epsilon{x} - k\\lg{x} \n -\\lg{c}$. Namely, for any given positive constant c, we need to find a $x_0$ such that $g(x_0)$ is greater than $-\\lg{c}$.\nNotice that $\\epsilon{x} - k\\lg{x} = f(x)$ and f(x) is a a monotonically increasing function. For a given constant, we can find a $x_0$ such that $f(x_0)$ is greater than that constant.\n\n\nSo, for any given constant c, we can find a $x_0$ such that $g(x_0) \n 0$. Thus, for any positive constant c, there exists positive constant $n_0 = 2^{x_0}$ such that $0 \\leq \\lg^k{n} \n cn^{\\epsilon} \\text{ for all } n \\geq n_0$. So $\\lg^k{n} = o(n^{\\epsilon})$.\n\n\nSince $\\lg^k{n} = o(n^{\\epsilon})$, then $\\lg^k{n}$ could not be $\\Omega(n^{\\epsilon})$, $w(n^{\\epsilon})$, $\\Theta(n^{\\epsilon})$.\n\n\nb\n\n\n$n^k = (2^{\\lg{n}})^k = 2^{k\\lg{n}}$, $c^n = (2^{\\lg{c}})^n = 2^{n\\lg{c}}$. So it's also obvious that $n\\lg{c}$ grows faster than $k\\lg{n}$. Let $f(n) = n\\lg{c} - k\\lg{n}$. Because $c \n 1$, so $\\lg{c} \n 0$. Thus we\nhave the same function $f(x)$ defined in question a. So similarly, we know $n^k = O(c^n)$.\n\n\nNow let's compare $n^k$ and $bc^n$. $bc^n = 2^{\\lg{b}}(2^{\\lg{c}})^n = 2^{\\lg{b} + n\\lg{c}}$. Let $g(n) = \\lg{b} + n\\lg{c} - k\\lg{n}$, and again we have the same function g(x) defined in question a.\nSo $n^k = o(c^n)$.\n\n\nc\n\n\nLet's compare $\\sqrt{n}$ and $cn^{\\sin{n}}$. $\\sqrt{n} = (2^{\\lg{n}})^{\\frac{1}{2}} = 2^{\\frac{1}{2}\\lg{n}}$, $cn^{\\sin{n}} = 2^{\\lg{c}}(2^{\\lg{n}})^{\\sin{n}} = 2^{\\sin{n}\\lg{n} + \\lg{c}}$. Let $f(n) = \\sin{n}\\lg{n} + \\lg{c} - \\frac{1}{2}\\lg{n} = (\\sin{n} - \\frac{1}{2})\\lg{n} + \\lg{c}$. So the question is: does there exist a positive constant c (or for any positive constant c), there exists a positive constant $n_0$, such that f(n) \n 0 (or f(n) \n 0) for all $n \\geq n_0$?\n\n\nFirst let's check $f(n) \n 0$, nomatter how big c is, we can find a $n_1$ such that $\\lg{n_1} \n \\lg{c}$, since $-\\frac{3}{2} \\leq \\sin{n} - \\frac{1}{2} \\leq \\frac{1}{2}$. So there exists a $n_2 \\geq n_1$ such that $\\sin{n_2} - \\frac{1}{2} = -1$, so $f(n_2) \n 0$. So for any given constant c, there doesn't exist a constant $n_0$ such that f(n) \n 0 for all $n \\geq n_0$.\n\n\nSimilarly, for $f(n) \n 0$, no matter how small c is, we can find a $n_1$ such that $\\lg{n_1} \n 2abs(\\lg{c})$, and there exists a $n_2 \\geq n_1$ such that $\\sin{n_2} - \\frac{1}{2} = \\frac{1}{2}$, so $f(n_2) \n 0$. So for any given constant c, there doesn't exist a constant $n_0$ such that f(n) \n 0 for all $n \\geq n_0$.\n\n\nThus, we cannot compare which grows faster.\n\n\nd\n\n\nIt's obvious that there exist a positive constant c = 1 and $n_0 = 1$ such that $0 \\leq c2^{\\frac{n}{2}} \\leq 2^n \\text{ for all } n \\geq n_0$. So $2^n = \\Omega(2^{\\frac{n}{2}})$.\n\n\nNow let's compare $2^n$ and $c2^{\\frac{n}{2}}$. $c2^{\\frac{n}{2}} = 2^{\\lg{c}}2^{\\frac{n}{2}} = 2^{\\frac{n}{2} + \\lg{c}}$. Let $f(n) = n - (\\frac{n}{2} + \\lg{c}) = \\frac{n}{2} - \\lg{c}$. Let $f(n) \n 0$, we have $n \n 2\\lg{c}$. So for any constant c, there exists a positive constant $n_0 = 2\\lg{c} + 1$ such that $2^n \n c2^{\\frac{n}{2}} \\text{ for all } n \n= n_0$. So $2^n = w(2^{\\frac{n}{2}})$.\n\n\ne\n\n\n$n^{\\lg{c}} = (2^{\\lg{n}})^{\\lg{c}} = 2^{\\lg{c}\\lg{n}}$, $c^{\\lg{n}} = (2^{\\lg{c}})^{\\lg{n}} = 2^{\\lg{c}\\lg{n}}$, so $n^{\\lg{c}} = c^{\\lg{n}}$, thus there exist positive constants $b_1 = 1$ and $n_1 = 1$ such that $0 \\leq n^{\\lg{c}} \\leq b_1c^{\\lg{n}} \\text{ for all } n \\geq n_1$, and there exist positive constants $b_2 = 1$ and $n_2 = 1$ such that $0 \\leq b_2c^{\\lg{n}} \\leq n^{\\lg{c}} \\text{ for all } n \\geq n_2$, and there exist positive constants $b_3 = 1$, $b_4 = 1$ and $n_3 = 1$ such that $0 \\leq b_3c^{\\lg{n}} \\leq n^{\\lg{c}} \\leq b_4c^{\\lg{n}} \\text{ for all } n \\geq n_3$. So $n^{\\lg{c}} = O(c^{\\lg{n}})$, $n^{\\lg{c}} = \\Omega(c^{\\lg{n}})$, $n^{\\lg{c}} = \\Theta(c^{\\lg{n}})$.\n\n\nSince $n^{\\lg{c}}$ and $c^{\\lg{n}}$ are the same functions, so for any positive constant b, we cannot find a positive constant $n_0$ such that $n^{\\lg{c}} \n bc^{\\lg{n}}$ or $n^{\\lg{c}} \n bc^{\\lg{n}}$ for all $n \\geq n_0$.\n\n\nf\n\n\nIn question 3.2-3, we've already proved that $\\lg{(n!)} = \\Theta(n\\lg{n})$, notice that $\\lg(n^n) = n\\lg{n}$, so $\\lg{(n!)} = \\Theta(\\lg{n^n})$. And $\\lg{(n!)} = O(\\lg{n^n})$, $\\lg{(n!)} = \\Omega(\\lg{n^n})$.\n\n\nSummary\n\n\n\n\n\n\n\n\nA\n\n\nB\n\n\nO\n\n\no\n\n\n$\\Omega$\n\n\nw\n\n\n$\\Theta$\n\n\n\n\n\n\n\n\n\n\n$\\lg^k{n}$\n\n\n$n^{\\epsilon}$\n\n\nyes\n\n\nyes\n\n\nno\n\n\nno\n\n\nno\n\n\n\n\n\n\n$n^k$\n\n\n$c^n$\n\n\nyes\n\n\nyes\n\n\nno\n\n\nno\n\n\nno\n\n\n\n\n\n\n$\\sqrt{n}$\n\n\n$n^{\\sin{n}}$\n\n\nno\n\n\nno\n\n\nno\n\n\nno\n\n\nno\n\n\n\n\n\n\n$2^n$\n\n\n$2^{\\frac{n}{2}}$\n\n\nno\n\n\nno\n\n\nyes\n\n\nyes\n\n\nno\n\n\n\n\n\n\n$n^{\\lg{c}}$\n\n\n$c^{\\lg{n}}$\n\n\nyes\n\n\nno\n\n\nyes\n\n\nno\n\n\nyes\n\n\n\n\n\n\n$\\lg(n!)$\n\n\n$\\lg(n^n)$\n\n\nyes\n\n\nno\n\n\nyes\n\n\nno\n\n\nyes\n\n\n\n\n\n\n\n\n3-3\n\n\na\n\n\nFirst, let's compare $2^{2^{n + 1}}$ and $2^{2^n}$, it's easy to see $2^{2^n} = O(2^{2^{n + 1}})$. But could $2^{2^n} = \\Omega(2^{2^{n + 1}})$? If it's true, then there exist a positive constant c and $n_0$ such that $0 \\leq c2^{2^{n + 1}} \\leq 2^{2^n}$ for all $n \\geq n_0$. But $\\frac{2^{2^n}}{c2^{2^{n + 1}}} = \\frac{1}{c2^{2^{n + 1} - 2^n}} = \\frac{1}{c2^{2^n}}$. No matter how small c is, $2^{2^n}$ will be greater than $\\frac{1}{c}$. So $2^{2^n} = \\Omega(2^{2^{n + 1}})$ could not be true.\n\n\nThen let's compare $n!$ and $e^n$. We know $n! = 2^{\\lg{(n!)}}$, and $e^n = (2^{\\lg{e}})^n = 2^{n\\lg{e}}$. In question 3.2-3 we know $\\lg{(n!)} = \\Theta(n\\lg{n})$, so $\\lg{(n!)}$ grows faster than $n\\lg{e}$. So $n!$ grows faster than $e^n$, thus $n! = \\Omega(e^n)$.\n\n\nAnd it's obvious $(n + 1)! = \\Omega(n!)$. Notice that $\\frac{n!}{c(n + 1)!} = \\frac{1}{c(n + 1)}$, which will eventually smaller than 1. So $n!$ could not be $\\Omega((n + 1)!)$.\n\n\nAnd what about $(n + 1)!$ and $2^{2^n}$? $(n + 1)! = 2^{\\lg{(n + 1)!}} = 2^{\\Theta((n + 1)\\lg{(n + 1)})} = 2^{O((n + 1)^2)} = 2^{O(n^2)}$. So $2^{2^{n}}$ grows faster, $2^{2^n} = \\Omega((n + 1)!)$.\n\n\nSince $e^n = (\\frac{e}{2})^n2^n$, and $(\\frac{e}{2})^n$ grows faster than $n$, so $e^n$ grows faster than $n2^n$, $e^n = \\Omega(n2^n)$.\n\n\nAnd it's easy to see that $n2^n = \\Omega(2^n)$, $2^n = \\Omega(\\frac{3}{2})^n$.\n\n\n$(\\lg{n})^{\\lg{n}} = (2^{\\lg{\\lg{n}}})^{\\lg{n}} = 2^{\\lg{n}\\lg{\\lg{n}}}$. And $n^{\\lg{\\lg{n}}} = (2^{\\lg{n}})^{\\lg{\\lg{n}}} = 2^{\\lg{n}\\lg{\\lg{n}}}$. So $(\\lg{n})^{\\lg{n}} = n^{\\lg{\\lg{n}}}$. And $(\\frac{3}{2})^n = (2^{\\lg{\\frac{3}{2}}})^n = 2^{n\\lg{\\frac{3}{2}}}$. Let $\\lg{n} = k$, so $\\lg{n}\\lg{\\lg{n}} = k\\lg{k}$, $n\\lg{\\frac{3}{2}} = 2^k\\lg{\\frac{3}{2}}$, so we can see $2^k\\lg{\\frac{3}{2}}$ grows faster. Thus $(\\frac{3}{2})^n = \\Omega((\\lg{n})^{\\lg{n}})$.\n\n\nAccording to \nStirling's approximation\n, we know $n! = \\Theta(n^{n + \\frac{1}{2}}e^{-n})$. So $(\\lg{n})! = \\Theta((\\lg{n})^{\\lg{n} + \\frac{1}{2}}e^{-\\lg{n}}) = \\Theta((\\lg{n})^{\\lg{n}}\\frac{\\sqrt{\\lg{n}}}{e^{\\lg{n}}}) = O((\\lg{n})^{\\lg{n}})$, since $e^x$ grows much faster than $\\sqrt{x}$. So $(\\lg{n})^{\\lg{n}} = \\Omega((\\lg{n})!)$.\n\n\nLet $\\lg{n} = x$, so $n = 2^x$, $n^3 = 2^{3x}$. $(\\lg{n})! = \\Theta((\\lg{n})^{\\lg{n}}\\frac{\\sqrt{\\lg{n}}}{e^{\\lg{n}}}) = \\Theta(x^{x + \\frac{1}{2}}e^{-x}) = \\Theta(e^{x(\\ln{x} - 1) + \\frac{1}{2}\\ln{x}})$. And $2^{3x} = e^{(3\\ln{2})x}$. So $(\\lg{n})!$ grows faster than $n^3$.\n\n\n$4^{\\lg{n}} = (2^2)^{\\lg{n}} = (2^{\\lg{n}})^2 = n^2$, so $n^2 = 4^{\\lg{n}}$. And it's obvious $n^3 = \\Omega(n^2)$, $n^2 = \\Omega(n\\lg{n})$. And in question 3.2-3 we already proved $\\lg({n!}) = \\Theta(n\\lg{n})$.\n\n\nAnd it's easy to know $n\\lg{n} = \\Omega(n)$, $n = 2^{\\lg{n}}$.\n\n\nSimilarly, $(\\sqrt{2})^{\\lg{n}} = (2^{\\lg{n}})^{\\frac{1}{2}} = \\sqrt{n}$, so $n = \\Omega({(\\sqrt{2})^{\\lg{n}}})$.\n\n\n$\\sqrt{n} = n^{\\frac{1}{2}} = (2^{\\lg{n}})^{\\frac{1}{2}} = 2^{\\frac{1}{2}\\lg{n}}$, it grows faster than $2^{\\sqrt{2\\lg{n}}}$, so $(\\sqrt{2})^{\\lg{n}} = \\Omega(2^{\\sqrt{2\\lg{n}}})$.\n\n\n$\\lg^2{n} = (2^{\\lg{\\lg{n}}})^2 = 2^{2\\lg{\\lg{n}}}$, in question 3-2 we know $\\lg^k{n} = o(n^{\\epsilon})$ for $k \\geq 1$ and $\\epsilon \n 0$, so here we have $k = 1$, $\\epsilon = \\frac{1}{2}$, so $\\sqrt{2\\lg{n}}$ grows faster than $2\\lg{\\lg{n}}$, so $2^{\\sqrt{2\\lg{n}}} = \\Omega(\\lg^2{n})$.\n\n\nAnd $\\lg^2{n} = \\Omega(\\ln{n})$, $\\ln{n} = \\Omega(\\sqrt{\\lg{n}})$.\n\n\nThen let's compare $\\sqrt{\\lg{n}}$ and $\\ln{\\ln{n}}$. $\\sqrt{\\lg{n}} = \\sqrt{\\frac{\\ln{n}}{\\ln{2}}}$, from previous proof, we know $\\sqrt{\\frac{\\ln{n}}{\\ln{2}}}$ grows faster than $\\ln{\\ln{n}}$. So $\\sqrt{\\lg{n}} = \\Omega(\\ln{\\ln{n}})$.\n\n\n$\\ln{\\ln{n}} = 2^{\\lg{\\ln{\\ln{n}}}}$, and according to the definition of \nIterated logarithm\n, we can see $\\lg{\\ln{\\ln{n}}}$ grows faster than $\\lg^*{n}$. So $\\ln{\\ln{n}} = \\Omega(2^{\\lg^*{n}})$. And $2^{\\lg^*{n}} = \\Omega(\\lg^*{n})$.\n\n\nSince $\\lg^*{n} = 1 + \\lg^*{\\lg{n}}$, so $\\lg^*{\\lg{n}} = \\Theta(\\lg^*{n})$.\n\n\nIn question 3.2-5 we proved that $\\lg^*{(\\lg{n})}$ is asymptotically larger than $\\lg{(\\lg^*{n})}$. So $\\lg^*{(\\lg{n})} = \\Omega(\\lg{(\\lg^*{n})})$.\n\n\n$n^{\\frac{1}{\\lg{n}}} = (2^{\\lg{n}})^{\\frac{1}{\\lg{n}}} = 2$, so $n^{\\frac{1}{\\lg{n}}} = \\Theta(1)$.\n\n\nSummary\n\n\n\n\n$g_1 = 2^{2^{n + 1}}$\n\n\n$g_2 = 2^{2^n}$\n\n\n$g_3 = (n + 1)!$\n\n\n$g_4 = n!$\n\n\n$g_5 = e^n$\n\n\n$g_6 = n2^n$\n\n\n$g_7 = 2^n$\n\n\n$g_8 = (\\frac{3}{2})^n$\n\n\n$g_9 = (\\lg{n})^{\\lg{n}}$\n\n\n$g_{10} = n^{\\lg{\\lg{n}}}, g_9 = \\Theta(g_{10})$\n\n\n$g_{11} = (\\lg{n})!$\n\n\n$g_{12} = n^3$\n\n\n$g_{13} = n^2$\n\n\n$g_{14} = 4^{\\lg{n}}, g_{13} = \\Theta(g_{14})$\n\n\n$g_{15} = n\\lg{n}$\n\n\n$g_{16} = \\lg({n!}), g_{15} = \\Theta(g_{16})$\n\n\n$g_{17} = n$\n\n\n$g_{18} = 2^{\\lg{n}}, g_{17} = \\Theta(g_{18})$\n\n\n$g_{19} = (\\sqrt{2})^{\\lg{n}}$\n\n\n$g_{20} = 2^{\\sqrt{2\\lg{n}}}$\n\n\n$g_{21} = \\lg^2{n}$\n\n\n$g_{22} = \\ln{n}$\n\n\n$g_{23} = \\sqrt{\\lg{n}}$\n\n\n$g_{24} = \\ln{\\ln{n}}$\n\n\n$g_{25} = 2^{\\lg^*{n}}$\n\n\n$g_{26} = \\lg^*{n}$\n\n\n$g_{27} = \\lg^*{\\lg{n}}, g_{26} = \\Theta(g_{27})$\n\n\n$g_{28} = \\lg{(\\lg^*{n})}$\n\n\n$g_{29} = n^{\\frac{1}{\\lg{n}}}$\n\n\n$g_{30} = 1, g_{29} = \\Theta(g_{30})$\n\n\n\n\nb\n\n\nHow do we find a function like this? In question 3-2, we know that $n^{\\sin{n}}$ is neither $o(\\sqrt{n})$ nor $w(\\sqrt{n})$. We can use the feature of $\\sin{n}$. So we can build a function that can be quite big and quite small. And it has to be not smaller than the largest function in $g_i(n)$ sometimes, so we can simply have a function like $n2^{2^{n + 1}}$, which is $\\Omega(g_i(n))$. Then we multiply $|\\sin{n}|$, so the function is $|\\sin{n}|n2^{2^{n + 1}}$, which is neither $O(g_i(n))$ nor $\\Omega(g_i(n))$.\n\n\n3-4\n\n\na\n\n\nThis is not true. If $g(n) = O(f(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq g(n) \\leq c_1f(n) \\text{ for all } n \\geq n_1$, since $f(n) = O(g(n))$, so there also exist positive constants $c_2$ and $n_2$ such that $0 \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_2$, so we have $0 \\leq \\frac{1}{c_1}g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq max(n_1, n_2)$, which means $f(n) = \\Theta(g(n))$. This is not 100% true.\n\n\nb\n\n\nThis is not true. Let $f(n) = n$ and $g(n) = \\lg{n}$, it's easy to see $f(n) + g(n) = \\Theta(n) \\neq \\Theta(min(f(n), g(n)))$.\n\n\nc\n\n\nThis is true. Because $f(n) = O(g(n))$, so there exist positive constants c and $n_0$ such that $0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0$. So $0\\leq \\lg{(f(n))} \\leq \\lg{(cg(n))} = \\lg{c} + \\lg{(g(n))} = \\lg{c} * 1 + \\lg{(g(n))} \\leq \\lg{c} * \\lg{(g(n))} + \\lg{(g(n))} = (\\lg{c} + 1)\\lg{(g(n))}$. So we find a positive constant $c_1 = \\lg{c} + 1$ such that $0 \\leq \\lg{(f(n))} \\leq c_1\\lg{(g(n))} \\text{ for all } n \\geq n_0$, so $\\lg{(f(n))} = O(\\lg{(g(n))}$.\n\n\nd\n\n\nThis is not true. If $g(n) = O(f(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq g(n) \\leq c_1f(n) \\text{ for all } n \\geq n_1$. Suppose $2^{f(n)} = O(2^{g(n)})$, then there exist positive constants $c_2$ and $n_2$ such that $0 \\leq 2^{f(n)} \\leq {c_2}2^{g(n)} \\text{ for all } n \\geq n_2$. And ${c_2}2^{g(n)} = 2^{\\lg{c_2} + g(n)}$. So we have $f(n) \\leq \\lg{c_2} + g(n)$. But this is not always true, let $f(n) = n + \\lg{n}$ and $g(n) = n$, so $f(n) \\leq 2g(n) \\text{ for all } n \\geq 1$. But for any given positive constant $c_2$, we can always find a positive constant $n_3$ such that $n + \\lg{n} \n \\lg{c_2} + n \\text{ for all } n \\geq n_3$. So $2^{f(n)} \\neq O(2^{g(n)})$ in this situation.\n\n\ne\n\n\nThis is not true. Let $f(n) = \\frac{1}{n}$, if $f(n) = O((f(n))^2)$, then there exist positive constants c and $n_0$ such that $0 \\leq \\frac{1}{n} \\leq \\frac{c}{n^2} \\text{ for all } n \\geq n_0$. But $\\frac{c}{n^2} - \\frac{1}{n} = \\frac{c - n}{n^2} \\leq 0 \\text{ for all } n \\geq c$. So we cannot find such $n_0$ for any positive constant $c$.\n\n\nf\n\n\nThis is true. If $f(n) = O(g(n))$, then there exist positive constants c and $n_0$ such that $0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0$, so we have $0 \\leq \\frac{1}{c}f(n) \\leq g(n) \\text{ for all } n \\geq n_0$, which is the definition of $g(n) = \\Omega(f(n))$.\n\n\ng\n\n\nThis is not true. Let $f(n) = 2^{2n}$, so $f(\\frac{n}{2}) = 2^n$. So it's obvious $f(n) \\neq \\Theta(f(\\frac{n}{2}))$.\n\n\nh\n\n\nThis is true. Suppose $f(n) + o(f(n)) = \\Theta(f(n))$, then we need to prove that there exist positive constatns $c_1$, $c_2$ and $n_0$ such that $0 \\leq c_1f(n) \\leq f(n) + o(f(n)) \\leq c_2f(n)$. It's easy to find $c_1 = 1$ since $o(f(n)) \\geq 0$. And according to the definition of $o(f(n))$, we know for any positive constant c there exists positive constant $n_1$ such that $0 \\leq o(f(n)) \n cf(n) \\text{ for all } n \\geq n_1$. so we can choose $c = 1$, so $f(n) + o(f(n)) \\leq f(n) + f(n) = 2f(n)$, so we have $c_2 = 2$ and $n_0 = n_1$. So $f(n) + o(f(n)) = \\Theta(f(n))$.\n\n\n3-5\n\n\na\n\n\nSuppose both $f(n) = O(g(n))$ and $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ are not true. If $f(n) = O(g(n))$ is not true, then there are two cases, first, we can not find any positive constant c such that $0 \\leq f(n) \\leq cg(n)$ for any integer n. It means for any positive constant c we have $f(n) \n cg(n)$. So it satisfies $f(n) \\geq cg(n) \\geq 0$ for infinitely many integers n. So it shows $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ is true, thus, the hypothesis is wrong. Second, we can find a constant c such that $0 \\leq f(n) \\leq cg(n)$ for some integers n. If the set of integers n is finite, then there is an infinite set such that $f(n) \\geq cg(n) \\geq 0$, so $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$. If the set is infinite but we cannot find a positive constant $n_0$ that satisfies $f(n) = O(g(n))$, there is also a infinite set such that $f(n) \n cg(n) \\geq 0$, so the hypothesis is wrong. So we proved either $f(n) = O(g(n))$ or $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ or both.\n\n\nIn problem 3-2 we proved both $\\sqrt{n} = O(n^{\\sin{n}})$ and $\\sqrt{n} = \\Omega(n^{\\sin{n}})$ are wrong. So it's not true if we use $\\Omega$ in place of $\\mathop{\\Omega}^{\\infty}$.\n\n\nb\n\n\nThe advantage is that we can describe the relationship of two functions when we cannot use $\\Omega$ notation. The disadvantage is that sometimes we cannot clearly know the running time of a function.\n\n\nc\n\n\nIf $f(n) = \\Theta(g(n))$, then we have $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$. If we have $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$, $f(n) = \\Theta(g(n))$ is also true. Since $f(n) = \\Omega(g(n))$ guarantees $f(n) \\geq 0$.\n\n\nd\n\n\n$\\tilde{\\Omega}(g(n)) = \\lbrace f(n): \\text{ there exist positive constants } c, \\text{ } k, \\text{ and } n_0 \\text{ such that } 0 \\leq cg(n)\\lg^k{n} \\leq f(n) \\text{ for all } n \\geq n_0 \\rbrace$.\n\n\n$\\tilde{\\Theta}(g(n)) = \\lbrace f(n): \\text{ there exist positive constants } c_1, \\text{ } c_2, \\text{ } k_1, \\text{ } k_2 \\text{ and } n_0 \\text{ such that } 0 \\leq c_1g(n)\\lg^{k_1}{n} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{n} \\text{ for all } n \\geq n_0 \\rbrace$.\n\n\nProve Theorem 3.1:\n\n\nIf $f(n) = \\tilde{\\Theta}(g(n))$, then there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, $n_0$ such that $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_0$.\n\n\nIt means: $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\text{ for all } n \\geq n_0$ and $0 \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_0$. They are the definition of $f(n) = \\tilde{O}(g(n))$ and $f(n) = \\tilde{\\Omega}(g(n))$.\n\n\nIf $f(n) = \\tilde{O}(g(n))$ and $f(n) = \\tilde{\\Omega}(g(n))$. Then there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, $n_1$, $n_2$ such that $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\text{ for all } n \\geq n_1$ and $0 \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_2$. Then we combine them together: $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq max(n_1, n_2)$. So $f(n) = \\tilde{\\Theta}(g(n))$.\n\n\n3-6\n\n\na\n\n\n$f^2(n) = n - 1 - 1 = n - 2$, $f^3(n) = n - 1 - 2 = n - 3$, so $f^k(n) = n - k$. It's easy to see $f^k(n) = 0$ after nth iterations.\n\n\nb\n\n\n$f^2(n) = \\lg{\\lg{n}}$, $f^3(n) = \\lg{\\lg{\\lg{n}}}$, so $f^k(n) = \\underbrace{\\lg{\\lg{\\ldots\\lg{n}}}}_\\text{k}$. Let $\\lg{\\lg{\\ldots\\lg{n}}} = 1$, so we have $f^{k - 1}(n) = 2$, and similiarly, $f^{k - 2}(n) = 2^2$, so $f^{k - (k - 1)}(n) = 2^{k - 1}$, so $k = \\lg{\\lg{n}} + 1$.\n\n\nc\n\n\n$f^2(n) = \\frac{n}{2^2}$, $f^k(n) = \\frac{n}{2^k}$, let $f^k(n) = \\frac{n}{2^k} = 1$, we have $k = \\lg{n}$.\n\n\nd\n\n\nLet $f^k(n) = \\frac{n}{2^k} = 2$, we have $k = \\lg{n} - 1$.\n\n\ne\n\n\n$f^2(n) = n^{\\frac{1}{2^2}}$, $f^k(n) = n^{\\frac{1}{2^k}}$, let $f^k(n) = n^{\\frac{1}{2^k}} = 2$, so $k = \\lg{\\lg{n}}$.\n\n\nf\n\n\nBecause $\\frac{1}{2^k}$ could not be 0, so $f^k(n) = n^{\\frac{1}{2^k}}$ could not be 1.\n\n\ng\n\n\n$f^2(n) = n^{\\frac{1}{3^2}}$, $f^k(n) = n^{\\frac{1}{3^k}}$, let $f^k(n) = n^{\\frac{1}{3^k}} = 2$, so $k = \\frac{\\lg{\\lg{n}}}{\\lg{3}}$.\n\n\nh\n\n\n?\n\n\nSummary\n\n\n\n\n\n\n\n\n$f(n)$\n\n\n$c$\n\n\n$f_c^*(n)$\n\n\n\n\n\n\n\n\n\n\n$n - 1$\n\n\n0\n\n\n$n$\n\n\n\n\n\n\n$\\lg{n}$\n\n\n1\n\n\n$\\lceil \\lg{\\lg{n}} + 1 \\rceil$\n\n\n\n\n\n\n$\\frac{n}{2}$\n\n\n1\n\n\n$\\lceil \\lg{n} \\rceil$\n\n\n\n\n\n\n$\\frac{n}{2}$\n\n\n2\n\n\n$\\lceil \\lg{n} - 1 \\rceil$\n\n\n\n\n\n\n$\\sqrt{n}$\n\n\n2\n\n\n$\\lceil \\lg{\\lg{n}} \\rceil$\n\n\n\n\n\n\n$\\sqrt{n}$\n\n\n1\n\n\n$+\\infty$\n\n\n\n\n\n\n$n^{\\frac{1}{3}}$\n\n\n2\n\n\n$\\lceil \\frac{\\lg{\\lg{n}}}{\\lg{3}} \\rceil$\n\n\n\n\n\n\n$\\frac{n}{\\lg{n}}$\n\n\n2\n\n\n?", 
            "title": "Problems"
        }, 
        {
            "location": "/3-Growth-of-Functions/Problems/#problems", 
            "text": "", 
            "title": "Problems"
        }, 
        {
            "location": "/3-Growth-of-Functions/Problems/#3-1", 
            "text": "Let $a_{max} = max(a_0, a_1, \\ldots, a_d)$. Because $a_d   0$, so $a_{max}   0$. Now let's prove there exists a constant $n_1$ such that $p(n) \\geq 0 \\text{ for all } n \\geq n_1$.  Let $a_{absmax} = max(abs(a_0), abs(a_1), \\ldots, abs(a_{d - 1}))$. So $p(n) \\geq a_dn^d + \\sum_{i = 0}^{d - 1} (-a_{absmax}n^{d - 1}) = a_dn^d -da_{absmax}n^{d - 1} = n^{d - 1}(a_dn - da_{absmax})$. So $p(n) \\geq 0$ when $n \\geq \\lceil\\frac{da_{absmax}}{a_d}\\rceil$, $n_1 = \\lceil\\frac{da_{absmax}}{a_d}\\rceil$.  a  If $k \\geq d$, then $p(n) \\leq \\sum_{i = 0}^d a_{max}n^d = (d + 1)a_{max}n^d \\leq (d + 1)a_{max}n^k$.  So there exist positive constants $c = (d + 1)a_{max}$ and $n_0 = max(1, n_1)$ such that $0 \\leq p(n) \\leq cn^k \\text{ for all } n \\geq n_0$. Thus $p(n) = O(n^k)$.  b  We know $p(n) \\geq n^{d - 1}(a_dn - da_{absmax})$. So $n^{d - 1}(a_dn - da_{absmax}) \\geq n^d$ when $n \\geq \\frac{da_{absmax}}{a_d - 1}$. Thus $p(n) \\geq n^d \\geq n^k$ when $n \\geq max(1, \\lceil \\frac{da_{absmax}}{a_d - 1} \\rceil)$.  Let $n_2 = max(1, \\lceil \\frac{da_{absmax}}{a_d - 1} \\rceil)$, so there exist positive constants c = 1 and $n_0 = max(n_1, n_2)$ such that $0 \\leq cn^k \\leq p(n) \\text{ for all } n \\geq n_0$. So $p(n) = \\Omega(n^k)$.  c  Let $n_3 = (n_0 \\text{ in question a})$ and $n_4 = (n_0 \\text{ in question b})$. From the questions a and b we know there exist positive constants $c_1 = 1$, $c_2 = (d + 1)a_{max}$ and $n_0 = max(n_3, n_4)$ such that $0 \\leq c_1n^d \\leq p(n) \\leq c_2n^d \\text{ for all } n \\geq n_0$. Because k = d, so this also holds true for k, so $p(n) = \\Theta(n^k)$.  d  From question a we know $p(n) \\leq (d + 1)a_{max}n^d$, because k   d, let $(d + 1)a_{max}n^d   cn^k$, then we have $n   (\\frac{(d + 1)a_{max}}{c})^{\\frac{1}{k - d}}$. So for any positive constant c, we can find a positive constant $n_0 = \\lceil (\\frac{(d + 1)a_{max}}{c})^{\\frac{1}{k - d}} \\rceil + 1$ such that $0 \\leq p(n)   cn^k \\text{ for all } n \\geq n_0$. So $p(n) = o(n^k)$.  e  From question b we know $p(n) \\geq n^{d - 1}(a_dn - da_{absmax})$, let $f(n) = n^{d - 1}(a_dn - da_{absmax}) - cn^k = n^k(n^{d - k}(a_d - \\frac{da_{absmax}}{n}) - c)$, because k   d, so it's obvious that f(n) is a  monotonically increasing function. First let $a_d - \\frac{da_{absmax}}{n}   \\frac{a_d}{2}$ and we get $n   \\frac{2da_{absmax}}{a_d}$. So $f(n)   n^k(n^{d - k}\\frac{a_d}{2} - c) \\text{ for all } n  = \\lceil \\frac{2da_{absmax}}{a_d} \\rceil + 1$. Then let $n^{d - k}\\frac{a_d}{2} - c   0$ and we have $n   (\\frac{2c}{a_d})^{\\frac{1}{d - k}}$. So for any given positive constant c we can find a positive constant $n_0 = max(\\lceil \\frac{2da_{absmax}}{a_d} \\rceil + 1, \\lceil (\\frac{2c}{a_d})^{\\frac{1}{d - k}} \\rceil + 1)$ such that $0 \\leq cn^k   p(n) \\text{ for all } n \\geq n_0$. So $p(n) = w(n^k)$.", 
            "title": "3-1"
        }, 
        {
            "location": "/3-Growth-of-Functions/Problems/#3-2", 
            "text": "a  Note that $n = 2^{\\lg{n}}$. So $\\lg^k{n} = (2^{\\lg{\\lg{n}}})^k = 2^{k\\lg{\\lg{n}}}$, $n^\\epsilon = (2^{\\lg{n}})^{\\epsilon} = 2^{\\epsilon\\lg{n}}$. It's obvious that $\\epsilon\\lg{n}$ grows faster than\n$k\\lg{\\lg{n}}$. Let $\\lg{n} = x, x   0$, so $k\\lg{\\lg{n}} = k\\lg{x}$, $\\epsilon\\lg{n} = \\epsilon{x}$. Let $f(x) = \\epsilon{x} - k\\lg{x}$, so $f'(x) = \\epsilon - \\frac{k}{x}$. Because $\\epsilon   0$ and $k \\geq 1$, we\nhave $f'(x)  = 0 \\text{ when } x \\geq \\frac{k}{\\epsilon}$. So f(x) is a monotonically increasing function when $x \\geq \\frac{k}{\\epsilon}$.  In order to solve $\\epsilon{x} - k\\lg{x}   0$, we only have to solve $\\frac{x}{\\lg{x}}   \\frac{k}{\\epsilon}$, since $\\lim_{x \\to +\\infty} \\frac{x}{\\lg{x}} = +\\infty$, so there exists a constant $x_0$ such that $\\frac{x_0}{\\lg{x_0}}   \\frac{k}{\\epsilon}$, so $f(x_0)   0$.  So we can find a constant $x_0$ such that $f(x) \\geq 0 \\text{ for all } x \\geq x_0$. Thus $\\epsilon\\lg{n} \\geq k\\lg{\\lg{n}} \\text{ for all } n \\geq 2^{x_0}$. Therefore we proved there exist positive constants c = 1 and $n_0 = 2^{x_0}$\nsuch that $0 \\leq \\lg^k{n} \\leq n^{\\epsilon} \\text{ for all } n \\geq n_0$. So $\\lg^k{n} = O(n^{\\epsilon})$.  Now let's compare $\\lg^k{n}$ and $cn^{\\epsilon}$. Similarly, $cn^{\\epsilon} = 2^{\\lg{c}}(2^{\\lg{n}})^{\\epsilon} = 2^{\\lg{c} + \\epsilon\\lg{n}}$. So let $\\lg{n} = x$, so $\\lg{c} + \\epsilon\\lg{n} - k\\lg{\\lg{n}} = \\lg{c} + \\epsilon{x} - k\\lg{x}$.\nLet $g(x) = \\epsilon{x} - k\\lg{x} + \\lg{c}$. And $g'(x) = \\epsilon - \\frac{k}{x}$, $g'(x)  = 0 \\text{ when } x \\geq \\frac{k}{\\epsilon}$. So g(x) is a monotonically increasing function when $x \\geq \\frac{k}{\\epsilon}$.  In order to solve $g(x)   0$, we need to solve $\\epsilon{x} - k\\lg{x}   -\\lg{c}$. Namely, for any given positive constant c, we need to find a $x_0$ such that $g(x_0)$ is greater than $-\\lg{c}$.\nNotice that $\\epsilon{x} - k\\lg{x} = f(x)$ and f(x) is a a monotonically increasing function. For a given constant, we can find a $x_0$ such that $f(x_0)$ is greater than that constant.  So, for any given constant c, we can find a $x_0$ such that $g(x_0)   0$. Thus, for any positive constant c, there exists positive constant $n_0 = 2^{x_0}$ such that $0 \\leq \\lg^k{n}   cn^{\\epsilon} \\text{ for all } n \\geq n_0$. So $\\lg^k{n} = o(n^{\\epsilon})$.  Since $\\lg^k{n} = o(n^{\\epsilon})$, then $\\lg^k{n}$ could not be $\\Omega(n^{\\epsilon})$, $w(n^{\\epsilon})$, $\\Theta(n^{\\epsilon})$.  b  $n^k = (2^{\\lg{n}})^k = 2^{k\\lg{n}}$, $c^n = (2^{\\lg{c}})^n = 2^{n\\lg{c}}$. So it's also obvious that $n\\lg{c}$ grows faster than $k\\lg{n}$. Let $f(n) = n\\lg{c} - k\\lg{n}$. Because $c   1$, so $\\lg{c}   0$. Thus we\nhave the same function $f(x)$ defined in question a. So similarly, we know $n^k = O(c^n)$.  Now let's compare $n^k$ and $bc^n$. $bc^n = 2^{\\lg{b}}(2^{\\lg{c}})^n = 2^{\\lg{b} + n\\lg{c}}$. Let $g(n) = \\lg{b} + n\\lg{c} - k\\lg{n}$, and again we have the same function g(x) defined in question a.\nSo $n^k = o(c^n)$.  c  Let's compare $\\sqrt{n}$ and $cn^{\\sin{n}}$. $\\sqrt{n} = (2^{\\lg{n}})^{\\frac{1}{2}} = 2^{\\frac{1}{2}\\lg{n}}$, $cn^{\\sin{n}} = 2^{\\lg{c}}(2^{\\lg{n}})^{\\sin{n}} = 2^{\\sin{n}\\lg{n} + \\lg{c}}$. Let $f(n) = \\sin{n}\\lg{n} + \\lg{c} - \\frac{1}{2}\\lg{n} = (\\sin{n} - \\frac{1}{2})\\lg{n} + \\lg{c}$. So the question is: does there exist a positive constant c (or for any positive constant c), there exists a positive constant $n_0$, such that f(n)   0 (or f(n)   0) for all $n \\geq n_0$?  First let's check $f(n)   0$, nomatter how big c is, we can find a $n_1$ such that $\\lg{n_1}   \\lg{c}$, since $-\\frac{3}{2} \\leq \\sin{n} - \\frac{1}{2} \\leq \\frac{1}{2}$. So there exists a $n_2 \\geq n_1$ such that $\\sin{n_2} - \\frac{1}{2} = -1$, so $f(n_2)   0$. So for any given constant c, there doesn't exist a constant $n_0$ such that f(n)   0 for all $n \\geq n_0$.  Similarly, for $f(n)   0$, no matter how small c is, we can find a $n_1$ such that $\\lg{n_1}   2abs(\\lg{c})$, and there exists a $n_2 \\geq n_1$ such that $\\sin{n_2} - \\frac{1}{2} = \\frac{1}{2}$, so $f(n_2)   0$. So for any given constant c, there doesn't exist a constant $n_0$ such that f(n)   0 for all $n \\geq n_0$.  Thus, we cannot compare which grows faster.  d  It's obvious that there exist a positive constant c = 1 and $n_0 = 1$ such that $0 \\leq c2^{\\frac{n}{2}} \\leq 2^n \\text{ for all } n \\geq n_0$. So $2^n = \\Omega(2^{\\frac{n}{2}})$.  Now let's compare $2^n$ and $c2^{\\frac{n}{2}}$. $c2^{\\frac{n}{2}} = 2^{\\lg{c}}2^{\\frac{n}{2}} = 2^{\\frac{n}{2} + \\lg{c}}$. Let $f(n) = n - (\\frac{n}{2} + \\lg{c}) = \\frac{n}{2} - \\lg{c}$. Let $f(n)   0$, we have $n   2\\lg{c}$. So for any constant c, there exists a positive constant $n_0 = 2\\lg{c} + 1$ such that $2^n   c2^{\\frac{n}{2}} \\text{ for all } n  = n_0$. So $2^n = w(2^{\\frac{n}{2}})$.  e  $n^{\\lg{c}} = (2^{\\lg{n}})^{\\lg{c}} = 2^{\\lg{c}\\lg{n}}$, $c^{\\lg{n}} = (2^{\\lg{c}})^{\\lg{n}} = 2^{\\lg{c}\\lg{n}}$, so $n^{\\lg{c}} = c^{\\lg{n}}$, thus there exist positive constants $b_1 = 1$ and $n_1 = 1$ such that $0 \\leq n^{\\lg{c}} \\leq b_1c^{\\lg{n}} \\text{ for all } n \\geq n_1$, and there exist positive constants $b_2 = 1$ and $n_2 = 1$ such that $0 \\leq b_2c^{\\lg{n}} \\leq n^{\\lg{c}} \\text{ for all } n \\geq n_2$, and there exist positive constants $b_3 = 1$, $b_4 = 1$ and $n_3 = 1$ such that $0 \\leq b_3c^{\\lg{n}} \\leq n^{\\lg{c}} \\leq b_4c^{\\lg{n}} \\text{ for all } n \\geq n_3$. So $n^{\\lg{c}} = O(c^{\\lg{n}})$, $n^{\\lg{c}} = \\Omega(c^{\\lg{n}})$, $n^{\\lg{c}} = \\Theta(c^{\\lg{n}})$.  Since $n^{\\lg{c}}$ and $c^{\\lg{n}}$ are the same functions, so for any positive constant b, we cannot find a positive constant $n_0$ such that $n^{\\lg{c}}   bc^{\\lg{n}}$ or $n^{\\lg{c}}   bc^{\\lg{n}}$ for all $n \\geq n_0$.  f  In question 3.2-3, we've already proved that $\\lg{(n!)} = \\Theta(n\\lg{n})$, notice that $\\lg(n^n) = n\\lg{n}$, so $\\lg{(n!)} = \\Theta(\\lg{n^n})$. And $\\lg{(n!)} = O(\\lg{n^n})$, $\\lg{(n!)} = \\Omega(\\lg{n^n})$.  Summary     A  B  O  o  $\\Omega$  w  $\\Theta$      $\\lg^k{n}$  $n^{\\epsilon}$  yes  yes  no  no  no    $n^k$  $c^n$  yes  yes  no  no  no    $\\sqrt{n}$  $n^{\\sin{n}}$  no  no  no  no  no    $2^n$  $2^{\\frac{n}{2}}$  no  no  yes  yes  no    $n^{\\lg{c}}$  $c^{\\lg{n}}$  yes  no  yes  no  yes    $\\lg(n!)$  $\\lg(n^n)$  yes  no  yes  no  yes", 
            "title": "3-2"
        }, 
        {
            "location": "/3-Growth-of-Functions/Problems/#3-3", 
            "text": "a  First, let's compare $2^{2^{n + 1}}$ and $2^{2^n}$, it's easy to see $2^{2^n} = O(2^{2^{n + 1}})$. But could $2^{2^n} = \\Omega(2^{2^{n + 1}})$? If it's true, then there exist a positive constant c and $n_0$ such that $0 \\leq c2^{2^{n + 1}} \\leq 2^{2^n}$ for all $n \\geq n_0$. But $\\frac{2^{2^n}}{c2^{2^{n + 1}}} = \\frac{1}{c2^{2^{n + 1} - 2^n}} = \\frac{1}{c2^{2^n}}$. No matter how small c is, $2^{2^n}$ will be greater than $\\frac{1}{c}$. So $2^{2^n} = \\Omega(2^{2^{n + 1}})$ could not be true.  Then let's compare $n!$ and $e^n$. We know $n! = 2^{\\lg{(n!)}}$, and $e^n = (2^{\\lg{e}})^n = 2^{n\\lg{e}}$. In question 3.2-3 we know $\\lg{(n!)} = \\Theta(n\\lg{n})$, so $\\lg{(n!)}$ grows faster than $n\\lg{e}$. So $n!$ grows faster than $e^n$, thus $n! = \\Omega(e^n)$.  And it's obvious $(n + 1)! = \\Omega(n!)$. Notice that $\\frac{n!}{c(n + 1)!} = \\frac{1}{c(n + 1)}$, which will eventually smaller than 1. So $n!$ could not be $\\Omega((n + 1)!)$.  And what about $(n + 1)!$ and $2^{2^n}$? $(n + 1)! = 2^{\\lg{(n + 1)!}} = 2^{\\Theta((n + 1)\\lg{(n + 1)})} = 2^{O((n + 1)^2)} = 2^{O(n^2)}$. So $2^{2^{n}}$ grows faster, $2^{2^n} = \\Omega((n + 1)!)$.  Since $e^n = (\\frac{e}{2})^n2^n$, and $(\\frac{e}{2})^n$ grows faster than $n$, so $e^n$ grows faster than $n2^n$, $e^n = \\Omega(n2^n)$.  And it's easy to see that $n2^n = \\Omega(2^n)$, $2^n = \\Omega(\\frac{3}{2})^n$.  $(\\lg{n})^{\\lg{n}} = (2^{\\lg{\\lg{n}}})^{\\lg{n}} = 2^{\\lg{n}\\lg{\\lg{n}}}$. And $n^{\\lg{\\lg{n}}} = (2^{\\lg{n}})^{\\lg{\\lg{n}}} = 2^{\\lg{n}\\lg{\\lg{n}}}$. So $(\\lg{n})^{\\lg{n}} = n^{\\lg{\\lg{n}}}$. And $(\\frac{3}{2})^n = (2^{\\lg{\\frac{3}{2}}})^n = 2^{n\\lg{\\frac{3}{2}}}$. Let $\\lg{n} = k$, so $\\lg{n}\\lg{\\lg{n}} = k\\lg{k}$, $n\\lg{\\frac{3}{2}} = 2^k\\lg{\\frac{3}{2}}$, so we can see $2^k\\lg{\\frac{3}{2}}$ grows faster. Thus $(\\frac{3}{2})^n = \\Omega((\\lg{n})^{\\lg{n}})$.  According to  Stirling's approximation , we know $n! = \\Theta(n^{n + \\frac{1}{2}}e^{-n})$. So $(\\lg{n})! = \\Theta((\\lg{n})^{\\lg{n} + \\frac{1}{2}}e^{-\\lg{n}}) = \\Theta((\\lg{n})^{\\lg{n}}\\frac{\\sqrt{\\lg{n}}}{e^{\\lg{n}}}) = O((\\lg{n})^{\\lg{n}})$, since $e^x$ grows much faster than $\\sqrt{x}$. So $(\\lg{n})^{\\lg{n}} = \\Omega((\\lg{n})!)$.  Let $\\lg{n} = x$, so $n = 2^x$, $n^3 = 2^{3x}$. $(\\lg{n})! = \\Theta((\\lg{n})^{\\lg{n}}\\frac{\\sqrt{\\lg{n}}}{e^{\\lg{n}}}) = \\Theta(x^{x + \\frac{1}{2}}e^{-x}) = \\Theta(e^{x(\\ln{x} - 1) + \\frac{1}{2}\\ln{x}})$. And $2^{3x} = e^{(3\\ln{2})x}$. So $(\\lg{n})!$ grows faster than $n^3$.  $4^{\\lg{n}} = (2^2)^{\\lg{n}} = (2^{\\lg{n}})^2 = n^2$, so $n^2 = 4^{\\lg{n}}$. And it's obvious $n^3 = \\Omega(n^2)$, $n^2 = \\Omega(n\\lg{n})$. And in question 3.2-3 we already proved $\\lg({n!}) = \\Theta(n\\lg{n})$.  And it's easy to know $n\\lg{n} = \\Omega(n)$, $n = 2^{\\lg{n}}$.  Similarly, $(\\sqrt{2})^{\\lg{n}} = (2^{\\lg{n}})^{\\frac{1}{2}} = \\sqrt{n}$, so $n = \\Omega({(\\sqrt{2})^{\\lg{n}}})$.  $\\sqrt{n} = n^{\\frac{1}{2}} = (2^{\\lg{n}})^{\\frac{1}{2}} = 2^{\\frac{1}{2}\\lg{n}}$, it grows faster than $2^{\\sqrt{2\\lg{n}}}$, so $(\\sqrt{2})^{\\lg{n}} = \\Omega(2^{\\sqrt{2\\lg{n}}})$.  $\\lg^2{n} = (2^{\\lg{\\lg{n}}})^2 = 2^{2\\lg{\\lg{n}}}$, in question 3-2 we know $\\lg^k{n} = o(n^{\\epsilon})$ for $k \\geq 1$ and $\\epsilon   0$, so here we have $k = 1$, $\\epsilon = \\frac{1}{2}$, so $\\sqrt{2\\lg{n}}$ grows faster than $2\\lg{\\lg{n}}$, so $2^{\\sqrt{2\\lg{n}}} = \\Omega(\\lg^2{n})$.  And $\\lg^2{n} = \\Omega(\\ln{n})$, $\\ln{n} = \\Omega(\\sqrt{\\lg{n}})$.  Then let's compare $\\sqrt{\\lg{n}}$ and $\\ln{\\ln{n}}$. $\\sqrt{\\lg{n}} = \\sqrt{\\frac{\\ln{n}}{\\ln{2}}}$, from previous proof, we know $\\sqrt{\\frac{\\ln{n}}{\\ln{2}}}$ grows faster than $\\ln{\\ln{n}}$. So $\\sqrt{\\lg{n}} = \\Omega(\\ln{\\ln{n}})$.  $\\ln{\\ln{n}} = 2^{\\lg{\\ln{\\ln{n}}}}$, and according to the definition of  Iterated logarithm , we can see $\\lg{\\ln{\\ln{n}}}$ grows faster than $\\lg^*{n}$. So $\\ln{\\ln{n}} = \\Omega(2^{\\lg^*{n}})$. And $2^{\\lg^*{n}} = \\Omega(\\lg^*{n})$.  Since $\\lg^*{n} = 1 + \\lg^*{\\lg{n}}$, so $\\lg^*{\\lg{n}} = \\Theta(\\lg^*{n})$.  In question 3.2-5 we proved that $\\lg^*{(\\lg{n})}$ is asymptotically larger than $\\lg{(\\lg^*{n})}$. So $\\lg^*{(\\lg{n})} = \\Omega(\\lg{(\\lg^*{n})})$.  $n^{\\frac{1}{\\lg{n}}} = (2^{\\lg{n}})^{\\frac{1}{\\lg{n}}} = 2$, so $n^{\\frac{1}{\\lg{n}}} = \\Theta(1)$.  Summary   $g_1 = 2^{2^{n + 1}}$  $g_2 = 2^{2^n}$  $g_3 = (n + 1)!$  $g_4 = n!$  $g_5 = e^n$  $g_6 = n2^n$  $g_7 = 2^n$  $g_8 = (\\frac{3}{2})^n$  $g_9 = (\\lg{n})^{\\lg{n}}$  $g_{10} = n^{\\lg{\\lg{n}}}, g_9 = \\Theta(g_{10})$  $g_{11} = (\\lg{n})!$  $g_{12} = n^3$  $g_{13} = n^2$  $g_{14} = 4^{\\lg{n}}, g_{13} = \\Theta(g_{14})$  $g_{15} = n\\lg{n}$  $g_{16} = \\lg({n!}), g_{15} = \\Theta(g_{16})$  $g_{17} = n$  $g_{18} = 2^{\\lg{n}}, g_{17} = \\Theta(g_{18})$  $g_{19} = (\\sqrt{2})^{\\lg{n}}$  $g_{20} = 2^{\\sqrt{2\\lg{n}}}$  $g_{21} = \\lg^2{n}$  $g_{22} = \\ln{n}$  $g_{23} = \\sqrt{\\lg{n}}$  $g_{24} = \\ln{\\ln{n}}$  $g_{25} = 2^{\\lg^*{n}}$  $g_{26} = \\lg^*{n}$  $g_{27} = \\lg^*{\\lg{n}}, g_{26} = \\Theta(g_{27})$  $g_{28} = \\lg{(\\lg^*{n})}$  $g_{29} = n^{\\frac{1}{\\lg{n}}}$  $g_{30} = 1, g_{29} = \\Theta(g_{30})$   b  How do we find a function like this? In question 3-2, we know that $n^{\\sin{n}}$ is neither $o(\\sqrt{n})$ nor $w(\\sqrt{n})$. We can use the feature of $\\sin{n}$. So we can build a function that can be quite big and quite small. And it has to be not smaller than the largest function in $g_i(n)$ sometimes, so we can simply have a function like $n2^{2^{n + 1}}$, which is $\\Omega(g_i(n))$. Then we multiply $|\\sin{n}|$, so the function is $|\\sin{n}|n2^{2^{n + 1}}$, which is neither $O(g_i(n))$ nor $\\Omega(g_i(n))$.", 
            "title": "3-3"
        }, 
        {
            "location": "/3-Growth-of-Functions/Problems/#3-4", 
            "text": "a  This is not true. If $g(n) = O(f(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq g(n) \\leq c_1f(n) \\text{ for all } n \\geq n_1$, since $f(n) = O(g(n))$, so there also exist positive constants $c_2$ and $n_2$ such that $0 \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_2$, so we have $0 \\leq \\frac{1}{c_1}g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq max(n_1, n_2)$, which means $f(n) = \\Theta(g(n))$. This is not 100% true.  b  This is not true. Let $f(n) = n$ and $g(n) = \\lg{n}$, it's easy to see $f(n) + g(n) = \\Theta(n) \\neq \\Theta(min(f(n), g(n)))$.  c  This is true. Because $f(n) = O(g(n))$, so there exist positive constants c and $n_0$ such that $0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0$. So $0\\leq \\lg{(f(n))} \\leq \\lg{(cg(n))} = \\lg{c} + \\lg{(g(n))} = \\lg{c} * 1 + \\lg{(g(n))} \\leq \\lg{c} * \\lg{(g(n))} + \\lg{(g(n))} = (\\lg{c} + 1)\\lg{(g(n))}$. So we find a positive constant $c_1 = \\lg{c} + 1$ such that $0 \\leq \\lg{(f(n))} \\leq c_1\\lg{(g(n))} \\text{ for all } n \\geq n_0$, so $\\lg{(f(n))} = O(\\lg{(g(n))}$.  d  This is not true. If $g(n) = O(f(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq g(n) \\leq c_1f(n) \\text{ for all } n \\geq n_1$. Suppose $2^{f(n)} = O(2^{g(n)})$, then there exist positive constants $c_2$ and $n_2$ such that $0 \\leq 2^{f(n)} \\leq {c_2}2^{g(n)} \\text{ for all } n \\geq n_2$. And ${c_2}2^{g(n)} = 2^{\\lg{c_2} + g(n)}$. So we have $f(n) \\leq \\lg{c_2} + g(n)$. But this is not always true, let $f(n) = n + \\lg{n}$ and $g(n) = n$, so $f(n) \\leq 2g(n) \\text{ for all } n \\geq 1$. But for any given positive constant $c_2$, we can always find a positive constant $n_3$ such that $n + \\lg{n}   \\lg{c_2} + n \\text{ for all } n \\geq n_3$. So $2^{f(n)} \\neq O(2^{g(n)})$ in this situation.  e  This is not true. Let $f(n) = \\frac{1}{n}$, if $f(n) = O((f(n))^2)$, then there exist positive constants c and $n_0$ such that $0 \\leq \\frac{1}{n} \\leq \\frac{c}{n^2} \\text{ for all } n \\geq n_0$. But $\\frac{c}{n^2} - \\frac{1}{n} = \\frac{c - n}{n^2} \\leq 0 \\text{ for all } n \\geq c$. So we cannot find such $n_0$ for any positive constant $c$.  f  This is true. If $f(n) = O(g(n))$, then there exist positive constants c and $n_0$ such that $0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0$, so we have $0 \\leq \\frac{1}{c}f(n) \\leq g(n) \\text{ for all } n \\geq n_0$, which is the definition of $g(n) = \\Omega(f(n))$.  g  This is not true. Let $f(n) = 2^{2n}$, so $f(\\frac{n}{2}) = 2^n$. So it's obvious $f(n) \\neq \\Theta(f(\\frac{n}{2}))$.  h  This is true. Suppose $f(n) + o(f(n)) = \\Theta(f(n))$, then we need to prove that there exist positive constatns $c_1$, $c_2$ and $n_0$ such that $0 \\leq c_1f(n) \\leq f(n) + o(f(n)) \\leq c_2f(n)$. It's easy to find $c_1 = 1$ since $o(f(n)) \\geq 0$. And according to the definition of $o(f(n))$, we know for any positive constant c there exists positive constant $n_1$ such that $0 \\leq o(f(n))   cf(n) \\text{ for all } n \\geq n_1$. so we can choose $c = 1$, so $f(n) + o(f(n)) \\leq f(n) + f(n) = 2f(n)$, so we have $c_2 = 2$ and $n_0 = n_1$. So $f(n) + o(f(n)) = \\Theta(f(n))$.", 
            "title": "3-4"
        }, 
        {
            "location": "/3-Growth-of-Functions/Problems/#3-5", 
            "text": "a  Suppose both $f(n) = O(g(n))$ and $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ are not true. If $f(n) = O(g(n))$ is not true, then there are two cases, first, we can not find any positive constant c such that $0 \\leq f(n) \\leq cg(n)$ for any integer n. It means for any positive constant c we have $f(n)   cg(n)$. So it satisfies $f(n) \\geq cg(n) \\geq 0$ for infinitely many integers n. So it shows $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ is true, thus, the hypothesis is wrong. Second, we can find a constant c such that $0 \\leq f(n) \\leq cg(n)$ for some integers n. If the set of integers n is finite, then there is an infinite set such that $f(n) \\geq cg(n) \\geq 0$, so $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$. If the set is infinite but we cannot find a positive constant $n_0$ that satisfies $f(n) = O(g(n))$, there is also a infinite set such that $f(n)   cg(n) \\geq 0$, so the hypothesis is wrong. So we proved either $f(n) = O(g(n))$ or $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ or both.  In problem 3-2 we proved both $\\sqrt{n} = O(n^{\\sin{n}})$ and $\\sqrt{n} = \\Omega(n^{\\sin{n}})$ are wrong. So it's not true if we use $\\Omega$ in place of $\\mathop{\\Omega}^{\\infty}$.  b  The advantage is that we can describe the relationship of two functions when we cannot use $\\Omega$ notation. The disadvantage is that sometimes we cannot clearly know the running time of a function.  c  If $f(n) = \\Theta(g(n))$, then we have $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$. If we have $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$, $f(n) = \\Theta(g(n))$ is also true. Since $f(n) = \\Omega(g(n))$ guarantees $f(n) \\geq 0$.  d  $\\tilde{\\Omega}(g(n)) = \\lbrace f(n): \\text{ there exist positive constants } c, \\text{ } k, \\text{ and } n_0 \\text{ such that } 0 \\leq cg(n)\\lg^k{n} \\leq f(n) \\text{ for all } n \\geq n_0 \\rbrace$.  $\\tilde{\\Theta}(g(n)) = \\lbrace f(n): \\text{ there exist positive constants } c_1, \\text{ } c_2, \\text{ } k_1, \\text{ } k_2 \\text{ and } n_0 \\text{ such that } 0 \\leq c_1g(n)\\lg^{k_1}{n} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{n} \\text{ for all } n \\geq n_0 \\rbrace$.  Prove Theorem 3.1:  If $f(n) = \\tilde{\\Theta}(g(n))$, then there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, $n_0$ such that $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_0$.  It means: $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\text{ for all } n \\geq n_0$ and $0 \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_0$. They are the definition of $f(n) = \\tilde{O}(g(n))$ and $f(n) = \\tilde{\\Omega}(g(n))$.  If $f(n) = \\tilde{O}(g(n))$ and $f(n) = \\tilde{\\Omega}(g(n))$. Then there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, $n_1$, $n_2$ such that $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\text{ for all } n \\geq n_1$ and $0 \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_2$. Then we combine them together: $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq max(n_1, n_2)$. So $f(n) = \\tilde{\\Theta}(g(n))$.", 
            "title": "3-5"
        }, 
        {
            "location": "/3-Growth-of-Functions/Problems/#3-6", 
            "text": "a  $f^2(n) = n - 1 - 1 = n - 2$, $f^3(n) = n - 1 - 2 = n - 3$, so $f^k(n) = n - k$. It's easy to see $f^k(n) = 0$ after nth iterations.  b  $f^2(n) = \\lg{\\lg{n}}$, $f^3(n) = \\lg{\\lg{\\lg{n}}}$, so $f^k(n) = \\underbrace{\\lg{\\lg{\\ldots\\lg{n}}}}_\\text{k}$. Let $\\lg{\\lg{\\ldots\\lg{n}}} = 1$, so we have $f^{k - 1}(n) = 2$, and similiarly, $f^{k - 2}(n) = 2^2$, so $f^{k - (k - 1)}(n) = 2^{k - 1}$, so $k = \\lg{\\lg{n}} + 1$.  c  $f^2(n) = \\frac{n}{2^2}$, $f^k(n) = \\frac{n}{2^k}$, let $f^k(n) = \\frac{n}{2^k} = 1$, we have $k = \\lg{n}$.  d  Let $f^k(n) = \\frac{n}{2^k} = 2$, we have $k = \\lg{n} - 1$.  e  $f^2(n) = n^{\\frac{1}{2^2}}$, $f^k(n) = n^{\\frac{1}{2^k}}$, let $f^k(n) = n^{\\frac{1}{2^k}} = 2$, so $k = \\lg{\\lg{n}}$.  f  Because $\\frac{1}{2^k}$ could not be 0, so $f^k(n) = n^{\\frac{1}{2^k}}$ could not be 1.  g  $f^2(n) = n^{\\frac{1}{3^2}}$, $f^k(n) = n^{\\frac{1}{3^k}}$, let $f^k(n) = n^{\\frac{1}{3^k}} = 2$, so $k = \\frac{\\lg{\\lg{n}}}{\\lg{3}}$.  h  ?  Summary     $f(n)$  $c$  $f_c^*(n)$      $n - 1$  0  $n$    $\\lg{n}$  1  $\\lceil \\lg{\\lg{n}} + 1 \\rceil$    $\\frac{n}{2}$  1  $\\lceil \\lg{n} \\rceil$    $\\frac{n}{2}$  2  $\\lceil \\lg{n} - 1 \\rceil$    $\\sqrt{n}$  2  $\\lceil \\lg{\\lg{n}} \\rceil$    $\\sqrt{n}$  1  $+\\infty$    $n^{\\frac{1}{3}}$  2  $\\lceil \\frac{\\lg{\\lg{n}}}{\\lg{3}} \\rceil$    $\\frac{n}{\\lg{n}}$  2  ?", 
            "title": "3-6"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.1-The-maximum-subarray-problem/", 
            "text": "4.1 The maximum-subarray problem\n\n\n4.1-1\n\n\nIt returns the index and value of the biggest negative number in A.\n\n\n4.1-2\n\n\nFIND-MAXIMUM-SUBARRAY-BRUTE-FORCE(A, low, high)\n\nmax = -\u221e\nstart = -1, end = -1\n\nfor i = low to high\n    sum = 0\n\n    for j = i to high\n        sum += A[j]\n\n        if sum \n max\n            max = sum\n            start = i\n            end = j\n\nreturn (start, end, max)\n\n\n\n\n4.1-3\n\n\nBrute force:\n\n\ndef find_maximum_subarray_brute_force(numbers, low, high):\n    start = -1\n    end = -1\n    max_sum = -float('inf')\n\n    for i in range(low, high + 1):\n        current_sum = 0\n\n        for j in range(i, high + 1):\n            current_sum += numbers[j]\n\n            if current_sum \n max_sum:\n                max_sum = current_sum\n                start, end = i, j\n\n    return (start, end, max_sum)\n\n\n\n\nDivide and conquer:\n\n\ndef find_maximum_subarray_divide_and_conquer(numbers, low, high):\n    if len(numbers) == 0:\n        return (-1, -1, -float('inf'))\n    elif low == high:\n        return (low, high, numbers[low])\n    else:\n        middle = (low + high) // 2\n\n        left_maximum_subarray = find_maximum_subarray_divide_and_conquer(\n            numbers, low, middle)\n        right_maximum_subarray = find_maximum_subarray_divide_and_conquer(\n            numbers, middle + 1, high)\n        cross_maximum_subarray = find_max_crossing_subarray(\n            numbers, low, middle, high)\n\n        if (left_maximum_subarray[2] \n= right_maximum_subarray[2] and\n                left_maximum_subarray[2] \n= cross_maximum_subarray[2]):\n            return left_maximum_subarray\n        elif (right_maximum_subarray[2] \n= left_maximum_subarray[2] and\n                right_maximum_subarray[2] \n= cross_maximum_subarray[2]):\n            return right_maximum_subarray\n        else:\n            return cross_maximum_subarray\n\n\ndef find_max_crossing_subarray(numbers, low, middle, high):\n    start = middle\n    end = middle\n    current_sum = 0\n    left_sum = -float('inf')\n    right_sum = -float('inf')\n\n    for i in range(middle, low - 1, -1):\n        current_sum += numbers[i]\n\n        if current_sum \n left_sum:\n            left_sum = current_sum\n            start = i\n\n    current_sum = 0\n\n    for i in range(middle + 1, high + 1):\n        current_sum += numbers[i]\n\n        if current_sum \n right_sum:\n            right_sum = current_sum\n            end = i\n\n    return (start, end, left_sum + right_sum)\n\n\n\n\nIn my env, when $n_0 = 64$ gives the crossover point. I changed the base of the recursive algorithm, but that doesn't change the crossover point.\n\n\n4.1-4\n\n\nCurrently the implementation already supports empty array, but it returns \n(-1, -1, -float('inf'))\n, we can change it to an empty array.\n\n\n4.1-5\n\n\ndef find_maximum_subarray_linear_time(numbers, low, high):\n    if len(numbers) == 0:\n        return (-1, -1, -float('inf'))\n\n    start = -1\n    max_sum_start = -1\n    max_sum_end = -1\n    max_sum_so_far = -float('inf')\n    max_sum_ending_here = -float('inf')\n\n    for i in range(low, high + 1):\n        if numbers[i] \n max_sum_ending_here + numbers[i]:\n            max_sum_ending_here = numbers[i]\n            start = i\n        else:\n            max_sum_ending_here += numbers[i]\n\n        if max_sum_ending_here \n max_sum_so_far:\n            max_sum_so_far = max_sum_ending_here\n            max_sum_start = start\n            max_sum_end = i\n\n    return (max_sum_start, max_sum_end, max_sum_so_far)", 
            "title": "4.1 The maximum-subarray problem"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-the-maximum-subarray-problem", 
            "text": "", 
            "title": "4.1 The maximum-subarray problem"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-1", 
            "text": "It returns the index and value of the biggest negative number in A.", 
            "title": "4.1-1"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-2", 
            "text": "FIND-MAXIMUM-SUBARRAY-BRUTE-FORCE(A, low, high)\n\nmax = -\u221e\nstart = -1, end = -1\n\nfor i = low to high\n    sum = 0\n\n    for j = i to high\n        sum += A[j]\n\n        if sum   max\n            max = sum\n            start = i\n            end = j\n\nreturn (start, end, max)", 
            "title": "4.1-2"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-3", 
            "text": "Brute force:  def find_maximum_subarray_brute_force(numbers, low, high):\n    start = -1\n    end = -1\n    max_sum = -float('inf')\n\n    for i in range(low, high + 1):\n        current_sum = 0\n\n        for j in range(i, high + 1):\n            current_sum += numbers[j]\n\n            if current_sum   max_sum:\n                max_sum = current_sum\n                start, end = i, j\n\n    return (start, end, max_sum)  Divide and conquer:  def find_maximum_subarray_divide_and_conquer(numbers, low, high):\n    if len(numbers) == 0:\n        return (-1, -1, -float('inf'))\n    elif low == high:\n        return (low, high, numbers[low])\n    else:\n        middle = (low + high) // 2\n\n        left_maximum_subarray = find_maximum_subarray_divide_and_conquer(\n            numbers, low, middle)\n        right_maximum_subarray = find_maximum_subarray_divide_and_conquer(\n            numbers, middle + 1, high)\n        cross_maximum_subarray = find_max_crossing_subarray(\n            numbers, low, middle, high)\n\n        if (left_maximum_subarray[2]  = right_maximum_subarray[2] and\n                left_maximum_subarray[2]  = cross_maximum_subarray[2]):\n            return left_maximum_subarray\n        elif (right_maximum_subarray[2]  = left_maximum_subarray[2] and\n                right_maximum_subarray[2]  = cross_maximum_subarray[2]):\n            return right_maximum_subarray\n        else:\n            return cross_maximum_subarray\n\n\ndef find_max_crossing_subarray(numbers, low, middle, high):\n    start = middle\n    end = middle\n    current_sum = 0\n    left_sum = -float('inf')\n    right_sum = -float('inf')\n\n    for i in range(middle, low - 1, -1):\n        current_sum += numbers[i]\n\n        if current_sum   left_sum:\n            left_sum = current_sum\n            start = i\n\n    current_sum = 0\n\n    for i in range(middle + 1, high + 1):\n        current_sum += numbers[i]\n\n        if current_sum   right_sum:\n            right_sum = current_sum\n            end = i\n\n    return (start, end, left_sum + right_sum)  In my env, when $n_0 = 64$ gives the crossover point. I changed the base of the recursive algorithm, but that doesn't change the crossover point.", 
            "title": "4.1-3"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-4", 
            "text": "Currently the implementation already supports empty array, but it returns  (-1, -1, -float('inf')) , we can change it to an empty array.", 
            "title": "4.1-4"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-5", 
            "text": "def find_maximum_subarray_linear_time(numbers, low, high):\n    if len(numbers) == 0:\n        return (-1, -1, -float('inf'))\n\n    start = -1\n    max_sum_start = -1\n    max_sum_end = -1\n    max_sum_so_far = -float('inf')\n    max_sum_ending_here = -float('inf')\n\n    for i in range(low, high + 1):\n        if numbers[i]   max_sum_ending_here + numbers[i]:\n            max_sum_ending_here = numbers[i]\n            start = i\n        else:\n            max_sum_ending_here += numbers[i]\n\n        if max_sum_ending_here   max_sum_so_far:\n            max_sum_so_far = max_sum_ending_here\n            max_sum_start = start\n            max_sum_end = i\n\n    return (max_sum_start, max_sum_end, max_sum_so_far)", 
            "title": "4.1-5"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/", 
            "text": "4.2 Strassen's algorithm for matrix multiplication\n\n\n4.2-1\n\n\nStep 1, we partition each of matrix A, B into four $\\frac{n}{2} \\text{ * } \\frac{n}{2}$ matrices. So we have:\n\n\n$$\nA = \\begin{pmatrix}\n    A_{11} \n A_{12} \\\\\n    A_{21} \n A_{22} \\\\\n\\end{pmatrix},\nA_{11} = \\begin{pmatrix}\n    1\n\\end{pmatrix},\nA_{12} = \\begin{pmatrix}\n    3\n\\end{pmatrix},\nA_{21} = \\begin{pmatrix}\n    7\n\\end{pmatrix},\nA_{22} = \\begin{pmatrix}\n    5\n\\end{pmatrix}\n$$\n\n\n$$\nB = \\begin{pmatrix}\n    B_{11} \n B_{12} \\\\\n    B_{21} \n B_{22} \\\\\n\\end{pmatrix},\nB_{11} = \\begin{pmatrix}\n    6\n\\end{pmatrix},\nB_{12} = \\begin{pmatrix}\n    8\n\\end{pmatrix},\nB_{21} = \\begin{pmatrix}\n    4\n\\end{pmatrix},\nB_{22} = \\begin{pmatrix}\n    2\n\\end{pmatrix}\n$$\n\n\nStep 2, we create 10 matrices:\n\n\n$$S_1 = B_{12} - B_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$\n$$S_2 = A_{11} + A_{12} = \\begin{pmatrix} 4 \\end{pmatrix}$$\n$$S_3 = A_{21} + A_{22} = \\begin{pmatrix} 12 \\end{pmatrix}$$\n$$S_4 = B_{21} - B_{11} = \\begin{pmatrix} -2 \\end{pmatrix}$$\n$$S_5 = A_{11} + A_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$\n$$S_6 = B_{11} + B_{22} = \\begin{pmatrix} 8 \\end{pmatrix}$$\n$$S_7 = A_{12} - A_{22} = \\begin{pmatrix} -2 \\end{pmatrix}$$\n$$S_8 = B_{21} + B_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$\n$$S_9 = A_{11} - A_{21} = \\begin{pmatrix} -6 \\end{pmatrix}$$\n$$S_{10} = B_{11} + B_{12} = \\begin{pmatrix} 14 \\end{pmatrix}$$\n\n\nStep 3, we compute the seven matrix products:\n\n\n$$P_1 = A_{11} * S_{1} = \\begin{pmatrix} 6 \\end{pmatrix}$$\n$$P_2 = S_{2} * B_{22} = \\begin{pmatrix} 8 \\end{pmatrix}$$\n$$P_3 = S_{3} * B_{11} = \\begin{pmatrix} 72 \\end{pmatrix}$$\n$$P_4 = A_{22} * S_{4} = \\begin{pmatrix} -10 \\end{pmatrix}$$\n$$P_5 = S_{5} * S_{6} = \\begin{pmatrix} 48 \\end{pmatrix}$$\n$$P_6 = S_{7} * S_{8} = \\begin{pmatrix} -12 \\end{pmatrix}$$\n$$P_7 = S_{9} * S_{10} = \\begin{pmatrix} -84 \\end{pmatrix}$$\n\n\nStep 4, compute the desired submatrices:\n\n\n$$C_{11} = P_5 + P_4 - P_2 + P_6 = \\begin{pmatrix} 18 \\end{pmatrix}$$\n$$C_{12} = P_1 + P_2 = \\begin{pmatrix} 14 \\end{pmatrix}$$\n$$C_{21} = P_3 + P_4 = \\begin{pmatrix} 62 \\end{pmatrix}$$\n$$C_{22} = P_5 + P_1 - P_3 - P_7 = \\begin{pmatrix} 66 \\end{pmatrix}$$\n\n\nSo $C = \\begin{pmatrix}\n    C_{11} \n C_{12} \\\\\n    C_{21} \n C_{22} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n    18 \n 14 \\\\\n    62 \n 66 \\\\\n\\end{pmatrix}$.\n\n\n4.2-2\n\n\nSQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A, B)\n\nn = A.rows\nlet C be a new n * n matrix\nif n == 1\n    C11 = A11 * B11\nelse partition A, B, and C as in equations (4.9)\n    S1 = B12 - B22\n    S2 = A11 + A12\n    S3 = A21 + A22\n    S4 = B21 - B11\n    S5 = A11 + A22\n    S6 = B11 + B22\n    S7 = A12 - A22\n    S8 = B21 + B22\n    S9 = A11 - A21\n    S10 = B11 + B12\n\n    P1 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A11, S1)\n    P2 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S2, B22)\n    P3 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S3, B11)\n    P4 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A22, S4)\n    P5 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S5, S6)\n    P6 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S7, S8)\n    P7 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S9, S10)\n\n    C11 = P5 + P4 - P2 + P6\n    C12 = P1 + P2\n    C21 = P3 + P4\n    C22 = P5 + P1 - P3 - P7\nreturn C\n\n\n\n\ndef square_matrix_multiply_strassen_algorithm(a, b):\n    n = len(a)\n    c = [[0] * n for i in range(n)]\n\n    if n == 1:\n        c[0][0] = a[0][0] * b[0][0]\n    else:\n        half = n // 2\n\n        a11 = [[0] * half for i in range(half)]\n        a12 = [[0] * half for i in range(half)]\n        a21 = [[0] * half for i in range(half)]\n        a22 = [[0] * half for i in range(half)]\n        b11 = [[0] * half for i in range(half)]\n        b12 = [[0] * half for i in range(half)]\n        b21 = [[0] * half for i in range(half)]\n        b22 = [[0] * half for i in range(half)]\n\n        for i in range(half):\n            for j in range(half):\n                a11[i][j] = a[i][j]\n                a12[i][j] = a[i][j + half]\n                a21[i][j] = a[i + half][j]\n                a22[i][j] = a[i + half][j + half]\n                b11[i][j] = b[i][j]\n                b12[i][j] = b[i][j + half]\n                b21[i][j] = b[i + half][j]\n                b22[i][j] = b[i + half][j + half]\n\n        s1 = subtract(b12, b22)\n        s2 = add(a11, a12)\n        s3 = add(a21, a22)\n        s4 = subtract(b21, b11)\n        s5 = add(a11, a22)\n        s6 = add(b11, b22)\n        s7 = subtract(a12, a22)\n        s8 = add(b21, b22)\n        s9 = subtract(a11, a21)\n        s10 = add(b11, b12)\n\n        p1 = square_matrix_multiply_strassen_algorithm(a11, s1)\n        p2 = square_matrix_multiply_strassen_algorithm(s2, b22)\n        p3 = square_matrix_multiply_strassen_algorithm(s3, b11)\n        p4 = square_matrix_multiply_strassen_algorithm(a22, s4)\n        p5 = square_matrix_multiply_strassen_algorithm(s5, s6)\n        p6 = square_matrix_multiply_strassen_algorithm(s7, s8)\n        p7 = square_matrix_multiply_strassen_algorithm(s9, s10)\n\n        c11 = add(subtract(add(p5, p4), p2), p6)\n        c12 = add(p1, p2)\n        c21 = add(p3, p4)\n        c22 = subtract(subtract(add(p5, p1), p3), p7)\n\n        for i in range(half):\n            for j in range(half):\n                c[i][j] = c11[i][j]\n                c[i][j + half] = c12[i][j]\n                c[i + half][j] = c21[i][j]\n                c[i + half][j + half] = c22[i][j]\n\n    return c\n\n\ndef add(a, b):\n    n = len(a)\n    c = [[0] * n for i in range(n)]\n\n    for i in range(n):\n        for j in range(n):\n            c[i][j] = a[i][j] + b[i][j]\n\n    return c\n\n\ndef subtract(a, b):\n    n = len(a)\n    c = [[0] * n for i in range(n)]\n\n    for i in range(n):\n        for j in range(n):\n            c[i][j] = a[i][j] - b[i][j]\n\n    return c\n\n\n\n\n4.2-3\n\n\nIf n is not an exact power of 2, then we can pad 0 to matrix to make it an exact power of 2. Let $k = \\lfloor \\lg{n} \\rfloor$, Let $m = 2^{k + 1}$, so we have a new m * m matrix. Thus, $T(m) = 7T(\\frac{m}{2}) + \\Theta(m^2)$. According to master method, we have $T(m) = \\Theta(m^{\\lg7})$. So there exist postive constants $c_1$, $c_2$ and $n_0$ such that $c_1m^{\\lg7} \\leq T(m) \\leq c_2m^{\\lg7} \\text{ for all } n \\geq n_0$.\n\n\nWe have $m^{\\lg7} = (2^{\\lfloor \\lg{n} \\rfloor + 1})^{\\lg7} \\leq (2^{\\lg{n} + 1})^{\\lg7} = 7n^{\\lg7}$. And $m^{\\lg7} = (2^{\\lfloor \\lg{n} \\rfloor + 1})^{\\lg7} \\geq (2^{\\lg{n} - 1 + 1})^{\\lg7} = n^{\\lg7}$. So $min(c_1, 1)n^{\\lg7} \\leq T(m) \\leq max(c_2, 7)n^{\\lg7} \\text{ for all } n \\geq n_0$. Thus $T(m) = \\Theta(n^{\\lg7})$, the resulting algorithm still runs in time $\\Theta(n^{\\lg7})$.\n\n\n4.2-4\n\n\nWe have $T(n) = kT(\\frac{n}{3}) + \\Theta(n^2)$ with $b = 3, a = k$. If we want to run the algorithm in time $o(n^{\\lg7})$, then case 3 cannot apply, since $f(n) = \\Omega(n^{\\lg7})$.\n\n\nIf case 2 applies, then $f(n) = \\Theta(n^{\\log_3k})$, so $\\log_3k = 2$, but the algorithm runs in time $\\Theta(n^2\\lg{n})$.\n\n\nSo case 1 applies, the algorithm runs in time $\\Theta(n^{\\log_3k}) = \\Theta(n^{\\frac{\\lg{k}}{\\lg3}})$.\n\n\nIf we want the algorithm to runs in time $o(n^{\\lg7})$, then we have $\\log_3k \n \\lg7, k \n 3^{\\lg7}$, so the largest k is 21. \n\n\n4.2-5\n\n\nWe know the best running time of multiplying b * b matrices using a multiplications is $\\Theta(n^{\\log_ba})$. So $\\log_68{132464} = 2.7951284873613815$, $\\log_70{143640} = 2.795122689748337$, $\\log_72{155424} = 2.795147391093449$. So multiplying 72 * 72 matrices using 155,424 multiplications yields the best asymptotic running time.\n\n\nSince $\\lg7 = 2.807354922057604$, so the new algorithm is faster than Strassen's algorithm.\n\n\n4.2-6\n\n\nWe can partition A, B into k n * n matrices, and partition C into $k^2$ n * n matrices:\n\n\n$$\nA = \\begin{pmatrix}\n    A_{11} \\\\\n    A_{21} \\\\\n    \\ldots \\\\\n    A_{k1}\n\\end{pmatrix},\nB = \\begin{pmatrix}\n    B_{11} \n B_{12} \n \\ldots \n B_{1k}\n\\end{pmatrix},\nC = \\begin{pmatrix}\n    C_{11} \n C_{12} \n \\ldots \n C_{1k} \\\\\n    \\ldots \\\\\n    C_{k1} \n C_{k2} \n \\ldots \n C_{kk}\n\\end{pmatrix}\n$$\n\n\nSo that we rewrite the equation C = A * B as:\n\n\n$$\n\\begin{pmatrix}\n    C_{11} \n C_{12} \n \\ldots \n C_{1k} \\\\\n    \\ldots \\\\\n    C_{k1} \n C_{k2} \n \\ldots \n C_{kk}\n\\end{pmatrix} =\n\\begin{pmatrix}\n    A_{11} \\\\\n    A_{21} \\\\\n    \\ldots \\\\\n    A_{k1}\n\\end{pmatrix} * \n\\begin{pmatrix}\n    B_{11} \n B_{12} \n \\ldots \n B_{1k}\n\\end{pmatrix},\nC_{ij} = A_{i1} * B_{1j}\n$$\n\n\nThen we can use Strassen's algorithm as a subroutine to calculate $C_{ij}$.\n\n\nlet C be a new kn * kn matrix\nfor i = 1 to k\n    for j = 1 to k\n        Cij = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(Ai1 * B1j)\nreturn C\n\n\n\n\nIf the input matrices are reversed, we can partition A, B into k n * n matrices:\n\n\n$$\nA = \\begin{pmatrix}\n    A_{11} \n A_{12} \n \\ldots \n A_{1k}\n\\end{pmatrix},\nB = \\begin{pmatrix}\n    B_{11} \\\\\n    B_{21} \\\\\n    \\ldots \\\\\n    B_{k1}\n\\end{pmatrix}\n$$\n\n\nSo that we rewrite the equation C = A * B as:\n\n\n$$\nC =\n\\begin{pmatrix}\n    A_{11} \n A_{12} \n \\ldots \n A_{1k}\n\\end{pmatrix} * \n\\begin{pmatrix}\n    B_{11} \\\\\n    B_{21} \\\\\n    \\ldots \\\\\n    B_{k1}\n\\end{pmatrix}\n$$\n\n\nThen we can use Strassen's algorithm as a subroutine to calculate C.\n\n\nlet C be a new n * n matrix\nfor i = 1 to k\n    C11 += SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A1i * Bi1)\nreturn C\n\n\n\n\n4.2-7\n\n\nMULTIPLY-COMPLEX-NUMBERS(a, b, c, d)\n\nn1 = (a + b) * (c + d)\nn2 = a * c\nn3 = b * d\nreturn (n2 - n3, n1 - n2 - n3)", 
            "title": "4.2 Strassen's algorithm for matrix multiplication"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-strassens-algorithm-for-matrix-multiplication", 
            "text": "", 
            "title": "4.2 Strassen's algorithm for matrix multiplication"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-1", 
            "text": "Step 1, we partition each of matrix A, B into four $\\frac{n}{2} \\text{ * } \\frac{n}{2}$ matrices. So we have:  $$\nA = \\begin{pmatrix}\n    A_{11}   A_{12} \\\\\n    A_{21}   A_{22} \\\\\n\\end{pmatrix},\nA_{11} = \\begin{pmatrix}\n    1\n\\end{pmatrix},\nA_{12} = \\begin{pmatrix}\n    3\n\\end{pmatrix},\nA_{21} = \\begin{pmatrix}\n    7\n\\end{pmatrix},\nA_{22} = \\begin{pmatrix}\n    5\n\\end{pmatrix}\n$$  $$\nB = \\begin{pmatrix}\n    B_{11}   B_{12} \\\\\n    B_{21}   B_{22} \\\\\n\\end{pmatrix},\nB_{11} = \\begin{pmatrix}\n    6\n\\end{pmatrix},\nB_{12} = \\begin{pmatrix}\n    8\n\\end{pmatrix},\nB_{21} = \\begin{pmatrix}\n    4\n\\end{pmatrix},\nB_{22} = \\begin{pmatrix}\n    2\n\\end{pmatrix}\n$$  Step 2, we create 10 matrices:  $$S_1 = B_{12} - B_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$\n$$S_2 = A_{11} + A_{12} = \\begin{pmatrix} 4 \\end{pmatrix}$$\n$$S_3 = A_{21} + A_{22} = \\begin{pmatrix} 12 \\end{pmatrix}$$\n$$S_4 = B_{21} - B_{11} = \\begin{pmatrix} -2 \\end{pmatrix}$$\n$$S_5 = A_{11} + A_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$\n$$S_6 = B_{11} + B_{22} = \\begin{pmatrix} 8 \\end{pmatrix}$$\n$$S_7 = A_{12} - A_{22} = \\begin{pmatrix} -2 \\end{pmatrix}$$\n$$S_8 = B_{21} + B_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$\n$$S_9 = A_{11} - A_{21} = \\begin{pmatrix} -6 \\end{pmatrix}$$\n$$S_{10} = B_{11} + B_{12} = \\begin{pmatrix} 14 \\end{pmatrix}$$  Step 3, we compute the seven matrix products:  $$P_1 = A_{11} * S_{1} = \\begin{pmatrix} 6 \\end{pmatrix}$$\n$$P_2 = S_{2} * B_{22} = \\begin{pmatrix} 8 \\end{pmatrix}$$\n$$P_3 = S_{3} * B_{11} = \\begin{pmatrix} 72 \\end{pmatrix}$$\n$$P_4 = A_{22} * S_{4} = \\begin{pmatrix} -10 \\end{pmatrix}$$\n$$P_5 = S_{5} * S_{6} = \\begin{pmatrix} 48 \\end{pmatrix}$$\n$$P_6 = S_{7} * S_{8} = \\begin{pmatrix} -12 \\end{pmatrix}$$\n$$P_7 = S_{9} * S_{10} = \\begin{pmatrix} -84 \\end{pmatrix}$$  Step 4, compute the desired submatrices:  $$C_{11} = P_5 + P_4 - P_2 + P_6 = \\begin{pmatrix} 18 \\end{pmatrix}$$\n$$C_{12} = P_1 + P_2 = \\begin{pmatrix} 14 \\end{pmatrix}$$\n$$C_{21} = P_3 + P_4 = \\begin{pmatrix} 62 \\end{pmatrix}$$\n$$C_{22} = P_5 + P_1 - P_3 - P_7 = \\begin{pmatrix} 66 \\end{pmatrix}$$  So $C = \\begin{pmatrix}\n    C_{11}   C_{12} \\\\\n    C_{21}   C_{22} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n    18   14 \\\\\n    62   66 \\\\\n\\end{pmatrix}$.", 
            "title": "4.2-1"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-2", 
            "text": "SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A, B)\n\nn = A.rows\nlet C be a new n * n matrix\nif n == 1\n    C11 = A11 * B11\nelse partition A, B, and C as in equations (4.9)\n    S1 = B12 - B22\n    S2 = A11 + A12\n    S3 = A21 + A22\n    S4 = B21 - B11\n    S5 = A11 + A22\n    S6 = B11 + B22\n    S7 = A12 - A22\n    S8 = B21 + B22\n    S9 = A11 - A21\n    S10 = B11 + B12\n\n    P1 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A11, S1)\n    P2 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S2, B22)\n    P3 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S3, B11)\n    P4 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A22, S4)\n    P5 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S5, S6)\n    P6 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S7, S8)\n    P7 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S9, S10)\n\n    C11 = P5 + P4 - P2 + P6\n    C12 = P1 + P2\n    C21 = P3 + P4\n    C22 = P5 + P1 - P3 - P7\nreturn C  def square_matrix_multiply_strassen_algorithm(a, b):\n    n = len(a)\n    c = [[0] * n for i in range(n)]\n\n    if n == 1:\n        c[0][0] = a[0][0] * b[0][0]\n    else:\n        half = n // 2\n\n        a11 = [[0] * half for i in range(half)]\n        a12 = [[0] * half for i in range(half)]\n        a21 = [[0] * half for i in range(half)]\n        a22 = [[0] * half for i in range(half)]\n        b11 = [[0] * half for i in range(half)]\n        b12 = [[0] * half for i in range(half)]\n        b21 = [[0] * half for i in range(half)]\n        b22 = [[0] * half for i in range(half)]\n\n        for i in range(half):\n            for j in range(half):\n                a11[i][j] = a[i][j]\n                a12[i][j] = a[i][j + half]\n                a21[i][j] = a[i + half][j]\n                a22[i][j] = a[i + half][j + half]\n                b11[i][j] = b[i][j]\n                b12[i][j] = b[i][j + half]\n                b21[i][j] = b[i + half][j]\n                b22[i][j] = b[i + half][j + half]\n\n        s1 = subtract(b12, b22)\n        s2 = add(a11, a12)\n        s3 = add(a21, a22)\n        s4 = subtract(b21, b11)\n        s5 = add(a11, a22)\n        s6 = add(b11, b22)\n        s7 = subtract(a12, a22)\n        s8 = add(b21, b22)\n        s9 = subtract(a11, a21)\n        s10 = add(b11, b12)\n\n        p1 = square_matrix_multiply_strassen_algorithm(a11, s1)\n        p2 = square_matrix_multiply_strassen_algorithm(s2, b22)\n        p3 = square_matrix_multiply_strassen_algorithm(s3, b11)\n        p4 = square_matrix_multiply_strassen_algorithm(a22, s4)\n        p5 = square_matrix_multiply_strassen_algorithm(s5, s6)\n        p6 = square_matrix_multiply_strassen_algorithm(s7, s8)\n        p7 = square_matrix_multiply_strassen_algorithm(s9, s10)\n\n        c11 = add(subtract(add(p5, p4), p2), p6)\n        c12 = add(p1, p2)\n        c21 = add(p3, p4)\n        c22 = subtract(subtract(add(p5, p1), p3), p7)\n\n        for i in range(half):\n            for j in range(half):\n                c[i][j] = c11[i][j]\n                c[i][j + half] = c12[i][j]\n                c[i + half][j] = c21[i][j]\n                c[i + half][j + half] = c22[i][j]\n\n    return c\n\n\ndef add(a, b):\n    n = len(a)\n    c = [[0] * n for i in range(n)]\n\n    for i in range(n):\n        for j in range(n):\n            c[i][j] = a[i][j] + b[i][j]\n\n    return c\n\n\ndef subtract(a, b):\n    n = len(a)\n    c = [[0] * n for i in range(n)]\n\n    for i in range(n):\n        for j in range(n):\n            c[i][j] = a[i][j] - b[i][j]\n\n    return c", 
            "title": "4.2-2"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-3", 
            "text": "If n is not an exact power of 2, then we can pad 0 to matrix to make it an exact power of 2. Let $k = \\lfloor \\lg{n} \\rfloor$, Let $m = 2^{k + 1}$, so we have a new m * m matrix. Thus, $T(m) = 7T(\\frac{m}{2}) + \\Theta(m^2)$. According to master method, we have $T(m) = \\Theta(m^{\\lg7})$. So there exist postive constants $c_1$, $c_2$ and $n_0$ such that $c_1m^{\\lg7} \\leq T(m) \\leq c_2m^{\\lg7} \\text{ for all } n \\geq n_0$.  We have $m^{\\lg7} = (2^{\\lfloor \\lg{n} \\rfloor + 1})^{\\lg7} \\leq (2^{\\lg{n} + 1})^{\\lg7} = 7n^{\\lg7}$. And $m^{\\lg7} = (2^{\\lfloor \\lg{n} \\rfloor + 1})^{\\lg7} \\geq (2^{\\lg{n} - 1 + 1})^{\\lg7} = n^{\\lg7}$. So $min(c_1, 1)n^{\\lg7} \\leq T(m) \\leq max(c_2, 7)n^{\\lg7} \\text{ for all } n \\geq n_0$. Thus $T(m) = \\Theta(n^{\\lg7})$, the resulting algorithm still runs in time $\\Theta(n^{\\lg7})$.", 
            "title": "4.2-3"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-4", 
            "text": "We have $T(n) = kT(\\frac{n}{3}) + \\Theta(n^2)$ with $b = 3, a = k$. If we want to run the algorithm in time $o(n^{\\lg7})$, then case 3 cannot apply, since $f(n) = \\Omega(n^{\\lg7})$.  If case 2 applies, then $f(n) = \\Theta(n^{\\log_3k})$, so $\\log_3k = 2$, but the algorithm runs in time $\\Theta(n^2\\lg{n})$.  So case 1 applies, the algorithm runs in time $\\Theta(n^{\\log_3k}) = \\Theta(n^{\\frac{\\lg{k}}{\\lg3}})$.  If we want the algorithm to runs in time $o(n^{\\lg7})$, then we have $\\log_3k   \\lg7, k   3^{\\lg7}$, so the largest k is 21.", 
            "title": "4.2-4"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-5", 
            "text": "We know the best running time of multiplying b * b matrices using a multiplications is $\\Theta(n^{\\log_ba})$. So $\\log_68{132464} = 2.7951284873613815$, $\\log_70{143640} = 2.795122689748337$, $\\log_72{155424} = 2.795147391093449$. So multiplying 72 * 72 matrices using 155,424 multiplications yields the best asymptotic running time.  Since $\\lg7 = 2.807354922057604$, so the new algorithm is faster than Strassen's algorithm.", 
            "title": "4.2-5"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-6", 
            "text": "We can partition A, B into k n * n matrices, and partition C into $k^2$ n * n matrices:  $$\nA = \\begin{pmatrix}\n    A_{11} \\\\\n    A_{21} \\\\\n    \\ldots \\\\\n    A_{k1}\n\\end{pmatrix},\nB = \\begin{pmatrix}\n    B_{11}   B_{12}   \\ldots   B_{1k}\n\\end{pmatrix},\nC = \\begin{pmatrix}\n    C_{11}   C_{12}   \\ldots   C_{1k} \\\\\n    \\ldots \\\\\n    C_{k1}   C_{k2}   \\ldots   C_{kk}\n\\end{pmatrix}\n$$  So that we rewrite the equation C = A * B as:  $$\n\\begin{pmatrix}\n    C_{11}   C_{12}   \\ldots   C_{1k} \\\\\n    \\ldots \\\\\n    C_{k1}   C_{k2}   \\ldots   C_{kk}\n\\end{pmatrix} =\n\\begin{pmatrix}\n    A_{11} \\\\\n    A_{21} \\\\\n    \\ldots \\\\\n    A_{k1}\n\\end{pmatrix} * \n\\begin{pmatrix}\n    B_{11}   B_{12}   \\ldots   B_{1k}\n\\end{pmatrix},\nC_{ij} = A_{i1} * B_{1j}\n$$  Then we can use Strassen's algorithm as a subroutine to calculate $C_{ij}$.  let C be a new kn * kn matrix\nfor i = 1 to k\n    for j = 1 to k\n        Cij = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(Ai1 * B1j)\nreturn C  If the input matrices are reversed, we can partition A, B into k n * n matrices:  $$\nA = \\begin{pmatrix}\n    A_{11}   A_{12}   \\ldots   A_{1k}\n\\end{pmatrix},\nB = \\begin{pmatrix}\n    B_{11} \\\\\n    B_{21} \\\\\n    \\ldots \\\\\n    B_{k1}\n\\end{pmatrix}\n$$  So that we rewrite the equation C = A * B as:  $$\nC =\n\\begin{pmatrix}\n    A_{11}   A_{12}   \\ldots   A_{1k}\n\\end{pmatrix} * \n\\begin{pmatrix}\n    B_{11} \\\\\n    B_{21} \\\\\n    \\ldots \\\\\n    B_{k1}\n\\end{pmatrix}\n$$  Then we can use Strassen's algorithm as a subroutine to calculate C.  let C be a new n * n matrix\nfor i = 1 to k\n    C11 += SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A1i * Bi1)\nreturn C", 
            "title": "4.2-6"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-7", 
            "text": "MULTIPLY-COMPLEX-NUMBERS(a, b, c, d)\n\nn1 = (a + b) * (c + d)\nn2 = a * c\nn3 = b * d\nreturn (n2 - n3, n1 - n2 - n3)", 
            "title": "4.2-7"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/", 
            "text": "4.3 The substitution method for solving recurrences\n\n\n4.3-1\n\n\nWe start by assuming that this bound holds for all positive m \n n, in particular for m = n - 1, yielding $T(n - 1) \\leq c(n - 1)^2$. Substituting into the recurrence yields:\n\n\n$$T(n) = T(n - 1) + n \\leq c(n - 1)^2 + n = cn^2 + (1 - 2c)n + c \\leq cn^2$$\n\n\nwhere the last step holds as long as $c \n \\frac{1}{2}$ and $n \\geq \\frac{c}{2c - 1}$.\n\n\n4.3-2\n\n\nWe start by assuming that $T(n) \\leq c\\lg{n}$ holds for all positive m \n n, in particular for $m = \\lceil \\frac{n}{2} \\rceil$, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\leq c\\lg{\\lceil \\frac{n}{2} \\rceil}$. Substituting ino the recurrence yields:\n\n\n$$T(n) = T(\\lceil \\frac{n}{2} \\rceil) + 1 \\leq c\\lg{\\lceil \\frac{n}{2} \\rceil} + 1 \\leq c(\\lg{(\\frac{n + (2 - 1)}{2})}) + 1 \\text{ (inequation 3.6) } = c\\lg{(n + 1)} - c + 1$$\n\n\nBut it's not easy to prove that $c\\lg{(n + 1)} - c + 1 \\leq c\\lg{n}$, so we reguess $T(n) = O(\\lg{(n - 1)})$, since if $T(n) = O(\\lg{(n - 1)})$, then it's obviously $O(\\lg{n})$. Substituting ino the recurrence yields:\n\n\n$$T(n) \\leq c\\lg{(\\lceil \\frac{n}{2} \\rceil -1)} + 1 \\leq c\\lg{(\\frac{n + (2 - 1)}{2} -1)} + 1 = c\\lg{(n - 1)} - c + 1 \\leq c\\lg{(n - 1)} \n c\\lg{n}$$\n\n\nwhere the last step holds as long as $c \\geq 1$.\n\n\n4.3-3\n\n\nWe start by assuming that $T(n) \\geq cn\\lg{n}$ holds for all positive m \n n, in particular for $m = \\lfloor \\frac{n}{2} \\rfloor$, yielding $T(\\lfloor \\frac{n}{2} \\rfloor) \\geq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields:\n\n\n$$T(n) = 2T(\\lfloor \\frac{n}{2} \\rfloor) + n \\geq 2c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor} + n \\geq cn\\lg(\\frac{n}{2}) + n = cn\\lg{n} + (1 - c)n \\geq cn\\lg{n}$$\n\n\nwhere the last step holds as long as $c \\leq 1$.\n\n\nSince $T(n) = O(n\\lg{n})$ and $T(n) = \\Omega(n\\lg{n})$, so $T(n) = \\Theta(n\\lg{n})$.\n\n\n4.3-4\n\n\nWe can choose $T(n) = O(n\\lg{n} + 1)$, then $T(n) \\leq cn\\lg{n} + 1$. It holds for the base case, since $c1\\lg{1} + 1 = 1 \\geq T(1)$.\n\n\n4.3-5\n\n\nFirst, let's prove $T(n) = O(n\\lg{n})$. We start by assuming that $T(n) \\leq cn\\lg{n}$ holds for all positive m \n n, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\leq c\\lceil \\frac{n}{2} \\rceil\\lg{\\lceil \\frac{n}{2} \\rceil}$ and $T(\\lfloor \\frac{n}{2} \\rfloor) \\leq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\\n\n\\leq\n c\\lceil \\frac{n}{2} \\rceil\\lg{\\lceil \\frac{n}{2} \\rceil} + c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor} + \\Theta(n) \\\\\n\n\\leq\n c(\\frac{n + (2 - 1)}{2})\\lg{(\\frac{n + (2 - 1)}{2})} + c(\\frac{n}{2})\\lg{(\\frac{n}{2})} + \\Theta(n) \\\\\n\n=\n c\\frac{n + 1}{2}\\lg{(n + 1)} + c\\frac{n}{2}\\lg{n} - cn + \\Theta(n) - \\frac{c}{2}\n\\end{eqnarray}\n$$\n\n\nIt's also not easy to prove that $T(n) \\leq cn\\lg{n}$, so we try to guess $T(n) = O((n - 1)\\lg{(n - 1)})$, so:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\\n\n\\leq\n c(\\lceil \\frac{n}{2} \\rceil - 1)\\lg{(\\lceil \\frac{n}{2} \\rceil - 1)} + c(\\lfloor \\frac{n}{2} \\rfloor - 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor - 1)} + \\Theta(n) \\\\\n\n\\leq\n c(\\frac{n + (2 - 1)}{2} - 1)\\lg{(\\frac{n + (2 - 1)}{2} - 1)} + c(\\frac{n}{2} - 1)\\lg{(\\frac{n}{2} - 1)} + \\Theta(n) \\\\\n\n=\n c\\frac{n - 1}{2}\\lg{(n - 1)} + c\\frac{n - 2}{2}\\lg{(n - 2)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\\n\n c\\frac{n - 1}{2}\\lg{(n - 1)} + c\\frac{n - 1}{2}\\lg{(n - 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\\n\n=\n c(n - 1)\\lg{(n - 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\\n\n\\leq\n c(n - 1)\\lg{(n - 1)} - cn + c_1n - \\frac{3c}{2} \\\\\n\n=\n c(n - 1)\\lg{(n - 1)} + (c_1 - c)n - \\frac{3c}{2} \\\\\n\n c(n - 1)\\lg{(n - 1)} \\\\\n\n cn\\lg{n}\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c \n c_1$ and $n \\geq \\frac{3c}{2(c - c_1)}$.\n\n\nThen let's prove $T(n) = \\Omega(n\\lg{n})$. We start by assuming that $T(n) \\geq c(n + 1)\\lg{(n + 1)}$ holds for all positive m \n n, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\geq c(\\lceil \\frac{n}{2} \\rceil + 1)\\lg{(\\lceil \\frac{n}{2} \\rceil + 1)}$ and $T(\\lfloor \\frac{n}{2} \\rfloor) \\geq c(\\lfloor \\frac{n}{2} \\rfloor + 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 1)}$. Substituting ino the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\\n\n\\geq\n c(\\lceil \\frac{n}{2} \\rceil + 1)\\lg{(\\lceil \\frac{n}{2} \\rceil + 1)} + c(\\lfloor \\frac{n}{2} \\rfloor + 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 1)} + \\Theta(n) \\\\\n\n\\geq\n c(\\frac{n}{2} + 1)\\lg{(\\frac{n}{2} + 1)} + c(\\frac{n - (2 - 1)}{2} + 1)\\lg{(\\frac{n - (2 - 1)}{2} + 1)} + \\Theta(n) \\\\\n\n=\n c\\frac{n + 2}{2}\\lg{(n + 2)} + c\\frac{n + 1}{2}\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\\n\n c\\frac{n + 1}{2}\\lg{(n + 1)} + c\\frac{n + 1}{2}\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\\n\n=\n c(n + 1)\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\\n\n\\geq\n c(n + 1)\\lg{(n + 1)} - cn + c_1n - \\frac{3c}{2} \\\\\n\n=\n c(n + 1)\\lg{(n + 1)} + (c_1 - c)n - \\frac{3c}{2} \\\\\n\n c(n + 1)\\lg{(n + 1)} \\\\\n\n cn\\lg{n}\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c \n c_1$ and $n \\geq \\frac{3c}{2(c_1 - c)} $.\n\n\nSo $T(n) = \\Theta(n\\lg{n})$.\n\n\n4.3-6\n\n\nWe start by assuming that $T(n) \\leq cn\\lg{n}$ holds for all positive m \n n, in particular for $m = \\lfloor \\frac{n}{2} \\rfloor$, yielding $T(\\lfloor \\frac{n}{2} \\rfloor) \\leq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 2T(\\lfloor \\frac{n}{2} \\rfloor + 17) + n \\\\\n\n\\leq\n 2c(\\lfloor \\frac{n}{2} \\rfloor + 17)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 17)} + n \\\\\n\n\\leq\n 2c(\\frac{n}{2} + 17)\\lg{(\\frac{n}{2} + 17)} + n \\\\\n\n=\n c(n + 34)\\lg{(n + 34)} + (1 - c)n - 34c\n\\end{eqnarray}\n$$\n\n\nIt's not easy to prove $T(n) \\leq cn\\lg{n}$. So we try to guess $T(n) = O((n - k)\\lg{(n - k)})$. But we don't know what k is now, substituting into the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 2T(\\lfloor \\frac{n}{2} \\rfloor + 17) + n \\\\\n\n\\leq\n 2c(\\lfloor \\frac{n}{2} \\rfloor + 17 - k)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 17 - k)} + n \\\\\n\n\\leq\n 2c(\\frac{n}{2} + 17 - k)\\lg{(\\frac{n}{2} + 17 - k)} + n \\\\\n\n=\n c(n + 34 - 2k)\\lg{(n + 34 - 2k)} + (1 - c)n + (2k - 34)c\n\\end{eqnarray}\n$$\n\n\nLet 34 - 2k = -k, we get k = 34. So $T(n) \\leq c(n - 34)\\lg{(n - 34)} + (1 - c)n + 34c \n c(n - 34)\\lg{(n - 34)} \n cn\\lg{n}$ where the last step holds as long as $c \n 1$ and $n \\geq \\frac{34c}{c - 1}$.\n\n\nSo $T(n) = O(n\\lg{n})$.\n\n\n4.3-7\n\n\nHere, we have a = 4, b = 3, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{4}}$. Since $n^{\\log_3{4}}$ is polynomially larger than f(n) (that is, $f(n) = O(n^{\\log_3{4} - \\epsilon})$ for $\\epsilon \\approx 0.2618595071429148$), case 1 applies, and $T(n) = \\Theta(n^{\\log_3{4}})$.\n\n\nWe start by assuming that this bound holds for all positive m \n n, in particular for $m = \\frac{n}{3}$, yielding $T(\\frac{n}{3}) \\leq c(\\frac{n}{3})^{\\log_3{4}}$. Substituting ino the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 4T(\\frac{n}{3}) + n \\\\\n\n\\leq\n 4c(\\frac{n}{3})^{\\log_3{4}} + n \\\\\n\n=\n cn^{\\log_3{4}} + n\n\\end{eqnarray}\n$$\n\n\nSo it fails, we cannot prove $T(n) \\leq cn^{\\log_3{4}}$.\n\n\nThen let's subtract n from our original guess, let's guess $T(n) \\leq cn^{\\log_3{4}} - n$. Substituting ino the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 4T(\\frac{n}{3}) + n \\\\\n\n\\leq\n 4(c(\\frac{n}{3})^{\\log_3{4}} - \\frac{n}{3}) + n \\\\\n\n=\n cn^{\\log_3{4}} -\\frac{n}{3} \\\\\n\n cn^{\\log_3{4}}\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds for all n.\n\n\n4.3-8\n\n\nHere, we have a = 4, b = 2, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{4}} = n^2$. So case 2 applies, $T(n) = n^2\\lg{n}$. But it says it's $\\Theta(n^2)$ in the book, so I think it's an error here. If we want to keep the solution to $\\Theta(n^2)$, then f(n) should be $n$, not $n^2$. So $T(n) = 4T(\\frac{n}{2}) + n$. Since $n^2$ is polynomially larger than f(n) (that is, $f(n) = O(n^{2 - \\epsilon})$ for $\\epsilon = \\frac{1}{2})$, case 1 applies, and $T(n) = \\Theta(n^2)$.\n\n\nWe start by assuming that this bound holds for all positive m \n n, in particular for $m = \\frac{n}{2}$, yielding $T(\\frac{n}{2}) \\leq c(\\frac{n}{2})^2$. Substituting ino the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 4T(\\frac{n}{2}) + n \\\\\n\n\\leq\n 4c(\\frac{n}{2})^2 + n \\\\\n\n=\n cn^2 + n\n\\end{eqnarray}\n$$\n\n\nSo we cannot prove $T(n) \\leq cn^2$.\n\n\nThen let's subtract n from our original guess, let's guess $T(n) \\leq cn^2 - n$. Substituting ino the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 4T(\\frac{n}{2}) + n \\\\\n\n\\leq\n 4(c(\\frac{n}{2})^2 - \\frac{n}{2}) + n \\\\\n\n=\n cn^2 -n \\\\\n\n cn^2\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds for all n.\n\n\n4.3-9\n\n\nRenaming $m = \\lg{n}$ yields $T(2^m) = 3T(2^{\\frac{m}{2}}) + m$. We can new rename $S(m) = T(2^m)$ to produce the new recurrence $S(m) = 3S(\\frac{m}{2}) + m$.\n\n\nWe start by assuming that $S(n) \\leq cm\\lg{m}$ holds for all positive p \n m, in particular for $p = \\frac{m}{2}$, yielding $S(\\frac{m}{2}) \\leq c\\frac{m}{2}\\lg{\\frac{m}{2}}$. Substituting ino the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nS(m) \n=\n 3S(\\frac{m}{2}) + m \\\\\n\n\\leq\n 3c\\frac{m}{2}\\lg{\\frac{m}{2}} + m \\\\\n\n=\n \\frac{3c}{2}m\\lg{m} + (1 - \\frac{3c}{2})m\n\\end{eqnarray}\n$$\n\n\nIt's not easy to prove $\\frac{3c}{2}m\\lg{m} + (1 - \\frac{3c}{2})m \\leq cm\\lg{m}$, so let's try to guess $S(m) \\leq c\\frac{2m}{3}\\lg{m}$. So:\n\n\n$$\n\\begin{eqnarray}\nS(m) \n\\leq\n 3c\\frac{m}{3}\\lg{\\frac{m}{2}} + m \\\\\n\n=\n cm\\lg{m} + (1 - c)m \\\\\n\n\\leq\n cm\\lg{m} \n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c \\geq 1$.\n\n\nSo $S(m) = O(m\\lg{m})$, so $T(n) = O(\\lg{n}\\lg{\\lg{n}})$.", 
            "title": "4.3 The substitution method for solving recurrences"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-the-substitution-method-for-solving-recurrences", 
            "text": "", 
            "title": "4.3 The substitution method for solving recurrences"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-1", 
            "text": "We start by assuming that this bound holds for all positive m   n, in particular for m = n - 1, yielding $T(n - 1) \\leq c(n - 1)^2$. Substituting into the recurrence yields:  $$T(n) = T(n - 1) + n \\leq c(n - 1)^2 + n = cn^2 + (1 - 2c)n + c \\leq cn^2$$  where the last step holds as long as $c   \\frac{1}{2}$ and $n \\geq \\frac{c}{2c - 1}$.", 
            "title": "4.3-1"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-2", 
            "text": "We start by assuming that $T(n) \\leq c\\lg{n}$ holds for all positive m   n, in particular for $m = \\lceil \\frac{n}{2} \\rceil$, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\leq c\\lg{\\lceil \\frac{n}{2} \\rceil}$. Substituting ino the recurrence yields:  $$T(n) = T(\\lceil \\frac{n}{2} \\rceil) + 1 \\leq c\\lg{\\lceil \\frac{n}{2} \\rceil} + 1 \\leq c(\\lg{(\\frac{n + (2 - 1)}{2})}) + 1 \\text{ (inequation 3.6) } = c\\lg{(n + 1)} - c + 1$$  But it's not easy to prove that $c\\lg{(n + 1)} - c + 1 \\leq c\\lg{n}$, so we reguess $T(n) = O(\\lg{(n - 1)})$, since if $T(n) = O(\\lg{(n - 1)})$, then it's obviously $O(\\lg{n})$. Substituting ino the recurrence yields:  $$T(n) \\leq c\\lg{(\\lceil \\frac{n}{2} \\rceil -1)} + 1 \\leq c\\lg{(\\frac{n + (2 - 1)}{2} -1)} + 1 = c\\lg{(n - 1)} - c + 1 \\leq c\\lg{(n - 1)}   c\\lg{n}$$  where the last step holds as long as $c \\geq 1$.", 
            "title": "4.3-2"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-3", 
            "text": "We start by assuming that $T(n) \\geq cn\\lg{n}$ holds for all positive m   n, in particular for $m = \\lfloor \\frac{n}{2} \\rfloor$, yielding $T(\\lfloor \\frac{n}{2} \\rfloor) \\geq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields:  $$T(n) = 2T(\\lfloor \\frac{n}{2} \\rfloor) + n \\geq 2c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor} + n \\geq cn\\lg(\\frac{n}{2}) + n = cn\\lg{n} + (1 - c)n \\geq cn\\lg{n}$$  where the last step holds as long as $c \\leq 1$.  Since $T(n) = O(n\\lg{n})$ and $T(n) = \\Omega(n\\lg{n})$, so $T(n) = \\Theta(n\\lg{n})$.", 
            "title": "4.3-3"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-4", 
            "text": "We can choose $T(n) = O(n\\lg{n} + 1)$, then $T(n) \\leq cn\\lg{n} + 1$. It holds for the base case, since $c1\\lg{1} + 1 = 1 \\geq T(1)$.", 
            "title": "4.3-4"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-5", 
            "text": "First, let's prove $T(n) = O(n\\lg{n})$. We start by assuming that $T(n) \\leq cn\\lg{n}$ holds for all positive m   n, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\leq c\\lceil \\frac{n}{2} \\rceil\\lg{\\lceil \\frac{n}{2} \\rceil}$ and $T(\\lfloor \\frac{n}{2} \\rfloor) \\leq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\ \\leq  c\\lceil \\frac{n}{2} \\rceil\\lg{\\lceil \\frac{n}{2} \\rceil} + c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor} + \\Theta(n) \\\\ \\leq  c(\\frac{n + (2 - 1)}{2})\\lg{(\\frac{n + (2 - 1)}{2})} + c(\\frac{n}{2})\\lg{(\\frac{n}{2})} + \\Theta(n) \\\\ =  c\\frac{n + 1}{2}\\lg{(n + 1)} + c\\frac{n}{2}\\lg{n} - cn + \\Theta(n) - \\frac{c}{2}\n\\end{eqnarray}\n$$  It's also not easy to prove that $T(n) \\leq cn\\lg{n}$, so we try to guess $T(n) = O((n - 1)\\lg{(n - 1)})$, so:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\ \\leq  c(\\lceil \\frac{n}{2} \\rceil - 1)\\lg{(\\lceil \\frac{n}{2} \\rceil - 1)} + c(\\lfloor \\frac{n}{2} \\rfloor - 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor - 1)} + \\Theta(n) \\\\ \\leq  c(\\frac{n + (2 - 1)}{2} - 1)\\lg{(\\frac{n + (2 - 1)}{2} - 1)} + c(\\frac{n}{2} - 1)\\lg{(\\frac{n}{2} - 1)} + \\Theta(n) \\\\ =  c\\frac{n - 1}{2}\\lg{(n - 1)} + c\\frac{n - 2}{2}\\lg{(n - 2)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\  c\\frac{n - 1}{2}\\lg{(n - 1)} + c\\frac{n - 1}{2}\\lg{(n - 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ =  c(n - 1)\\lg{(n - 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ \\leq  c(n - 1)\\lg{(n - 1)} - cn + c_1n - \\frac{3c}{2} \\\\ =  c(n - 1)\\lg{(n - 1)} + (c_1 - c)n - \\frac{3c}{2} \\\\  c(n - 1)\\lg{(n - 1)} \\\\  cn\\lg{n}\n\\end{eqnarray}\n$$  where the last step holds as long as $c   c_1$ and $n \\geq \\frac{3c}{2(c - c_1)}$.  Then let's prove $T(n) = \\Omega(n\\lg{n})$. We start by assuming that $T(n) \\geq c(n + 1)\\lg{(n + 1)}$ holds for all positive m   n, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\geq c(\\lceil \\frac{n}{2} \\rceil + 1)\\lg{(\\lceil \\frac{n}{2} \\rceil + 1)}$ and $T(\\lfloor \\frac{n}{2} \\rfloor) \\geq c(\\lfloor \\frac{n}{2} \\rfloor + 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 1)}$. Substituting ino the recurrence yields:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\ \\geq  c(\\lceil \\frac{n}{2} \\rceil + 1)\\lg{(\\lceil \\frac{n}{2} \\rceil + 1)} + c(\\lfloor \\frac{n}{2} \\rfloor + 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 1)} + \\Theta(n) \\\\ \\geq  c(\\frac{n}{2} + 1)\\lg{(\\frac{n}{2} + 1)} + c(\\frac{n - (2 - 1)}{2} + 1)\\lg{(\\frac{n - (2 - 1)}{2} + 1)} + \\Theta(n) \\\\ =  c\\frac{n + 2}{2}\\lg{(n + 2)} + c\\frac{n + 1}{2}\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\  c\\frac{n + 1}{2}\\lg{(n + 1)} + c\\frac{n + 1}{2}\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ =  c(n + 1)\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ \\geq  c(n + 1)\\lg{(n + 1)} - cn + c_1n - \\frac{3c}{2} \\\\ =  c(n + 1)\\lg{(n + 1)} + (c_1 - c)n - \\frac{3c}{2} \\\\  c(n + 1)\\lg{(n + 1)} \\\\  cn\\lg{n}\n\\end{eqnarray}\n$$  where the last step holds as long as $c   c_1$ and $n \\geq \\frac{3c}{2(c_1 - c)} $.  So $T(n) = \\Theta(n\\lg{n})$.", 
            "title": "4.3-5"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-6", 
            "text": "We start by assuming that $T(n) \\leq cn\\lg{n}$ holds for all positive m   n, in particular for $m = \\lfloor \\frac{n}{2} \\rfloor$, yielding $T(\\lfloor \\frac{n}{2} \\rfloor) \\leq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields:  $$\n\\begin{eqnarray}\nT(n)  =  2T(\\lfloor \\frac{n}{2} \\rfloor + 17) + n \\\\ \\leq  2c(\\lfloor \\frac{n}{2} \\rfloor + 17)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 17)} + n \\\\ \\leq  2c(\\frac{n}{2} + 17)\\lg{(\\frac{n}{2} + 17)} + n \\\\ =  c(n + 34)\\lg{(n + 34)} + (1 - c)n - 34c\n\\end{eqnarray}\n$$  It's not easy to prove $T(n) \\leq cn\\lg{n}$. So we try to guess $T(n) = O((n - k)\\lg{(n - k)})$. But we don't know what k is now, substituting into the recurrence yields:  $$\n\\begin{eqnarray}\nT(n)  =  2T(\\lfloor \\frac{n}{2} \\rfloor + 17) + n \\\\ \\leq  2c(\\lfloor \\frac{n}{2} \\rfloor + 17 - k)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 17 - k)} + n \\\\ \\leq  2c(\\frac{n}{2} + 17 - k)\\lg{(\\frac{n}{2} + 17 - k)} + n \\\\ =  c(n + 34 - 2k)\\lg{(n + 34 - 2k)} + (1 - c)n + (2k - 34)c\n\\end{eqnarray}\n$$  Let 34 - 2k = -k, we get k = 34. So $T(n) \\leq c(n - 34)\\lg{(n - 34)} + (1 - c)n + 34c   c(n - 34)\\lg{(n - 34)}   cn\\lg{n}$ where the last step holds as long as $c   1$ and $n \\geq \\frac{34c}{c - 1}$.  So $T(n) = O(n\\lg{n})$.", 
            "title": "4.3-6"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-7", 
            "text": "Here, we have a = 4, b = 3, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{4}}$. Since $n^{\\log_3{4}}$ is polynomially larger than f(n) (that is, $f(n) = O(n^{\\log_3{4} - \\epsilon})$ for $\\epsilon \\approx 0.2618595071429148$), case 1 applies, and $T(n) = \\Theta(n^{\\log_3{4}})$.  We start by assuming that this bound holds for all positive m   n, in particular for $m = \\frac{n}{3}$, yielding $T(\\frac{n}{3}) \\leq c(\\frac{n}{3})^{\\log_3{4}}$. Substituting ino the recurrence yields:  $$\n\\begin{eqnarray}\nT(n)  =  4T(\\frac{n}{3}) + n \\\\ \\leq  4c(\\frac{n}{3})^{\\log_3{4}} + n \\\\ =  cn^{\\log_3{4}} + n\n\\end{eqnarray}\n$$  So it fails, we cannot prove $T(n) \\leq cn^{\\log_3{4}}$.  Then let's subtract n from our original guess, let's guess $T(n) \\leq cn^{\\log_3{4}} - n$. Substituting ino the recurrence yields:  $$\n\\begin{eqnarray}\nT(n)  =  4T(\\frac{n}{3}) + n \\\\ \\leq  4(c(\\frac{n}{3})^{\\log_3{4}} - \\frac{n}{3}) + n \\\\ =  cn^{\\log_3{4}} -\\frac{n}{3} \\\\  cn^{\\log_3{4}}\n\\end{eqnarray}\n$$  where the last step holds for all n.", 
            "title": "4.3-7"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-8", 
            "text": "Here, we have a = 4, b = 2, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{4}} = n^2$. So case 2 applies, $T(n) = n^2\\lg{n}$. But it says it's $\\Theta(n^2)$ in the book, so I think it's an error here. If we want to keep the solution to $\\Theta(n^2)$, then f(n) should be $n$, not $n^2$. So $T(n) = 4T(\\frac{n}{2}) + n$. Since $n^2$ is polynomially larger than f(n) (that is, $f(n) = O(n^{2 - \\epsilon})$ for $\\epsilon = \\frac{1}{2})$, case 1 applies, and $T(n) = \\Theta(n^2)$.  We start by assuming that this bound holds for all positive m   n, in particular for $m = \\frac{n}{2}$, yielding $T(\\frac{n}{2}) \\leq c(\\frac{n}{2})^2$. Substituting ino the recurrence yields:  $$\n\\begin{eqnarray}\nT(n)  =  4T(\\frac{n}{2}) + n \\\\ \\leq  4c(\\frac{n}{2})^2 + n \\\\ =  cn^2 + n\n\\end{eqnarray}\n$$  So we cannot prove $T(n) \\leq cn^2$.  Then let's subtract n from our original guess, let's guess $T(n) \\leq cn^2 - n$. Substituting ino the recurrence yields:  $$\n\\begin{eqnarray}\nT(n)  =  4T(\\frac{n}{2}) + n \\\\ \\leq  4(c(\\frac{n}{2})^2 - \\frac{n}{2}) + n \\\\ =  cn^2 -n \\\\  cn^2\n\\end{eqnarray}\n$$  where the last step holds for all n.", 
            "title": "4.3-8"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-9", 
            "text": "Renaming $m = \\lg{n}$ yields $T(2^m) = 3T(2^{\\frac{m}{2}}) + m$. We can new rename $S(m) = T(2^m)$ to produce the new recurrence $S(m) = 3S(\\frac{m}{2}) + m$.  We start by assuming that $S(n) \\leq cm\\lg{m}$ holds for all positive p   m, in particular for $p = \\frac{m}{2}$, yielding $S(\\frac{m}{2}) \\leq c\\frac{m}{2}\\lg{\\frac{m}{2}}$. Substituting ino the recurrence yields:  $$\n\\begin{eqnarray}\nS(m)  =  3S(\\frac{m}{2}) + m \\\\ \\leq  3c\\frac{m}{2}\\lg{\\frac{m}{2}} + m \\\\ =  \\frac{3c}{2}m\\lg{m} + (1 - \\frac{3c}{2})m\n\\end{eqnarray}\n$$  It's not easy to prove $\\frac{3c}{2}m\\lg{m} + (1 - \\frac{3c}{2})m \\leq cm\\lg{m}$, so let's try to guess $S(m) \\leq c\\frac{2m}{3}\\lg{m}$. So:  $$\n\\begin{eqnarray}\nS(m)  \\leq  3c\\frac{m}{3}\\lg{\\frac{m}{2}} + m \\\\ =  cm\\lg{m} + (1 - c)m \\\\ \\leq  cm\\lg{m} \n\\end{eqnarray}\n$$  where the last step holds as long as $c \\geq 1$.  So $S(m) = O(m\\lg{m})$, so $T(n) = O(\\lg{n}\\lg{\\lg{n}})$.", 
            "title": "4.3-9"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/", 
            "text": "4.4 The recursion-tree method for solving recurrences\n\n\n4.4-1\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = 3T(\\frac{n}{2}) + n$ and assume that n is an exact power of 2.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=24mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){n}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=4 of root] {n}[no edge from this parent]\n    child {node {$\\frac{3n}{2}$}[no edge from this parent]\n        child {node {$(\\frac{3}{2})^2n$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(n^{\\lg{3}})$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nEach level has three times more nodes than the level above, so the number of nodes at depth i is $3^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n}{2^i}$. So the total cost over all nodes at depth i, is $3^i\\frac{n}{2^i} = (\\frac{3}{2})^in$. The bottom level, at depth $\\lg{n}$, has $3^{\\lg{n}} = n^{\\lg{3}}$ nodes, each contributing cost $T(1)$, for a total cost of $n^{\\lg{3}}T(1)$, which is $\\Theta(n^{\\lg{3}})$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sum_{i = 0}^{\\lg{n} - 1}(\\frac{3}{2})^in + \\Theta(n^{\\lg{3}}) \\\\\n\n=\n n\\frac{1 - (\\frac{3}{2})^{\\lg{n}}}{1 - \\frac{3}{2}} + \\Theta(n^{\\lg3}) \\\\\n\n=\n 2n\\frac{3^{\\lg{n}} - 2^{\\lg{n}}}{2^{\\lg{n}}} + \\Theta(n^{\\lg3}) \\\\\n\n=\n 2n\\frac{3^{\\lg{n}} - 2^{\\lg{n}}}{n} + \\Theta(n^{\\lg3}) \\\\\n\n=\n 2(3^{\\lg{n}} - 2^{\\lg{n}}) + \\Theta(n^{\\lg3}) \\\\\n\n=\n 2(n^{\\lg{3}} -n) + \\Theta(n^{\\lg3}) \\\\\n\n 2n^{\\lg3} + \\Theta(n^{\\lg3}) \\\\\n\n=\n O(n^{\\lg3})\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = O(n^{\\lg3})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^{\\lg3}$ for some constant $c \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 3T(\\lfloor \\frac{n}{2} \\rfloor) + n \\\\\n\n\\leq\n 3c\\lfloor \\frac{n}{2} \\rfloor^{\\lg3} + n \\\\\n\n\\leq\n 3c(\\frac{n}{2})^{\\lg3} + n \\\\\n\n=\n 3c(\\frac{n}{2})^{\\lg3} + n \\\\\n\n=\n 3c\\frac{n^{\\lg3}}{2^{\\lg3}} + n \\\\\n\n=\n 3c\\frac{n^{\\lg3}}{3} + n \\\\\n\n=\n cn^{\\lg{3}} + n\n\\end{eqnarray}\n$$\n\n\nBut $cn^{\\lg{3}} + n \n cn^{\\lg{3}}$, so we need to try another guess. Let's try $T(n) \\leq cn^{\\lg3} - \\frac{2n}{3}$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 3T(\\lfloor \\frac{n}{2} \\rfloor) + n \\\\\n\n\\leq\n 3(c\\lfloor \\frac{n}{2} \\rfloor^{\\lg3} - \\frac{n}{3}) + n \\\\\n\n\\leq\n 3(c(\\frac{n}{2})^{\\lg3} - \\frac{n}{3}) + n \\\\\n\n=\n 3c(\\frac{n}{2})^{\\lg3} \\\\\n\n=\n cn^{\\lg3}\n\\end{eqnarray}\n$$\n\n\n4.4-2\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{2}) + n^2$ and assume that n is an exact power of 2.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$n^2$}\n    child {node {$\\frac{n^2}{4}$}\n        child {node {$\\frac{n^2}{16}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$n^2$}[no edge from this parent]\n    child {node {$\\frac{n^2}{4}$}[no edge from this parent]\n        child {node {$(\\frac{1}{4})^2n$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nThe number of nodes at depth i is 1. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n^2}{4^i}$. So the total cost over all nodes at depth i, is $\\frac{n^2}{4^i}$. The bottom level, at depth $\\lg{n}$, has 1 node, which contributing cost $T(1)$, for a total cost of $T(1)$, which is $\\Theta(1)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sum_{i = 0}^{\\lg{n} - 1}\\frac{n^2}{4^i} + \\Theta(1) \\\\\n\n \\sum_{i = 0}^{\\infty}\\frac{n^2}{4^i} + \\Theta(1) \\\\\n\n=\n \\frac{1}{1 - \\frac{1}{4}}n^2 + \\Theta(1) \\\\\n\n=\n \\frac{4}{3}n^2 + \\Theta(1) \\\\\n\n=\n O(n^2)\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^2$ for some constant $c \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\frac{n}{2}) + n^2 \\\\\n\n\\leq\n c(\\frac{n}{2})^2 + n^2 \\\\\n\n=\n (\\frac{c}{4} + 1)n^2 \\\\\n\n\\leq\n cn^2\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c \\geq \\frac{4}{3}$.\n\n\n4.4-3\n\n\nWhen n is large, the difference between $\\frac{n}{2} + 2$ and $\\frac{n}{2}$ not that large, so it's also a sloppiness that we can tolerate. Then let's create a recursion tree for the recurrence $T(n) = 4T(\\frac{n}{2}) + n$ and assume that n is an exact power of 2.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=32mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){n}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=7 of root] {n}[no edge from this parent]\n    child {node {2n}[no edge from this parent]\n        child {node {4n}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(n^2)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nEach level has four times more nodes than the level above, so the number of nodes at depth i is $4^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n}{2^i}$. So the total cost over all nodes at depth i, is $4^i\\frac{n}{2^i} = 2^in$. The bottom level, at depth $\\lg{n}$, has $4^{\\lg{n}} = n^2$ nodes, each contributing cost $T(1)$, for a total cost of $n^2T(1)$, which is $\\Theta(n^2)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sum_{i = 0}^{\\lg{n} - 1}2^in + \\Theta(n^2) \\\\\n\n=\n n\\frac{1 - 2^{\\lg{n}}}{1 - 2} + \\Theta(n^2) \\\\\n\n=\n n(n - 1) + \\Theta(n^2) \\\\\n\n=\n n^2 - n + \\Theta(n^2) \\\\\n\n=\n O(n^2)\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^2$ for some constant $c \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 4T(\\frac{n}{2} + 2) + n \\\\\n\n\\leq\n 4c(\\frac{n}{2} + 2)^2 + n \\\\\n\n=\n 4c(\\frac{n^2}{4} + 2n + 4) + n \\\\\n\n=\n cn^2 + 8cn + 16c + n \\\\\n\n=\n cn^2 + (8c + 1)n + 16c\n\\end{eqnarray}\n$$\n\n\nBut $cn^2 + (8c + 1)n + 16c \n cn^2$, so we need to try another guess, let's try $T(n) \\leq cn^2 - 5n$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 4T(\\frac{n}{2} + 2) + n \\\\\n\n\\leq\n 4c((\\frac{n}{2} + 2)^2 - 5(\\frac{n}{2} + 2)) + n \\\\\n\n=\n 4c(\\frac{n^2}{4} + 2n + 4 - \\frac{5n}{2} - 10) + n \\\\\n\n=\n 4c(\\frac{n^2}{4} - \\frac{n}{2} - 6) + n \\\\\n\n=\n cn^2 + (1 - 2c)n - 24c \\\\\n\n cn^2\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c \\geq \\frac{1}{2}$.\n\n\n4.4-4\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = 2T(n - 1) + 1$.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=16mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){1}\n    child {node {1}\n        child {node {1}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {1}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {1}\n        child {node {1}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {1}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=2 of root] {1}[no edge from this parent]\n    child {node {2}[no edge from this parent]\n        child {node {4}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(2^n)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nEach level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, n - 1 - 1$, has a cost of 1. So the total cost over all nodes at depth i, is $2^i$. The bottom level, at depth n, has $2^{n - 1}$ nodes, each contributing cost $T(1)$, for a total cost of $2^{n - 1}T(1)$, which is $\\Theta(2^n)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sum_{i = 0}^{n - 1 - 1}2^i + \\Theta(2^n) \\\\\n\n=\n \\frac{1 - 2^{n - 1}}{1 - 2} + \\Theta(2^n) \\\\\n\n=\n 2^{n - 1} - 1 + \\Theta(2^n) \\\\\n\n=\n O(2^n)\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = O(2^n)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq c2^n$ for some constant $c \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 2T(n - 1) + 1 \\\\\n\n\\leq\n 2c2^{n - 1} + 1 \\\\\n\n=\n c2^n + 1\n\\end{eqnarray}\n$$\n\n\nBut $c2^n + 1 \n c2^n$, so we need to try another guess. Lets try $T(n) \\leq c2^n - 1$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 2T(n - 1) + 1 \\\\\n\n\\leq\n 2(c2^{n - 1} - 1) + 1 \\\\\n\n=\n c2^n -1 \\\\\n\n c2^n\n\\end{eqnarray}\n$$\n\n\n4.4-5\n\n\n4.4-6\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{3}) + T(\\frac{2n}{3})$ and assume that n is an exact power of 3.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=16mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){cn}\n    child {node {$c\\frac{n}{3}$}\n        child {node {$c\\frac{n}{9}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$c\\frac{2n}{9}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$c\\frac{2n}{3}$}\n        child {node {$c\\frac{2n}{9}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$c\\frac{4n}{9}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=2 of root] {cn}[no edge from this parent]\n    child {node {cn}[no edge from this parent]\n        child {node {cn}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(?)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nEach level has 2 times more nodes than the level above, so the number of nodes at depth i is $2^i$. But in this question, not all leaves reach at the bottom at the same time. The left most branch reaches the bottom first. So when the left most branch reaches the bottom, we assume other nodes also reach the bottom. Then the depth of recursion tree is $\\log_3{n}$.\n\n\nThe total cost over all nodes at depth i, is $cn$ and the cost at bottom is also cn So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\geq\n \\sum_{i = 0}^{\\log_3{n} - 1}cn + cn \\\\\n\n=\n cn\\log_3{n} + cn \\\\\n\n=\n \\frac{c}{\\lg3}n\\lg{n} + cn \\\\\n\n=\n \\Theta(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nBut remember we've cut some branches of the recursion tree, so $T(n) = \\Omega(n\\lg{n})$.\n\n\n4.4-7\n\n\nWhen n is large, the difference between $\\frac{n}{2}$ and $\\lfloor \\frac{n}{2} \\rfloor$ not that large, so it's also a sloppiness that we can tolerate. Then let's create a recursion tree for the recurrence $T(n) = 4T(\\frac{n}{2}) + cn$ and assume that n is an exact power of 2.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=32mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){cn}\n    child {node {$\\frac{cn}{2}$}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{cn}{2}$}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{cn}{2}$}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{cn}{2}$}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=7 of root] {cn}[no edge from this parent]\n    child {node {2cn}[no edge from this parent]\n        child {node {4cn}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(n^2)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nEach level has 4 times more nodes than the level above, so the number of nodes at depth i is $4^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{cn}{2^i}$. So the total cost over all nodes at depth i, is $4^i\\frac{cn}{2^i} = c2^in$. The bottom level, at depth $\\lg{n}$, has $4^{\\lg{n}} = n^2$ nodes, each contributing cost $T(1)$, for a total cost of $n^2T(1)$, which is $\\Theta(n^2)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sum_{i = 0}^{\\lg{n} - 1}c2^in + \\Theta(n^2) \\\\\n\n=\n cn\\frac{1 - 2^{\\lg{n}}}{1 - 2} + \\Theta(n^2) \\\\\n\n=\n \\Theta(n^2)\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq dn^2$ for some constant $d \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\\n\n\\leq\n 4T(\\frac{n}{2}) + cn \\\\\n\n\\leq\n 4d(\\frac{n}{2})^2 + cn \\\\\n\n=\n dn^2 + cn\n\\end{eqnarray}\n$$\n\n\nBut $dn^2 + cn \n dn^2$, so let's try another guess, let's try $T(n) \\leq dn^2 - dn$.\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\\n\n\\leq\n 4T(\\frac{n}{2}) + cn \\\\\n\n\\leq\n 4d((\\frac{n}{2})^2 - \\frac{n}{2}) + cn \\\\\n\n=\n dn^2 + (c - 2d)n \\\\\n\n\\leq\n dn^2\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $d \\geq \\frac{c}{2}$.\n\n\nSo $T(n) = O(n^2)$. Then we want to show that $T(n) \\geq dn^2$ for some constant $d \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\\n\n\\geq\n 4T(\\frac{n - (2 - 1)}{2}) + cn \\\\\n\n=\n 4T(\\frac{n - 1}{2}) + cn \\\\\n\n\\geq\n 4d(\\frac{n - 1}{2})^2 + cn \\\\\n\n=\n dn^2 + (c - 2d)n + d \\\\\n\n dn^2\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $d \n \\frac{c}{2}$.\n\n\nSo $T(n) = \\Omega(n^2)$, so $T(n) = \\Theta(n^2)$.\n\n\n4.4-8\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = T(n - a) + T(a) + cn$ and assume that n - 1 is divisible by a.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=14mm]\n\\tikzstyle{level 2}=[sibling distance=10mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){cn}\n    child {node {c(n - a)}\n        child {node {c(n - 2a)}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {ca}}}\n    child {node {ca}};\n\n\\node[right=1 of root] {cn}[no edge from this parent]\n    child {node {cn}[no edge from this parent]\n        child {node {cn - ca}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(a)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nThe total cost over all nodes at depth i, for $i = 0, 1, 2, \\ldots, \\frac{n - 1}{a} - 1$, is $c(n - (i - 1)a), \\text{ for } i \\geq 1$. The bottom level, at depth $\\frac{n - 1}{a}$, the total cost is $T(1) + ca$, which is $\\Theta(a)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n cn + \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}c(n - (i - 1)a) + \\Theta(a) \\\\\n\n=\n cn + \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}cn - \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}(i - 1)ca + \\Theta(a) \\\\\n\n=\n \\frac{n - 1}{a}cn - \\frac{n^2 - 2a -3na + 3a + 2a^2 + 1}{2a}c \\\\\n\n=\n \\frac{n^2 +3na - 3a - 2a^2 - 1}{2a}c \\\\\n\n=\n \\Theta(n^2)\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = \\Theta(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq dn^2$ for some constant $d \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(n - a) + T(a) + cn \\\\\n\n\\leq\n d(n - a)^2 + da^2 + cn \\\\\n\n=\n dn^2 + (c - ad)n + (2a^2 - an)d \\\\\n\n\\leq\n dn^2\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $d \\geq \\frac{c}{a}$ and $n \\geq 2a$.\n\n\nSo $T(n) = O(n^2)$.\n\n\nThen we want to show that $T(n) \\geq dn^2$ for some constant $d \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(n - a) + T(a) + cn \\\\\n\n\\geq\n d(n - a)^2 + da^2 + cn \\\\\n\n=\n dn^2 + (c - 2ad)n + 2a^2d \\\\\n\n dn^2\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $d \n \\frac{c}{2a}$.\n\n\nSo $T(n) = \\Omega(n^2)$. So $T(n) = \\Theta(n^2)$.\n\n\n4.4-9\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = T(\\alpha{n}) + T((1 - \\alpha)n) + cn$.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=38mm]\n\\tikzstyle{level 2}=[sibling distance=18mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){cn}\n    child {node {$c\\alpha{n}$}\n        child {node {$c\\alpha^2{n}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$c\\alpha(1 - \\alpha)n$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$c(1 - \\alpha)n$}\n        child {node {$c\\alpha(1 - \\alpha)n$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$c(1 - \\alpha)^2n$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=4 of root] {cn}[no edge from this parent]\n    child {node {cn}[no edge from this parent]\n        child {node {cn}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(?)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nSo we can see not each branch reaches at the bottom at the same time, it might be the left most branch reaches at the bottom first, or the right most branch reaches at the bottom first. It depends on which is smaller, $\\alpha$ or $1 - \\alpha$. Let $\\alpha \\leq 1 - \\alpha$, so we get $\\alpha \\leq \\frac{1}{2}$. That is, when $0 \n \\alpha \\leq \\frac{1}{2}$, the left most branch reaches at the bottom first, when $\\frac{1}{2} \n \\alpha \n 1$, the right most branch reaches at the bottom first.\n\n\nIf $0 \n \\alpha \\leq \\frac{1}{2}$, when the left most branch reaches at the bottom, let's assume other branches also reach at the bottom, so the total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{\\alpha}{\\frac{1}{n}} - 1$, is cn. The cost at the bottom is also cn. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\\n\n\\geq\n \\sum_{i = 0}^{\\log_{\\alpha}\\frac{1}{n} - 1}cn + cn \\\\\n\n=\n cn(\\log_{\\alpha}\\frac{1}{n}) + cn \\\\\n\n=\n cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} + cn \\\\\n\n cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} \\\\\n\n=\n \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = \\Omega(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq dn\\lg{n}$ for some constant $d \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\\n\n\\geq\n d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\\n\n=\n d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\\n\n=\n dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\\n\n\\geq\n  dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d\\alpha\\lg{\\alpha}n + cn \\, (0 \n \\alpha \\leq \\frac{1}{2}) \\\\\n\n=\n dn\\lg{n} + (c + 2d\\alpha\\lg{\\alpha})n \\\\\n\n\\geq\n dn\\lg{n}\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $d \\leq \\frac{c}{2\\alpha\\lg{\\frac{1}{\\alpha}}}$.\n\n\nNow let's check the right most branch. When it reaches at the bottom, other branches have already reached at the bottom, if the tree depth is k, let's assume other branches become T(1) at depth k - 1.\n\n\nEach level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And total cost over all nodes at depth i, for $i = 0, 1, 2, ..., \\log_{1 - \\alpha}\\frac{1}{n}$, is cn. The cost at the bottom is $T(1)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\\n\n\\leq\n \\sum_{i = 0}^{\\log_{1 - \\alpha}\\frac{1}{n} - 1}cn + \\Theta(1) \\\\\n\n=\n cn(\\log_{1 - \\alpha}\\frac{1}{n}) + \\Theta(1) \\\\\n\n=\n cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} + \\Theta(1) \\\\\n\n cn\\frac{\\lg{n}}{\\lg{\\frac{1}{1 - \\alpha}}} + n\\lg{n}\\\\\n\n=\n O(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = O(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to veify that our guess was correct. We want to show that $T(n) \\leq dn\\lg{n}$ for some constant $d \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\\n\n\\leq\n d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\\n\n=\n d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\\n\n=\n dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\\n\n\\leq\n  dn\\lg{n} + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\, (0 \n \\alpha \\leq \\frac{1}{2}) \\\\\n\n=\n dn\\lg{n} + (c + 2d(1 - \\alpha)\\lg{(1 - \\alpha)})n \\\\\n\n\\leq\n dn\\lg{n}\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $d \\geq \\frac{c}{2(1 - \\alpha)\\lg{\\frac{1}{1 - \\alpha}}}$.\n\n\nSo we proved $T(n) = \\Omega(n\\lg{n})$ and $T(n) = O(n\\lg{n})$. So $T(n) = \\Theta(n\\lg{n})$ when $0 \n \\alpha \\leq \\frac{1}{2}$.\n\n\nThen let's check another case, when $\\frac{1}{2} \n \\alpha \n 1$. In this case, the right most branch reaches at the bottom first. Let's assume other branches also reach at the bottom, so the total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{1 - \\alpha}\\frac{1}{n}$, is cn. The cost at the bottom is cn. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\\n\n\\geq\n \\sum_{i = 0}^{\\log_{1 - \\alpha}\\frac{1}{n} - 1}cn + cn \\\\\n\n=\n cn(\\log_{1 - \\alpha}\\frac{1}{n}) + cn \\\\\n\n=\n cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} + cn \\\\\n\n cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} \\\\\n\n=\n \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = \\Omega(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq dn\\lg{n}$ for some constant $d \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\\n\n\\geq\n d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\\n\n=\n d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\\n\n=\n dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\\n\n dn\\lg{n} + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\, (\\frac{1}{2} \n \\alpha \n 1) \\\\\n\n=\n dn\\lg{n} + (c + 2d(1 - \\alpha)\\lg{(1 - \\alpha)})n \\\\\n\n\\geq\n dn\\lg{n}\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $d \\leq \\frac{c}{2(1 - \\alpha)\\lg{\\frac{1}{1 - \\alpha}}}$.\n\n\nNow let's check the left most branch. When it reaches at the bottom, other branches have already reached at the bottom, if the tree depth is k, let's assume other branches become T(1) at depth k - 1.\n\n\nEach level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{\\alpha}\\frac{1}{n} - 1$, is cn. The cost at the bottom is T(1). So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\\n\n \\sum_{i = 0}^{\\log_{\\alpha}\\frac{1}{n} - 1}cn + \\Theta(1) \\\\\n\n=\n cn(\\log_{\\alpha}\\frac{1}{n}) + \\Theta(1) \\\\\n\n=\n cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{\\alpha}}} + \\Theta(1) \\\\\n\n cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} + n\\lg{n}\\\\\n\n=\n O(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = O(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to veify that our guess was correct. We want to show that $T(n) \\leq dn\\lg{n}$ for some constant $d \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\\n\n\\leq\n d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\\n\n=\n d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\\n\n=\n dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\\n\n  dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d\\alpha\\lg{\\alpha}n + cn \\, (\\frac{1}{2} \n \\alpha \n 1) \\\\\n\n=\n dn\\lg{n} + (c + 2d\\alpha\\lg{\\alpha})n \\\\\n\n\\leq\n dn\\lg{n}\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $d \\geq \\frac{c}{2\\alpha\\lg{\\frac{1}{\\alpha}}}$.\n\n\nSo we proved $T(n) = \\Omega(n\\lg{n})$ and $T(n) = O(n\\lg{n})$. So $T(n) = \\Theta(n\\lg{n})$ when $\\frac{1}{2} \n \\alpha \n 1$. Thus $T(n) = \\Theta(n\\lg{n})$ for all $0 \n \\alpha \n 1$.", 
            "title": "4.4 The recursion-tree method for solving recurrences"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-the-recursion-tree-method-for-solving-recurrences", 
            "text": "", 
            "title": "4.4 The recursion-tree method for solving recurrences"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-1", 
            "text": "First let's create a recursion tree for the recurrence $T(n) = 3T(\\frac{n}{2}) + n$ and assume that n is an exact power of 2.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=24mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){n}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=4 of root] {n}[no edge from this parent]\n    child {node {$\\frac{3n}{2}$}[no edge from this parent]\n        child {node {$(\\frac{3}{2})^2n$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(n^{\\lg{3}})$}}}}};\n\\end{tikzpicture}\n\\end{document}   Each level has three times more nodes than the level above, so the number of nodes at depth i is $3^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n}{2^i}$. So the total cost over all nodes at depth i, is $3^i\\frac{n}{2^i} = (\\frac{3}{2})^in$. The bottom level, at depth $\\lg{n}$, has $3^{\\lg{n}} = n^{\\lg{3}}$ nodes, each contributing cost $T(1)$, for a total cost of $n^{\\lg{3}}T(1)$, which is $\\Theta(n^{\\lg{3}})$. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sum_{i = 0}^{\\lg{n} - 1}(\\frac{3}{2})^in + \\Theta(n^{\\lg{3}}) \\\\ =  n\\frac{1 - (\\frac{3}{2})^{\\lg{n}}}{1 - \\frac{3}{2}} + \\Theta(n^{\\lg3}) \\\\ =  2n\\frac{3^{\\lg{n}} - 2^{\\lg{n}}}{2^{\\lg{n}}} + \\Theta(n^{\\lg3}) \\\\ =  2n\\frac{3^{\\lg{n}} - 2^{\\lg{n}}}{n} + \\Theta(n^{\\lg3}) \\\\ =  2(3^{\\lg{n}} - 2^{\\lg{n}}) + \\Theta(n^{\\lg3}) \\\\ =  2(n^{\\lg{3}} -n) + \\Theta(n^{\\lg3}) \\\\  2n^{\\lg3} + \\Theta(n^{\\lg3}) \\\\ =  O(n^{\\lg3})\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = O(n^{\\lg3})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^{\\lg3}$ for some constant $c   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  3T(\\lfloor \\frac{n}{2} \\rfloor) + n \\\\ \\leq  3c\\lfloor \\frac{n}{2} \\rfloor^{\\lg3} + n \\\\ \\leq  3c(\\frac{n}{2})^{\\lg3} + n \\\\ =  3c(\\frac{n}{2})^{\\lg3} + n \\\\ =  3c\\frac{n^{\\lg3}}{2^{\\lg3}} + n \\\\ =  3c\\frac{n^{\\lg3}}{3} + n \\\\ =  cn^{\\lg{3}} + n\n\\end{eqnarray}\n$$  But $cn^{\\lg{3}} + n   cn^{\\lg{3}}$, so we need to try another guess. Let's try $T(n) \\leq cn^{\\lg3} - \\frac{2n}{3}$. So:  $$\n\\begin{eqnarray}\nT(n)  =  3T(\\lfloor \\frac{n}{2} \\rfloor) + n \\\\ \\leq  3(c\\lfloor \\frac{n}{2} \\rfloor^{\\lg3} - \\frac{n}{3}) + n \\\\ \\leq  3(c(\\frac{n}{2})^{\\lg3} - \\frac{n}{3}) + n \\\\ =  3c(\\frac{n}{2})^{\\lg3} \\\\ =  cn^{\\lg3}\n\\end{eqnarray}\n$$", 
            "title": "4.4-1"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-2", 
            "text": "First let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{2}) + n^2$ and assume that n is an exact power of 2.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$n^2$}\n    child {node {$\\frac{n^2}{4}$}\n        child {node {$\\frac{n^2}{16}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$n^2$}[no edge from this parent]\n    child {node {$\\frac{n^2}{4}$}[no edge from this parent]\n        child {node {$(\\frac{1}{4})^2n$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}   The number of nodes at depth i is 1. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n^2}{4^i}$. So the total cost over all nodes at depth i, is $\\frac{n^2}{4^i}$. The bottom level, at depth $\\lg{n}$, has 1 node, which contributing cost $T(1)$, for a total cost of $T(1)$, which is $\\Theta(1)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sum_{i = 0}^{\\lg{n} - 1}\\frac{n^2}{4^i} + \\Theta(1) \\\\  \\sum_{i = 0}^{\\infty}\\frac{n^2}{4^i} + \\Theta(1) \\\\ =  \\frac{1}{1 - \\frac{1}{4}}n^2 + \\Theta(1) \\\\ =  \\frac{4}{3}n^2 + \\Theta(1) \\\\ =  O(n^2)\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^2$ for some constant $c   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\frac{n}{2}) + n^2 \\\\ \\leq  c(\\frac{n}{2})^2 + n^2 \\\\ =  (\\frac{c}{4} + 1)n^2 \\\\ \\leq  cn^2\n\\end{eqnarray}\n$$  where the last step holds as long as $c \\geq \\frac{4}{3}$.", 
            "title": "4.4-2"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-3", 
            "text": "When n is large, the difference between $\\frac{n}{2} + 2$ and $\\frac{n}{2}$ not that large, so it's also a sloppiness that we can tolerate. Then let's create a recursion tree for the recurrence $T(n) = 4T(\\frac{n}{2}) + n$ and assume that n is an exact power of 2.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=32mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){n}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=7 of root] {n}[no edge from this parent]\n    child {node {2n}[no edge from this parent]\n        child {node {4n}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(n^2)$}}}}};\n\\end{tikzpicture}\n\\end{document}   Each level has four times more nodes than the level above, so the number of nodes at depth i is $4^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n}{2^i}$. So the total cost over all nodes at depth i, is $4^i\\frac{n}{2^i} = 2^in$. The bottom level, at depth $\\lg{n}$, has $4^{\\lg{n}} = n^2$ nodes, each contributing cost $T(1)$, for a total cost of $n^2T(1)$, which is $\\Theta(n^2)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sum_{i = 0}^{\\lg{n} - 1}2^in + \\Theta(n^2) \\\\ =  n\\frac{1 - 2^{\\lg{n}}}{1 - 2} + \\Theta(n^2) \\\\ =  n(n - 1) + \\Theta(n^2) \\\\ =  n^2 - n + \\Theta(n^2) \\\\ =  O(n^2)\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^2$ for some constant $c   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  4T(\\frac{n}{2} + 2) + n \\\\ \\leq  4c(\\frac{n}{2} + 2)^2 + n \\\\ =  4c(\\frac{n^2}{4} + 2n + 4) + n \\\\ =  cn^2 + 8cn + 16c + n \\\\ =  cn^2 + (8c + 1)n + 16c\n\\end{eqnarray}\n$$  But $cn^2 + (8c + 1)n + 16c   cn^2$, so we need to try another guess, let's try $T(n) \\leq cn^2 - 5n$. So:  $$\n\\begin{eqnarray}\nT(n)  =  4T(\\frac{n}{2} + 2) + n \\\\ \\leq  4c((\\frac{n}{2} + 2)^2 - 5(\\frac{n}{2} + 2)) + n \\\\ =  4c(\\frac{n^2}{4} + 2n + 4 - \\frac{5n}{2} - 10) + n \\\\ =  4c(\\frac{n^2}{4} - \\frac{n}{2} - 6) + n \\\\ =  cn^2 + (1 - 2c)n - 24c \\\\  cn^2\n\\end{eqnarray}\n$$  where the last step holds as long as $c \\geq \\frac{1}{2}$.", 
            "title": "4.4-3"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-4", 
            "text": "First let's create a recursion tree for the recurrence $T(n) = 2T(n - 1) + 1$.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=16mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){1}\n    child {node {1}\n        child {node {1}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {1}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {1}\n        child {node {1}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {1}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=2 of root] {1}[no edge from this parent]\n    child {node {2}[no edge from this parent]\n        child {node {4}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(2^n)$}}}}};\n\\end{tikzpicture}\n\\end{document}   Each level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, n - 1 - 1$, has a cost of 1. So the total cost over all nodes at depth i, is $2^i$. The bottom level, at depth n, has $2^{n - 1}$ nodes, each contributing cost $T(1)$, for a total cost of $2^{n - 1}T(1)$, which is $\\Theta(2^n)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sum_{i = 0}^{n - 1 - 1}2^i + \\Theta(2^n) \\\\ =  \\frac{1 - 2^{n - 1}}{1 - 2} + \\Theta(2^n) \\\\ =  2^{n - 1} - 1 + \\Theta(2^n) \\\\ =  O(2^n)\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = O(2^n)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq c2^n$ for some constant $c   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  2T(n - 1) + 1 \\\\ \\leq  2c2^{n - 1} + 1 \\\\ =  c2^n + 1\n\\end{eqnarray}\n$$  But $c2^n + 1   c2^n$, so we need to try another guess. Lets try $T(n) \\leq c2^n - 1$. So:  $$\n\\begin{eqnarray}\nT(n)  =  2T(n - 1) + 1 \\\\ \\leq  2(c2^{n - 1} - 1) + 1 \\\\ =  c2^n -1 \\\\  c2^n\n\\end{eqnarray}\n$$", 
            "title": "4.4-4"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-5", 
            "text": "", 
            "title": "4.4-5"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-6", 
            "text": "First let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{3}) + T(\\frac{2n}{3})$ and assume that n is an exact power of 3.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=16mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){cn}\n    child {node {$c\\frac{n}{3}$}\n        child {node {$c\\frac{n}{9}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$c\\frac{2n}{9}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$c\\frac{2n}{3}$}\n        child {node {$c\\frac{2n}{9}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$c\\frac{4n}{9}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=2 of root] {cn}[no edge from this parent]\n    child {node {cn}[no edge from this parent]\n        child {node {cn}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(?)$}}}}};\n\\end{tikzpicture}\n\\end{document}   Each level has 2 times more nodes than the level above, so the number of nodes at depth i is $2^i$. But in this question, not all leaves reach at the bottom at the same time. The left most branch reaches the bottom first. So when the left most branch reaches the bottom, we assume other nodes also reach the bottom. Then the depth of recursion tree is $\\log_3{n}$.  The total cost over all nodes at depth i, is $cn$ and the cost at bottom is also cn So:  $$\n\\begin{eqnarray}\nT(n)  \\geq  \\sum_{i = 0}^{\\log_3{n} - 1}cn + cn \\\\ =  cn\\log_3{n} + cn \\\\ =  \\frac{c}{\\lg3}n\\lg{n} + cn \\\\ =  \\Theta(n\\lg{n})\n\\end{eqnarray}\n$$  But remember we've cut some branches of the recursion tree, so $T(n) = \\Omega(n\\lg{n})$.", 
            "title": "4.4-6"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-7", 
            "text": "When n is large, the difference between $\\frac{n}{2}$ and $\\lfloor \\frac{n}{2} \\rfloor$ not that large, so it's also a sloppiness that we can tolerate. Then let's create a recursion tree for the recurrence $T(n) = 4T(\\frac{n}{2}) + cn$ and assume that n is an exact power of 2.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=32mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){cn}\n    child {node {$\\frac{cn}{2}$}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{cn}{2}$}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{cn}{2}$}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{cn}{2}$}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{cn}{4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=7 of root] {cn}[no edge from this parent]\n    child {node {2cn}[no edge from this parent]\n        child {node {4cn}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(n^2)$}}}}};\n\\end{tikzpicture}\n\\end{document}   Each level has 4 times more nodes than the level above, so the number of nodes at depth i is $4^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{cn}{2^i}$. So the total cost over all nodes at depth i, is $4^i\\frac{cn}{2^i} = c2^in$. The bottom level, at depth $\\lg{n}$, has $4^{\\lg{n}} = n^2$ nodes, each contributing cost $T(1)$, for a total cost of $n^2T(1)$, which is $\\Theta(n^2)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sum_{i = 0}^{\\lg{n} - 1}c2^in + \\Theta(n^2) \\\\ =  cn\\frac{1 - 2^{\\lg{n}}}{1 - 2} + \\Theta(n^2) \\\\ =  \\Theta(n^2)\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq dn^2$ for some constant $d   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\ \\leq  4T(\\frac{n}{2}) + cn \\\\ \\leq  4d(\\frac{n}{2})^2 + cn \\\\ =  dn^2 + cn\n\\end{eqnarray}\n$$  But $dn^2 + cn   dn^2$, so let's try another guess, let's try $T(n) \\leq dn^2 - dn$.  $$\n\\begin{eqnarray}\nT(n)  =  4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\ \\leq  4T(\\frac{n}{2}) + cn \\\\ \\leq  4d((\\frac{n}{2})^2 - \\frac{n}{2}) + cn \\\\ =  dn^2 + (c - 2d)n \\\\ \\leq  dn^2\n\\end{eqnarray}\n$$  where the last step holds as long as $d \\geq \\frac{c}{2}$.  So $T(n) = O(n^2)$. Then we want to show that $T(n) \\geq dn^2$ for some constant $d   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\ \\geq  4T(\\frac{n - (2 - 1)}{2}) + cn \\\\ =  4T(\\frac{n - 1}{2}) + cn \\\\ \\geq  4d(\\frac{n - 1}{2})^2 + cn \\\\ =  dn^2 + (c - 2d)n + d \\\\  dn^2\n\\end{eqnarray}\n$$  where the last step holds as long as $d   \\frac{c}{2}$.  So $T(n) = \\Omega(n^2)$, so $T(n) = \\Theta(n^2)$.", 
            "title": "4.4-7"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-8", 
            "text": "First let's create a recursion tree for the recurrence $T(n) = T(n - a) + T(a) + cn$ and assume that n - 1 is divisible by a.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=14mm]\n\\tikzstyle{level 2}=[sibling distance=10mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){cn}\n    child {node {c(n - a)}\n        child {node {c(n - 2a)}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {ca}}}\n    child {node {ca}};\n\n\\node[right=1 of root] {cn}[no edge from this parent]\n    child {node {cn}[no edge from this parent]\n        child {node {cn - ca}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(a)$}}}}};\n\\end{tikzpicture}\n\\end{document}   The total cost over all nodes at depth i, for $i = 0, 1, 2, \\ldots, \\frac{n - 1}{a} - 1$, is $c(n - (i - 1)a), \\text{ for } i \\geq 1$. The bottom level, at depth $\\frac{n - 1}{a}$, the total cost is $T(1) + ca$, which is $\\Theta(a)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  cn + \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}c(n - (i - 1)a) + \\Theta(a) \\\\ =  cn + \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}cn - \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}(i - 1)ca + \\Theta(a) \\\\ =  \\frac{n - 1}{a}cn - \\frac{n^2 - 2a -3na + 3a + 2a^2 + 1}{2a}c \\\\ =  \\frac{n^2 +3na - 3a - 2a^2 - 1}{2a}c \\\\ =  \\Theta(n^2)\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = \\Theta(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq dn^2$ for some constant $d   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(n - a) + T(a) + cn \\\\ \\leq  d(n - a)^2 + da^2 + cn \\\\ =  dn^2 + (c - ad)n + (2a^2 - an)d \\\\ \\leq  dn^2\n\\end{eqnarray}\n$$  where the last step holds as long as $d \\geq \\frac{c}{a}$ and $n \\geq 2a$.  So $T(n) = O(n^2)$.  Then we want to show that $T(n) \\geq dn^2$ for some constant $d   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(n - a) + T(a) + cn \\\\ \\geq  d(n - a)^2 + da^2 + cn \\\\ =  dn^2 + (c - 2ad)n + 2a^2d \\\\  dn^2\n\\end{eqnarray}\n$$  where the last step holds as long as $d   \\frac{c}{2a}$.  So $T(n) = \\Omega(n^2)$. So $T(n) = \\Theta(n^2)$.", 
            "title": "4.4-8"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-9", 
            "text": "First let's create a recursion tree for the recurrence $T(n) = T(\\alpha{n}) + T((1 - \\alpha)n) + cn$.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=38mm]\n\\tikzstyle{level 2}=[sibling distance=18mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){cn}\n    child {node {$c\\alpha{n}$}\n        child {node {$c\\alpha^2{n}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$c\\alpha(1 - \\alpha)n$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$c(1 - \\alpha)n$}\n        child {node {$c\\alpha(1 - \\alpha)n$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$c(1 - \\alpha)^2n$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=4 of root] {cn}[no edge from this parent]\n    child {node {cn}[no edge from this parent]\n        child {node {cn}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(?)$}}}}};\n\\end{tikzpicture}\n\\end{document}   So we can see not each branch reaches at the bottom at the same time, it might be the left most branch reaches at the bottom first, or the right most branch reaches at the bottom first. It depends on which is smaller, $\\alpha$ or $1 - \\alpha$. Let $\\alpha \\leq 1 - \\alpha$, so we get $\\alpha \\leq \\frac{1}{2}$. That is, when $0   \\alpha \\leq \\frac{1}{2}$, the left most branch reaches at the bottom first, when $\\frac{1}{2}   \\alpha   1$, the right most branch reaches at the bottom first.  If $0   \\alpha \\leq \\frac{1}{2}$, when the left most branch reaches at the bottom, let's assume other branches also reach at the bottom, so the total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{\\alpha}{\\frac{1}{n}} - 1$, is cn. The cost at the bottom is also cn. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ \\geq  \\sum_{i = 0}^{\\log_{\\alpha}\\frac{1}{n} - 1}cn + cn \\\\ =  cn(\\log_{\\alpha}\\frac{1}{n}) + cn \\\\ =  cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} + cn \\\\  cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} \\\\ =  \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = \\Omega(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq dn\\lg{n}$ for some constant $d   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ \\geq  d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ =  d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ =  dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ \\geq   dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d\\alpha\\lg{\\alpha}n + cn \\, (0   \\alpha \\leq \\frac{1}{2}) \\\\ =  dn\\lg{n} + (c + 2d\\alpha\\lg{\\alpha})n \\\\ \\geq  dn\\lg{n}\n\\end{eqnarray}\n$$  where the last step holds as long as $d \\leq \\frac{c}{2\\alpha\\lg{\\frac{1}{\\alpha}}}$.  Now let's check the right most branch. When it reaches at the bottom, other branches have already reached at the bottom, if the tree depth is k, let's assume other branches become T(1) at depth k - 1.  Each level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And total cost over all nodes at depth i, for $i = 0, 1, 2, ..., \\log_{1 - \\alpha}\\frac{1}{n}$, is cn. The cost at the bottom is $T(1)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ \\leq  \\sum_{i = 0}^{\\log_{1 - \\alpha}\\frac{1}{n} - 1}cn + \\Theta(1) \\\\ =  cn(\\log_{1 - \\alpha}\\frac{1}{n}) + \\Theta(1) \\\\ =  cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} + \\Theta(1) \\\\  cn\\frac{\\lg{n}}{\\lg{\\frac{1}{1 - \\alpha}}} + n\\lg{n}\\\\ =  O(n\\lg{n})\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = O(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to veify that our guess was correct. We want to show that $T(n) \\leq dn\\lg{n}$ for some constant $d   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ \\leq  d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ =  d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ =  dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ \\leq   dn\\lg{n} + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\, (0   \\alpha \\leq \\frac{1}{2}) \\\\ =  dn\\lg{n} + (c + 2d(1 - \\alpha)\\lg{(1 - \\alpha)})n \\\\ \\leq  dn\\lg{n}\n\\end{eqnarray}\n$$  where the last step holds as long as $d \\geq \\frac{c}{2(1 - \\alpha)\\lg{\\frac{1}{1 - \\alpha}}}$.  So we proved $T(n) = \\Omega(n\\lg{n})$ and $T(n) = O(n\\lg{n})$. So $T(n) = \\Theta(n\\lg{n})$ when $0   \\alpha \\leq \\frac{1}{2}$.  Then let's check another case, when $\\frac{1}{2}   \\alpha   1$. In this case, the right most branch reaches at the bottom first. Let's assume other branches also reach at the bottom, so the total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{1 - \\alpha}\\frac{1}{n}$, is cn. The cost at the bottom is cn. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ \\geq  \\sum_{i = 0}^{\\log_{1 - \\alpha}\\frac{1}{n} - 1}cn + cn \\\\ =  cn(\\log_{1 - \\alpha}\\frac{1}{n}) + cn \\\\ =  cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} + cn \\\\  cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} \\\\ =  \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = \\Omega(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq dn\\lg{n}$ for some constant $d   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ \\geq  d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ =  d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ =  dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\  dn\\lg{n} + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\, (\\frac{1}{2}   \\alpha   1) \\\\ =  dn\\lg{n} + (c + 2d(1 - \\alpha)\\lg{(1 - \\alpha)})n \\\\ \\geq  dn\\lg{n}\n\\end{eqnarray}\n$$  where the last step holds as long as $d \\leq \\frac{c}{2(1 - \\alpha)\\lg{\\frac{1}{1 - \\alpha}}}$.  Now let's check the left most branch. When it reaches at the bottom, other branches have already reached at the bottom, if the tree depth is k, let's assume other branches become T(1) at depth k - 1.  Each level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{\\alpha}\\frac{1}{n} - 1$, is cn. The cost at the bottom is T(1). So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\  \\sum_{i = 0}^{\\log_{\\alpha}\\frac{1}{n} - 1}cn + \\Theta(1) \\\\ =  cn(\\log_{\\alpha}\\frac{1}{n}) + \\Theta(1) \\\\ =  cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{\\alpha}}} + \\Theta(1) \\\\  cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} + n\\lg{n}\\\\ =  O(n\\lg{n})\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = O(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to veify that our guess was correct. We want to show that $T(n) \\leq dn\\lg{n}$ for some constant $d   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ \\leq  d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ =  d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ =  dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\   dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d\\alpha\\lg{\\alpha}n + cn \\, (\\frac{1}{2}   \\alpha   1) \\\\ =  dn\\lg{n} + (c + 2d\\alpha\\lg{\\alpha})n \\\\ \\leq  dn\\lg{n}\n\\end{eqnarray}\n$$  where the last step holds as long as $d \\geq \\frac{c}{2\\alpha\\lg{\\frac{1}{\\alpha}}}$.  So we proved $T(n) = \\Omega(n\\lg{n})$ and $T(n) = O(n\\lg{n})$. So $T(n) = \\Theta(n\\lg{n})$ when $\\frac{1}{2}   \\alpha   1$. Thus $T(n) = \\Theta(n\\lg{n})$ for all $0   \\alpha   1$.", 
            "title": "4.4-9"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/", 
            "text": "4.5 The master method for solving recurrences\n\n\n4.5-1\n\n\na\n\n\nHere, we have a = 2, b = 4, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $n^{\\frac{1}{2}}$ is polynomially larger than f(n) (that is, $f(n) = O(n^{\\frac{1}{2} - \\epsilon})$ for $\\epsilon \\leq \\frac{1}{2}$), case 1 applies, and $T(n) = \\Theta(n^{\\frac{1}{2}})$.\n\n\nb\n\n\nHere, we have a = 2, b = 4, and $f(n) = \\Theta(\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Case 2 applies, so $T(n) = \\Theta(\\sqrt{n}\\lg{n})$.\n\n\nc\n\n\nHere, we have a = 2, b = 4, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $f(n) = \\Omega(n^{\\frac{1}{2} + \\epsilon})$, for $\\epsilon \\leq \\frac{1}{2}$. Case 3 applies if we can show that the regularity condition holds for f(n). For sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{4}) = 2\\frac{n}{4} = \\frac{n}{2} \\leq cf(n)$ for $c = \\frac{2}{3}$. So $T(n) = \\Theta(n)$.\n\n\nd\n\n\nHere, we have a = 2, b = 4, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $f(n) = \\Omega(n^{\\frac{1}{2} + \\epsilon})$, for $\\epsilon \\leq \\frac{3}{2}$. Case 3 applies if we can show that the regularity condition holds for f(n). For sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{4}) = 2\\frac{n}{4} = \\frac{n}{2} \\leq cf(n)$ for $c = \\frac{1}{2}$. So $T(n) = \\Theta(n^2)$.\n\n\n4.5-2\n\n\nIn Strassen's algorithm, $T(n) = \\Theta(n^{\\lg7})$. In Professor Caesar's algorithm, $T(n) = aT(\\frac{n}{4}) + \\Theta(n^2)$. In order to beat Strassen's algorithm, then Professor Caesar's algorithm cannot apply case 3, since $f(n) = \\Theta(n^2)$ which is polynomially larger than $\\Theta(n^{\\lg7})$.\n\n\nWe have that $n^{\\log_ba} = n^{\\log_4a} = n^{\\frac{\\lg{a}}{2}}$. If case 2 applies, then $f(n) = \\Theta(n^{\\frac{\\lg{a}}{2}})$, so $n^{\\frac{\\lg{a}}{2}} = n^2$, a = 16. And $T(n) = \\Theta(n^{\\frac{\\lg{a}}{2}}\\lg{n}) = \\Theta(n^2\\lg{n})$ which cannot beat Strassen's algorithm.\n\n\nSo case 1 applies. And $T(n) = \\Theta(n^{\\frac{\\lg{a}}{2}})$, and we have $\\frac{\\lg{a}}{2} \n \\lg7$, thus $a \n 49$, so the largest integer value of a is 48.\n\n\n4.5-3\n\n\nHere, we have a = 1, b = 2, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. Case 2 applies, so $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(\\lg{n})$.\n\n\n4.5-4\n\n\nHere, we have a = 4, b = 2, and $f(n) = \\Theta(n^2\\lg{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_2{4}} = n^2$. Since f(n) is not polymonially larger than $n^2$, so we cannot use master method to solve the recurrence.\n\n\nAccording to exercise 4.6-2, $f(n) = \\Theta(n^2\\lg{n}) = \\Theta(n^{\\log_b{a}}\\lg^k{n})$, where k = 1. So $T(n) = \\Theta(n^{\\log_b{a}}\\lg^{k + 1}{n}) = \\Theta(n^2\\lg^2{n}) = \\Theta((n\\lg{n})^2)$.\n\n\n4.5-5", 
            "title": "4.5 The master method for solving recurrences"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-the-master-method-for-solving-recurrences", 
            "text": "", 
            "title": "4.5 The master method for solving recurrences"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-1", 
            "text": "a  Here, we have a = 2, b = 4, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $n^{\\frac{1}{2}}$ is polynomially larger than f(n) (that is, $f(n) = O(n^{\\frac{1}{2} - \\epsilon})$ for $\\epsilon \\leq \\frac{1}{2}$), case 1 applies, and $T(n) = \\Theta(n^{\\frac{1}{2}})$.  b  Here, we have a = 2, b = 4, and $f(n) = \\Theta(\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Case 2 applies, so $T(n) = \\Theta(\\sqrt{n}\\lg{n})$.  c  Here, we have a = 2, b = 4, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $f(n) = \\Omega(n^{\\frac{1}{2} + \\epsilon})$, for $\\epsilon \\leq \\frac{1}{2}$. Case 3 applies if we can show that the regularity condition holds for f(n). For sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{4}) = 2\\frac{n}{4} = \\frac{n}{2} \\leq cf(n)$ for $c = \\frac{2}{3}$. So $T(n) = \\Theta(n)$.  d  Here, we have a = 2, b = 4, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $f(n) = \\Omega(n^{\\frac{1}{2} + \\epsilon})$, for $\\epsilon \\leq \\frac{3}{2}$. Case 3 applies if we can show that the regularity condition holds for f(n). For sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{4}) = 2\\frac{n}{4} = \\frac{n}{2} \\leq cf(n)$ for $c = \\frac{1}{2}$. So $T(n) = \\Theta(n^2)$.", 
            "title": "4.5-1"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-2", 
            "text": "In Strassen's algorithm, $T(n) = \\Theta(n^{\\lg7})$. In Professor Caesar's algorithm, $T(n) = aT(\\frac{n}{4}) + \\Theta(n^2)$. In order to beat Strassen's algorithm, then Professor Caesar's algorithm cannot apply case 3, since $f(n) = \\Theta(n^2)$ which is polynomially larger than $\\Theta(n^{\\lg7})$.  We have that $n^{\\log_ba} = n^{\\log_4a} = n^{\\frac{\\lg{a}}{2}}$. If case 2 applies, then $f(n) = \\Theta(n^{\\frac{\\lg{a}}{2}})$, so $n^{\\frac{\\lg{a}}{2}} = n^2$, a = 16. And $T(n) = \\Theta(n^{\\frac{\\lg{a}}{2}}\\lg{n}) = \\Theta(n^2\\lg{n})$ which cannot beat Strassen's algorithm.  So case 1 applies. And $T(n) = \\Theta(n^{\\frac{\\lg{a}}{2}})$, and we have $\\frac{\\lg{a}}{2}   \\lg7$, thus $a   49$, so the largest integer value of a is 48.", 
            "title": "4.5-2"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-3", 
            "text": "Here, we have a = 1, b = 2, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. Case 2 applies, so $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(\\lg{n})$.", 
            "title": "4.5-3"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-4", 
            "text": "Here, we have a = 4, b = 2, and $f(n) = \\Theta(n^2\\lg{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_2{4}} = n^2$. Since f(n) is not polymonially larger than $n^2$, so we cannot use master method to solve the recurrence.  According to exercise 4.6-2, $f(n) = \\Theta(n^2\\lg{n}) = \\Theta(n^{\\log_b{a}}\\lg^k{n})$, where k = 1. So $T(n) = \\Theta(n^{\\log_b{a}}\\lg^{k + 1}{n}) = \\Theta(n^2\\lg^2{n}) = \\Theta((n\\lg{n})^2)$.", 
            "title": "4.5-4"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-5", 
            "text": "", 
            "title": "4.5-5"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/", 
            "text": "4.6 Proof of the master theorem\n\n\n4.6-1\n\n\nLet's prove that if b is a positive integer, then $\\lceil \\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\rceil = \\lceil \\frac{n}{b^2} \\rceil$.\n\n\nSince $\\lceil \\frac{n}{b} \\rceil \\geq \\frac{n}{b}$, so $\\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\geq \\frac{n}{b}\\frac{1}{b} = \\frac{n}{b^2}$, thus $\\lceil \\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\rceil \\geq \\lceil \\frac{n}{b^2} \\rceil$.\n\n\nAnd we have $\\frac{n}{b}\\frac{1}{b} \\leq \\lceil\\frac{n}{b}\\frac{1}{b} \\rceil = \\lceil\\frac{n}{b^2} \\rceil$, hence $\\frac{n}{b} \\leq b\\lceil \\frac{n}{b^2} \\rceil$. Because both b and $\\lceil \\frac{n}{b^2} \\rceil$ are integers, so $b\\lceil \\frac{n}{b^2} \\rceil$ is also a integer. So $\\lceil \\frac{n}{b} \\rceil \\leq b\\lceil \\frac{n}{b^2} \\rceil$. So $\\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\leq \\lceil \\frac{n}{b^2} \\rceil$, so $\\lceil \\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\rceil \\leq \\lceil \\frac{n}{b^2} \\rceil$.\n\n\nThus $\\lceil \\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\rceil = \\lceil \\frac{n}{b^2} \\rceil$, so $n_j = \\lceil \\frac{n}{b^{j - 1}} \\rceil$ if j \n 0. If j = 0, $n_j = n$, so we can get a more simpler expression, $n_j = \\lceil \\frac{n}{b^j} \\rceil$.\n\n\nYou can get some useful lemmas \nhere\n.\n\n\n4.6-2\n\n\nFrom lemma 4.2 we know $T(n) = \\Theta(n^{\\log_b{a}}) + \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j})$.\n\n\n$$\n\\begin{eqnarray}\n\\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j}) \n=\n \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j}) \\\\\n\n=\n \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta((\\frac{n}{b^j})^{\\log_b{a}}\\lg^k{\\frac{n}{b^j}}) \\\\\n\n=\n \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta(\\frac{n^{\\log_b{a}}}{(b^{\\log_b{a}})^j}\\lg^k{\\frac{n}{b^j}}) \\\\\n\n=\n \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta(\\frac{n^{\\log_b{a}}}{a^j}\\lg^k{\\frac{n}{b^j}}) \\\\\n\n=\n \\sum_{j = 0}^{\\log_b{n - 1}}\\Theta(n^{\\log_b{a}}\\lg^k{\\frac{n}{b^j}}) \\\\\n\n=\n n^{\\log_b{a}}\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta(\\lg^k{\\frac{n}{b^j}}) \\\\\n\n=\n n^{\\log_b{a}}\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k)\n\\end{eqnarray}\n$$\n\n\nWe have:\n\n\n$$\n\\begin{eqnarray}\n\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k) \n\\leq\n \\sum_{j = 0}^{\\log_b{n - 1}}\\lg^k{n} \\\\\n\n=\n \\log_b{n}\\lg^k{n} \\\\\n\n=\n \\frac{\\lg^{k + 1}{n}}{\\lg{b}} \\\\\n\n=\n O(\\lg^{k + 1}{n})\n\\end{eqnarray}\n$$\n\n\nBut I don't know how to prove $\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k) = \\Omega(\\lg^{k + 1}{n})$.\n\n\n4.6-3\n\n\nWe have $af(\\frac{n}{b}) \\leq cf(n)$, so $f(n) \\geq \\frac{a}{c}f(\\frac{n}{b})$, and:\n\n\n$$\n\\begin{eqnarray}\nf(n) \n\\geq\n \\frac{a}{c}f(\\frac{n}{b}) \\\\\n\n\\geq\n (\\frac{a}{c})^2f(\\frac{n}{b^2}) \\\\\n\n\\geq\n \\ldots \\\\\n\n\\geq\n (\\frac{a}{c})^{\\log_b{n}}f(1) \\\\\n\n=\n n^{\\log_b{\\frac{a}{c}}}f(1) \\\\\n\n=\n n^{\\log_b{a + \\epsilon}}f(1) \\text{ (c \n 1)} \\\\\n\n=\n \\Omega(n^{\\log_b{a + \\epsilon}})\n\\end{eqnarray}\n$$\n\n\nSo $f(n) = \\Omega(n^{\\log_b{a + \\epsilon}})$.", 
            "title": "4.6 Proof of the master theorem"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/#46-proof-of-the-master-theorem", 
            "text": "", 
            "title": "4.6 Proof of the master theorem"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/#46-1", 
            "text": "Let's prove that if b is a positive integer, then $\\lceil \\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\rceil = \\lceil \\frac{n}{b^2} \\rceil$.  Since $\\lceil \\frac{n}{b} \\rceil \\geq \\frac{n}{b}$, so $\\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\geq \\frac{n}{b}\\frac{1}{b} = \\frac{n}{b^2}$, thus $\\lceil \\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\rceil \\geq \\lceil \\frac{n}{b^2} \\rceil$.  And we have $\\frac{n}{b}\\frac{1}{b} \\leq \\lceil\\frac{n}{b}\\frac{1}{b} \\rceil = \\lceil\\frac{n}{b^2} \\rceil$, hence $\\frac{n}{b} \\leq b\\lceil \\frac{n}{b^2} \\rceil$. Because both b and $\\lceil \\frac{n}{b^2} \\rceil$ are integers, so $b\\lceil \\frac{n}{b^2} \\rceil$ is also a integer. So $\\lceil \\frac{n}{b} \\rceil \\leq b\\lceil \\frac{n}{b^2} \\rceil$. So $\\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\leq \\lceil \\frac{n}{b^2} \\rceil$, so $\\lceil \\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\rceil \\leq \\lceil \\frac{n}{b^2} \\rceil$.  Thus $\\lceil \\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\rceil = \\lceil \\frac{n}{b^2} \\rceil$, so $n_j = \\lceil \\frac{n}{b^{j - 1}} \\rceil$ if j   0. If j = 0, $n_j = n$, so we can get a more simpler expression, $n_j = \\lceil \\frac{n}{b^j} \\rceil$.  You can get some useful lemmas  here .", 
            "title": "4.6-1"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/#46-2", 
            "text": "From lemma 4.2 we know $T(n) = \\Theta(n^{\\log_b{a}}) + \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j})$.  $$\n\\begin{eqnarray}\n\\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j})  =  \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j}) \\\\ =  \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta((\\frac{n}{b^j})^{\\log_b{a}}\\lg^k{\\frac{n}{b^j}}) \\\\ =  \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta(\\frac{n^{\\log_b{a}}}{(b^{\\log_b{a}})^j}\\lg^k{\\frac{n}{b^j}}) \\\\ =  \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta(\\frac{n^{\\log_b{a}}}{a^j}\\lg^k{\\frac{n}{b^j}}) \\\\ =  \\sum_{j = 0}^{\\log_b{n - 1}}\\Theta(n^{\\log_b{a}}\\lg^k{\\frac{n}{b^j}}) \\\\ =  n^{\\log_b{a}}\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta(\\lg^k{\\frac{n}{b^j}}) \\\\ =  n^{\\log_b{a}}\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k)\n\\end{eqnarray}\n$$  We have:  $$\n\\begin{eqnarray}\n\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k)  \\leq  \\sum_{j = 0}^{\\log_b{n - 1}}\\lg^k{n} \\\\ =  \\log_b{n}\\lg^k{n} \\\\ =  \\frac{\\lg^{k + 1}{n}}{\\lg{b}} \\\\ =  O(\\lg^{k + 1}{n})\n\\end{eqnarray}\n$$  But I don't know how to prove $\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k) = \\Omega(\\lg^{k + 1}{n})$.", 
            "title": "4.6-2"
        }, 
        {
            "location": "/4-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/#46-3", 
            "text": "We have $af(\\frac{n}{b}) \\leq cf(n)$, so $f(n) \\geq \\frac{a}{c}f(\\frac{n}{b})$, and:  $$\n\\begin{eqnarray}\nf(n)  \\geq  \\frac{a}{c}f(\\frac{n}{b}) \\\\ \\geq  (\\frac{a}{c})^2f(\\frac{n}{b^2}) \\\\ \\geq  \\ldots \\\\ \\geq  (\\frac{a}{c})^{\\log_b{n}}f(1) \\\\ =  n^{\\log_b{\\frac{a}{c}}}f(1) \\\\ =  n^{\\log_b{a + \\epsilon}}f(1) \\text{ (c   1)} \\\\ =  \\Omega(n^{\\log_b{a + \\epsilon}})\n\\end{eqnarray}\n$$  So $f(n) = \\Omega(n^{\\log_b{a + \\epsilon}})$.", 
            "title": "4.6-3"
        }, 
        {
            "location": "/4-Divide-and-Conquer/Problems/", 
            "text": "Problems\n\n\n4-1\n\n\na\n\n\nHere, we have a = 2, b = 2, and $f(n) = \\Theta(n^4)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{2}} = n$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 3$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{2}) = \\frac{n^4}{8} = cf(n)$, for $c = \\frac{1}{8}$. So, the solution to the recurrence is $T(n) = \\Theta(n^4)$.\n\n\nb\n\n\nHere, we have a = 1, $b = \\frac{10}{7}$, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_{\\frac{10}{7}}{1}} = 1$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 1$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = f(\\frac{7n}{10}) = \\frac{7n}{10} = cf(n)$, for $c = \\frac{7}{10}$. So, the solution to the recurrence is $T(n) = \\Theta(n)$.\n\n\nc\n\n\nHere, we have a = 16, b = 4, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{16}} = n^2$. So case 2 applies, so the solution to the recurrence is $T(n) = \\Theta(n^2\\lg{n})$.\n\n\nd\n\n\nHere, we have a = 7, b = 3, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{7}}$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 2 - \\log_3{7}$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 7f(\\frac{n}{3}) = \\frac{7n^2}{9} = cf(n)$, for $c = \\frac{7}{9}$. So, the solution to the recurrence is $T(n) = \\Theta(n^2)$.\n\n\ne\n\n\nHere, we have a = 7, b = 2, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{7}} = n^{\\lg{7}}$. Since $f(n) = O(n^{\\log_b{a} - \\epsilon})$, where $\\epsilon \\leq \\lg{\\frac{7}{4}}$, case 1 applies. So, the solution to the recurrence is $T(n) = \\Theta(n^{\\lg7})$.\n\n\nf\n\n\nHere, we have a = 2, b = 4, and $f(n) = \\Theta(\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = \\sqrt{n}$, case 2 applies. So, the solution to the recurrence is $T(n) = \\Theta(\\sqrt{n}\\lg{n})$.\n\n\ng\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = T(n - 2) + n^2$.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$n^2$}\n    child {node {$(n - 2)^2$}\n        child {node {$(n - 4)^2$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$n^2$}[no edge from this parent]\n    child {node {$(n - 2)^2$}[no edge from this parent]\n        child {node {$(n - 4)^2$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nThe number of nodes at depth i is 1. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\frac{n - 1}{2} - 1$, has a cost of $(n - 2i)^2$. So the total cost over all nodes at depth i, is $(n - 2i)^2$. The bottom level, at depth $\\frac{n - 1}{2}$, has 1 node, which contributing cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}(n - 2i)^2 + \\Theta(1) \\\\\n\n=\n \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}(n^2 - 2ni + 4i^2) + \\Theta(1) \\\\\n\n=\n \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}n^2 - \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}2ni + \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}4i^2 + \\Theta(1) \\\\\n\n=\n \\frac{n - 1}{2}n^2 - 2n\\frac{(\\frac{n - 1}{2} - 1)(1 + \\frac{n - 1}{2} - 1)}{2} + 4\\frac{(\\frac{n - 1}{2} - 1)(\\frac{n - 1}{2} - 1 + 1)(2(\\frac{n - 1}{2} - 1) + 1)}{6} + \\Theta(1) \\\\\n\n=\n \\frac{n^2(n - 1)}{2} - \\frac{n(n - 1)(n - 3)}{4} + \\frac{(n - 1)(n - 2)(n - 3)}{6} + \\Theta(1) \\\\\n\n=\n \\frac{5n^3 - 6n^2 + 13n -12}{6} + \\Theta(1) \\\\\n\n=\n \\Theta(n^3)\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = \\Theta(n^3)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n^3$ and $T(n) \\leq c_2n^3$ for some constants $c_1 \n 0$ and $c_2 \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(n - 2) + n^2 \\\\\n\n\\geq\n c_1(n - 2)^3 + n^2 \\\\\n\n=\n c_1n^3 + (1 - 6c_1)n^2 + 4c_1(3n - 2) \\\\\n\n c_1n^3\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c_1 \\leq \\frac{1}{6}$.\n\n\nSo $T(n) = \\Omega(n^3)$.\n\n\nAnd:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(n - 2) + n^2 \\\\\n\n\\leq\n c_2(n - 2)^3 + n^2 \\\\\n\n=\n c_2n^3 + n((1 - 6c_2)n + 12c_2) - 8c_2 \\\\\n\n\\leq\n c_2n^3\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c_2 \n \\frac{1}{6}$ and $n \\geq \\frac{12c_2}{6c_2 - 1}$.\n\n\nSo $T(n) = O(n^3)$, thus $T(n) = \\Theta(n^3)$.\n\n\n4-2\n\n\na\n\n\nHere is the pseudocode of recursive binary search algorithm:\n\n\nRECURSIVE-BINARY-SEARCH(A, v, low, high)\n\nif low \n= high\n    middle = (low + high) / 2\n\n    if A[middle] \n v\n        return RECURSIVE-BINARY-SEARCH(A, v, middle + 1, high)\n    else if A[middle] \n v\n        return RECURSIVE-BINARY-SEARCH(A, v, low, middle - 1)\n    else\n        return middle\n\nreturn NIL\n\n\n\n\nAn array is passed by pointer\n\n\nBefore it halves the problem size, it needs to do some operations like comparing \nlow\n and \nhigh\n, calculating \nmiddle\n. But they are constant operations, we can let it be $\\Theta(1)$.\n\n\nSo $T(n) = T(\\frac{n}{2}) + \\Theta(1)$. Here, we have a = 1, b = 2, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. So case 2 applies, thus $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(\\lg{n}) = \\Theta(\\lg{N})$.\n\n\nAn array is passed by copying\n\n\nEach time it halves the problem size, it needs additional $\\Theta(N)$ operation to copy the array. So $T(n) = T(\\frac{n}{2}) + \\Theta(N) + \\Theta(1) = T(\\frac{n}{2}) + \\Theta(N) = T(\\frac{n}{4}) + \\Theta(N) + \\Theta(N) = \\ldots = T(1) + \\lg{n}\\Theta(N) = \\Theta(N\\lg{N})$.\n\n\nAn array is passed by range\n\n\nEach time it halves the problem size, it needs additional $\\Theta(n)$ operation to copy the array. So $T(n) = T(\\frac{n}{2}) + \\Theta(n) + \\Theta(1) = T(\\frac{n}{2}) + \\Theta(n)$.\n\n\nHere, we have a = 1, b = 2, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 1$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = f(\\frac{n}{2}) = \\Theta(\\frac{n}{2}) = c\\Theta(n)$, for $c = \\frac{1}{2}$. So, the solution to the recurrence is $T(n) = \\Theta(n) = \\Theta(N)$.\n\n\nb\n\n\nHere is the pseudocode of merge sor algorithm:\n\n\nMERGE-SORT(A, p, r)\n\nif p \n r\n    q = (p + r) / 2\n    MERGE-SORT(A, p ,q)\n    MERGE-SORT(A, q + 1, r)\n    MERGE(A, p, q, r)\n\n\n\n\nAn array is passed by pointer\n\n\nWe already know the solution that $T(n) = \\Theta(n\\lg{n}) = \\Theta(N\\lg{N})$.\n\n\nAn array is passed by copying\n\n\nFrom the pseudocode we know it needs to pass the array 3 times to subroutine. So we have $T(n) = 2T(\\frac{n}{2}) + \\Theta(n) + 3\\Theta(N) = 2T(\\frac{n}{2}) + \\Theta(n) + \\Theta(N)$. Let's use the iterative method to solve it:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 2T(\\frac{n}{2}) + \\Theta(n) + \\Theta(N) \\\\\n\n=\n 2(2T(\\frac{n}{4}) + \\Theta(\\frac{n}{2}) + \\Theta(N)) + \\Theta(n) + \\Theta(N) \\\\\n\n=\n 4T(\\frac{n}{4}) + 2\\Theta(\\frac{n}{2}) + 2\\Theta(N) + \\Theta(n) + \\Theta(N) \\\\\n\n=\n 4T(\\frac{n}{4}) + 2\\Theta(n) + 3\\Theta(N) \\\\\n\n=\n \\ldots \\\\\n\n=\n 2^iT(\\frac{n}{2^i}) + i\\Theta(n) + (1 + 2 + \\ldots + 2^{i - 1})\\Theta(N) \\text{ for } i = 1, 2, \\ldots, \\lg{n} \\\\\n\n=\n 2^{\\lg{n}}T(1) + \\lg{n}\\Theta(n) + \\Theta(N)\\frac{1 - 2^{\\lg{n}}}{1 - 2} \\\\\n\n=\n nT(1) + \\Theta(n\\lg{n}) + \\Theta(Nn) \\\\\n\n=\n \\Theta(N^2)\n\\end{eqnarray}\n$$\n\n\nAn array is passed by range\n\n\nIt needs additional $3\\Theta(n)$ to copy the array. So $T(n) = 2T(\\frac{n}{2}) + \\Theta(n) + 3\\Theta(n) = 2T(\\frac{n}{2}) + \\Theta(n) = \\Theta(n\\lg{n}) = \\Theta(N\\lg{N})$.\n\n\n4-3\n\n\na\n\n\nHere, we have a = 4, b = 3, and $f(n) = \\Theta(n\\lg{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_3{4}}$.\n\n\nIn problems 3-2-a, we proved that $\\lg^k{n} = O(n^\\epsilon)$ for $k \\geq 1$ and $\\epsilon \n 0$, so $\\lg{n} = O(n^{\\epsilon})$. Since $\\log_3{4} \\approx 1.2618595071429148$, so $f(n) = O(n^{\\log_ba - \\epsilon})$ for $\\epsilon \\leq 0.26$. So case 1 applies, the solution to the recurrence is $T(n) = \\Theta(n^{\\log_34})$.\n\n\nb\n\n\nHere, we have a = 3, b = 3, and $f(n) = \\Theta(\\frac{n}{\\lg{n}})$, and thus we have that $n^{\\log_ba} = n^{\\log_3{3}} = n$. So $f(n) = O(n^{\\log_ba})$. But it's not that easy to prove that $f(n) = O(n^{\\log_ba - \\epsilon})$.\n\n\nLet's solve it by iterative method:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n 3(3T(\\frac{n}{9}) + \\frac{\\frac{n}{3}}{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n 9T(\\frac{n}{9}) + \\frac{n}{\\lg{n} - \\lg3} + \\frac{n}{\\lg{n}} \\\\\n\n=\n \\ldots \\\\\n\n=\n 3^iT(\\frac{n}{3^i}) + \\sum_{i = 1}^{\\log_3{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg3} \\\\\n\n=\n nT(1) + \\sum_{i = 1}^{\\log_3{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg3} \\\\\n\n=\n nT(1) + \\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\lg{n} - i\\lg3} \\\\\n\n=\n nT(1) + \\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\frac{\\log_3{n}}{\\log_3{2}} - i\\frac{\\log_3{3}}{\\log_3{2}}} \\\\\n\n=\n nT(1) + \\log_3{2}\\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\log_3{n} - i} \\\\\n\n=\n nT(1) + \\log_3{2}\\sum_{i = 1}^{\\log_3{n}}\\frac{n}{i} \\\\\n\n=\n nT(1) + \\log_3{2}(n(\\ln{\\log_3{n}} + O(1))) \\text{ by equation (A.7)} \\\\\n\n=\n \\Theta(n\\lg{\\lg{n}})\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = \\Theta(n\\lg{\\lg{n}})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{\\lg{n}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}}$ for some constants $c_1 \n 0$ and $c_2 \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\\n\n\\leq\n 3(c_2\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n c_2n\\lg{(\\lg{n} - \\lg3)} + \\frac{n}{\\lg{n}}\n\\end{eqnarray}\n$$\n\n\nIt's not that easy to prove that $T(n) \\leq c_2n\\lg{\\lg{n}}$, let's guess $T(n) \\leq c_2n\\lg{\\lg{n}} - \\frac{n}{\\lg{3n}}$.\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\\n\n\\leq\n 3(c_2\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}} - \\frac{\\frac{n}{3}}{\\lg{3\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n c_2n\\lg{(\\lg{n} - \\lg3)} - \\frac{n}{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\\n\n c_2n\\lg{\\lg{n}}\n\\end{eqnarray}\n$$\n\n\nSo $T(n) = O(n\\lg{\\lg{n}})$. And:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\\n\n\\geq\n 3(c_1\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n c_1n\\lg{(\\lg{n} - \\lg3)} + \\frac{n}{\\lg{n}}\n\\end{eqnarray}\n$$\n\n\nLet's reguess $T(n) \\geq c_1n\\lg{\\lg{3n}}$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\\n\n\\geq\n 3(c_1\\frac{n}{3}\\lg{\\lg{3\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n c_1n\\lg{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\\n\n c_1n\\lg{\\lg{n}}\n\\end{eqnarray}\n$$\n\n\nSo $T(n) = \\Omega(n\\lg{\\lg{n}})$. Thus = $T(n) \\ Theta(n\\lg{\\lg{n}})$.\n\n\nc\n\n\nHere, we have a = 4, b = 2, and $f(n) = \\Theta(n^2\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = \\sqrt{n}$. So $f(n) = \\Omega(n^{\\log_ba + \\epsilon})$ for $\\epsilon \\leq 2$. case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 4f(\\frac{n}{2}) = \\frac{\\sqrt{2}}{2}n^2\\sqrt{n} = cf(n)$, for $c = \\frac{\\sqrt{2}}{2}$. So, the solution to the recurrence is $T(n) = \\Theta(n^2\\sqrt{n})$.\n\n\nd\n\n\nIf n is large enough, then we can ignore the \n-2\n. Then we can solve it by the master method. So we have a = 3, b = 3, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{3}} = n$. Case 2 applies, thus the solution to the recurrence is $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(n\\lg{n})$.\n\n\ne\n\n\nLet's try to solve a general form of the recurrence $T(n) = aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}}, a \n 1$.\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n a(aT(\\frac{n}{a^2}) + \\frac{\\frac{n}{a}}{\\lg{\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n a^2T(\\frac{n}{a^2}) + \\frac{n}{\\lg{n} - \\lg{a}} + \\frac{n}{\\lg{n}} \\\\\n\n=\n \\ldots \\\\\n\n=\n a^iT(\\frac{n}{a^i}) + \\sum_{i = 1}^{\\log_a{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg{a}} \\\\\n\n=\n nT(1) + \\sum_{i = 1}^{\\log_a{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg{a}} \\\\\n\n=\n nT(1) + \\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\lg{n} - i\\lg{a}} \\\\\n\n=\n nT(1) + \\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\frac{\\log_a{n}}{\\log_a{2}} - i\\frac{\\log_a{a}}{\\log_a{2}}} \\\\\n\n=\n nT(1) + \\log_a{2}\\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\log_a{n} - i} \\\\\n\n=\n nT(1) + \\log_a{2}\\sum_{i = 1}^{\\log_a{n}}\\frac{n}{i} \\\\\n\n=\n nT(1) + \\log_a{2}(n(\\ln{\\log_a{n}} + O(1))) \\text{ by equation (A.7)} \\\\\n\n=\n \\Theta(n\\lg{\\lg{n}})\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = \\Theta(n\\lg{\\lg{n}})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{\\lg{n}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}}$ for some constants $c_1 \n 0$ and $c_2 \n 0$. Like the problem b, let's guess $T(n) \\geq c_1n\\lg{\\lg{an}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}} - \\frac{n}{\\lg{an}}$.\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\\n\n\\leq\n a(c_2\\frac{n}{a}\\lg{\\lg{\\frac{n}{a}}} - \\frac{\\frac{n}{a}}{\\lg{a\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n c_2n\\lg{(\\lg{n} - \\lg{a})} - \\frac{n}{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\\n\n c_2n\\lg{\\lg{n}}\n\\end{eqnarray}\n$$\n\n\nSo $T(n) = O(n\\lg{\\lg{n}})$. And:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\\n\n\\geq\n a(c_1\\frac{n}{a}\\lg{\\lg{a\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\\n\n=\n c_1n\\lg{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\\n\n c_1n\\lg{\\lg{n}}\n\\end{eqnarray}\n$$\n\n\nSo $T(n) = \\Omega(n\\lg{\\lg{n}})$. Thus = $T(n) = \\Theta(n\\lg{\\lg{n}})$.\n\n\nThe solution to recurrence $T(n) = 2T(\\frac{n}{2}) + \\frac{n}{\\lg{n}}$ is $\\Theta(n\\lg{\\lg{n}})$.\n\n\nf\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n$ and assume that n is an exact power of 8.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=24mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){n}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{2^2}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^3}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2^2}$}\n        child {node {$\\frac{n}{2^3}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^5}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2^3}$}\n        child {node {$\\frac{n}{2^4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^5}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^6}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=4 of root] {n}[no edge from this parent]\n    child {node {$\\frac{7n}{8}$}[no edge from this parent]\n        child {node {$(\\frac{7}{8})^2n$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(?)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nWe've solved similar problems before. Not all branch reaches at the bottom at the same time. The right most branch reaches at the bottom first. The left most branch is the last one that reaches at the bottom.\n\n\nAnd we can see the total cost over all nodes at depth i is $(\\frac{7}{8})^in$.\n\n\nIf the left most branch reaches at the bottom, and assume the current depth is k, then we assume other branches reach at depth k - 1 at the same time. So we have:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\leq\n \\sum_{i = 0}^{\\lg{n} - 1}(\\frac{7}{8})^in + T(1) \\\\\n\n=\n n\\frac{1 - (\\frac{7}{8})^{\\lg{n}}}{1 - \\frac{7}{8}} + T(1) \\\\\n\n=\n 8n(1 - (\\frac{7}{8})^{\\lg{n}}) + T(1) \\\\\n\n 8n + T(1) \\\\\n\n\\leq\n 9n \\\\\n\n=\n O(n)\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $n \\geq T(1)$.\n\n\nThen if the right most branch reaches at the bottom, other branches have not reach at the bottom. But we assume they also reach at the bottom. So we have:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\geq\n \\sum_{i = 0}^{\\log_8{n} - 1}(\\frac{7}{8})^in + (\\frac{7}{8})^{\\log_8{n}}n \\\\\n\n=\n n\\frac{1 - (\\frac{7}{8})^{\\log_8{n} + 1}}{1 - \\frac{7}{8}} \\\\\n\n=\n 8n(1 - (\\frac{7}{8})^{\\log_8{n} + 1}) \\\\\n\n\\geq\n 8n(1 - (\\frac{7}{8})^{\\log_8{1} + 1}) \\\\\n\n=\n n \\\\\n\n=\n \\Omega(n)\n\\end{eqnarray}\n$$\n\n\nThus we devird a guess of $T(n) = \\Theta(n)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n$ and $T(n) \\leq c_2n$ for some constants $c_1 \n 0$ and $c_2 \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n \\\\\n\n\\geq\n c_1\\frac{n}{2} + c_1\\frac{n}{4} + c_1\\frac{n}{8} + n \\\\\n\n=\n (1 + \\frac{7}{8}c_1)n \\\\\n\n\\geq\n c_1n \\\\\n\n=\n \\Omega(n)\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c_1 \\leq 8$.\n\n\nAnd:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n \\\\\n\n\\leq\n c_2\\frac{n}{2} + c_2\\frac{n}{4} + c_2\\frac{n}{8} + n \\\\\n\n=\n (1 + \\frac{7}{8}c_2)n \\\\\n\n\\leq\n c_2n \\\\\n\n=\n O(n)\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c_2 \\geq 8$.\n\n\nThus $T(n) = \\Theta(n)$.\n\n\ng\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = T(n - 1) + \\frac{1}{n}$.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$\\frac{1}{n}$}\n    child {node {$\\frac{1}{n - 1}$}\n        child {node {$\\frac{1}{n - 2}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$\\frac{1}{n}$}[no edge from this parent]\n    child {node {$\\frac{1}{n - 1}$}[no edge from this parent]\n        child {node {$\\frac{1}{n - 2}$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nThe number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, n - 2$, has a cost of $\\frac{1}{n - i}$. The bottom level, at depth n - 1, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sum_{i = 0}^{n - 2}\\frac{1}{n - i} + \\Theta(1) \\\\\n\n=\n \\sum_{i = 2}^{n}\\frac{1}{i} + \\Theta(1) \\\\\n\n=\n \\ln{n} - 1 + \\Theta(1) \\\\\n\n=\n \\Theta(n)\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = T(n - 1) + \\frac{1}{n}$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1\\lg{n}$ and $T(n) \\leq c_2\\lg{n}$ for some constants $c_1 \n 0$ and $c_2 \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\geq\n c_1\\lg{(n - 1)} + \\frac{1}{n}\n\\end{eqnarray}\n$$\n\n\nIt's not obvious to see that $T(n) \\geq c_1\\lg{n}$. Let's try to guess $T(n) \\geq c_1\\lg{(n + 1)}$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\geq\n c_1\\lg{(n - 1 + 1)} + \\frac{1}{n} \\\\\n\n=\n c_1\\lg{n} + \\frac{1}{n} \\\\\n\n c_1\\lg{n} \\\\\n\n=\n \\Omega(\\lg{n})\n\\end{eqnarray}\n$$\n\n\nSo $T(n) = \\Omega(\\lg{n})$. And:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\leq\n c_2\\lg{(n - 1)} + \\frac{1}{n}\n\\end{eqnarray}\n$$\n\n\nLet's try to reguess $T(n) \\leq c_2\\lg{n} - \\frac{1}{n + 1}$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\leq\n c_2\\lg{(n - 1)} - \\frac{1}{n - 1 + 1} + \\frac{1}{n} \\\\\n\n=\n c_2\\lg{(n - 1)} \\\\\n\n c_2\\lg{n} \\\\\n\n=\n O(\\lg{n})\n\\end{eqnarray}\n$$\n\n\nSo $T(n) = O(\\lg{n})$, thus $T(n) = \\Theta(\\lg{n})$.\n\n\nh\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = T(n - 1) + \\lg{n}$.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$\\lg{n}$}\n    child {node {$\\lg{(n - 1)}$}\n        child {node {$\\lg{(n - 2)}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$\\lg{n}$}[no edge from this parent]\n    child {node {$\\lg{(n - 1)}$}[no edge from this parent]\n        child {node {$\\lg{(n - 2)}$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nThe number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, n - 2$, has a cost of $\\lg({n - i})$. The bottom level, at depth n - 1, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sum_{i = 0}^{n - 2}\\lg{(n - i)} + \\Theta(1) \\\\\n\n=\n \\sum_{i = 2}^{n}\\lg{n} + \\Theta(1) \\\\\n\n=\n \\lg{(n!)} + \\Theta(1) \\\\\n\n=\n \\Theta(n\\lg{n}) \\text{ (we have proved it in 3.2-3)}\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = T(n - 1) + \\lg{n}$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{n}$ and $T(n) \\leq c_2n\\lg{n}$ for some constants $c_1 \n 0$ and $c_2 \n 0$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\geq\n c_1(n - 1)\\lg{(n - 1)} + \\lg{n}\n\\end{eqnarray}\n$$\n\n\nLet's try to reguess $T(n) \\geq c_1(n + 1)\\lg{(n + 1)}$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\geq\n c_1(n - 1 + 1)\\lg{(n - 1 + 1)} + \\lg{n} \\\\\n\n=\n c_1n\\lg{n} + \\lg{n} \\\\\n\n c_1n\\lg{n} \\\\\n\n=\n \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nSo $T(n) = \\Omega(n\\lg{n})$. And:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\leq\n c_2(n - 1)\\lg{(n - 1)} + \\lg{n} \\\\\n\n c_2(n - 1)\\lg{n} + \\lg{n} \\\\\n\n=\n \\lg{n}(c_2(n - 1) + 1) \\\\\n\n\\leq\n c_2n\\lg{n} \\\\\n\n=\n O(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c_2 \\geq 1$.\n\n\nSo $T(n) = O(n\\lg{n})$. Thus $T(n) = \\Theta(n\\lg{n})$.\n\n\ni\n\n\nFirst let's create a recursion tree for the recurrence $T(n) = T(n - 2) + \\frac{1}{\\lg{n}}$.\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$\\frac{1}{\\lg{n}}$}\n    child {node {$\\frac{1}{\\lg{(n - 2)}}$}\n        child {node {$\\frac{1}{\\lg{(n - 4)}}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$\\frac{1}{\\lg{n}}$}[no edge from this parent]\n    child {node {$\\frac{1}{\\lg{(n - 2)}}$}[no edge from this parent]\n        child {node {$\\frac{1}{\\lg{(n - 4)}}$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\nThe number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, \\lg{(n - 1)} - 1$, has a cost of $\\frac{1}{\\lg{(n - 2^i)}}$. The bottom level, at depth $\\lg{(n - 1)}$, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sum_{i = 0}^{\\lg{(n - 1)} - 1}\\frac{1}{\\lg{(n - 2^i)}} + \\Theta(1)\n\\end{eqnarray}\n$$\n\n\nBut I don't know how to compute the sum.\n\n\nj\n\n\nLet's solve it by the iterative method and assume n is an exact power of 2, and n is also a perfect square.\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sqrt{n}T(\\sqrt{n}) + n \\\\\n\n=\n n^{\\frac{1}{2}}T(n^{\\frac{1}{2}}) + n \\\\\n\n=\n n^{\\frac{1}{2}}((n^{\\frac{1}{2}})^{\\frac{1}{2}}T((n^{\\frac{1}{2}})^{\\frac{1}{2}}) + n^{\\frac{1}{2}}) + n \\\\\n\n=\n n^{\\frac{1}{2}}(n^{\\frac{1}{4}}T(n^{\\frac{1}{4}}) + n^{\\frac{1}{2}}) + n \\\\\n\n=\n n^{\\frac{1}{2} + \\frac{1}{4}}T(n^{\\frac{1}{4}}) + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\\n\n=\n n^{\\frac{1}{2} + \\frac{1}{4}}(n^{\\frac{1}{8}}T(n^{\\frac{1}{8}}) + n^{\\frac{1}{4}}) + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\\n\n=\n n^{\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8}}T(n^{\\frac{1}{8}}) + n^{\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{4}} + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\\n\n=\n \\ldots \\\\\n\n=\n n^{\\sum_{i = 1}^{\\lg{\\sqrt{n}}}\\frac{1}{2^i}}T(2) + \\lg{\\sqrt{n}} * n \\\\\n\n n^{\\sum_{i = 1}^{\\infty}\\frac{1}{2^i}}T(2) + \\lg{\\sqrt{n}} * n \\\\\n\n=\n n^{\\frac{\\frac{1}{2}}{1 - \\frac{1}{2}}}T(2) + \\frac{1}{2}n\\lg{n} \\\\\n\n=\n nT(2) + \\frac{1}{2}n\\lg{n} \\\\\n\n=\n O(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nThus, we have derived a guess of $T(n) = O(n\\lg{n})$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn\\lg{n}$ for some constant c \n 0. So:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\sqrt{n}T(\\sqrt{n}) + n \\\\\n\n\\leq\n \\sqrt{n}c\\sqrt{n}\\lg{\\sqrt{n}} + n \\\\\n\n=\n cn\\lg{\\sqrt{n}} + n \\\\\n\n=\n \\frac{1}{2}cn\\lg{n} + n\n\\end{eqnarray}\n$$\n\n\nBecause $\\frac{1}{2}cn\\lg{n} + n - cn\\lg{n} = n(1 - \\frac{1}{2}c\\lg{n}) \\leq n(1 - \\frac{1}{2}c)$ when $n \\geq 2$. So $n(1 - \\frac{1}{2}c) \\leq 0$ when $c \\geq 2$. So $T(n) \\leq cn\\lg{n}$ when $c \\geq 2$ and $n \\geq 2$. Thus, $T(n) = O(n\\lg{n})$.\n\n\nBut I don't know how to get the lower bound.\n\n\n4-4\n\n\na\n\n\n$$\n\\begin{eqnarray}\nz + z\\mathcal{F}(z) + z^2\\mathcal{F}(Z) \n=\n z + z\\sum_{i = 0}^{\\infty}F_iz^i + z^2\\sum_{i = 0}^{\\infty}F_iz^i \\\\\n\n=\n z + (0 + z^2 + z^3 + 2z^4 + 3z^5 + \\ldots) + (0 + z^3 + z^4 + 2z^5 + \\ldots) \\\\\n\n=\n 0 + z + z^2 + 2z^3 + 3z^4 + 5z^5 + \\ldots \\\\\n\n=\n \\mathcal{F}(z)\n\\end{eqnarray}\n$$\n\n\nb\n\n\nBecause $\\mathcal{F}(z) = z + z\\mathcal{F}(z) + z^2\\mathcal{F}(z)$, so $\\mathcal{F}(z) - z\\mathcal{F}(z) - z^2\\mathcal{F}(z) = z$, thus $\\mathcal{F}(z)(1 - z - z^2) = z$, so $\\mathcal{F}(z) = \\frac{z}{1 - z - z^2}$.\n\n\nAnd:\n\n\n$$\n\\begin{eqnarray}\n\\frac{z}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \n=\n \\frac{z}{1 - (\\phi + \\hat\\phi)z + \\phi\\hat\\phi{z^2}} \\\\\n\n=\n \\frac{z}{1 - (\\frac{1 - \\sqrt{5}}{2} + \\frac{1 + \\sqrt{5}}{2})z + \\frac{1 - \\sqrt{5}}{2}\\frac{1 + \\sqrt{5}}{2}z^2} \\\\\n\n=\n \\frac{z}{1 - z - z^2}\n\\end{eqnarray}\n$$\n\n\nAnd:\n\n\n$$\n\\begin{eqnarray}\n\\frac{1}{\\sqrt{5}}(\\frac{1}{1 - \\phi{z}} - \\frac{1}{1 - \\hat\\phi{z}}) \n=\n \\frac{1}{\\sqrt{5}}\\frac{1 - \\hat\\phi{z} - 1 + \\phi{z}}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\\n\n=\n \\frac{1}{\\sqrt{5}}\\frac{z(\\phi - \\hat\\phi)}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\\n\n=\n \\frac{1}{\\sqrt{5}}\\frac{z(\\frac{1 + \\sqrt{5}}{2} - \\frac{1 - \\sqrt{5}}{2})}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\\n\n=\n \\frac{1}{\\sqrt{5}}\\frac{z\\sqrt{5}}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\\n\n=\n \\frac{z}{(1 - \\phi{z})(1 - \\hat\\phi{z})}\n\\end{eqnarray}\n$$\n\n\nc\n\n\n$$\n\\begin{eqnarray}\n\\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)z^i \n=\n \\frac{1}{\\sqrt{5}}\\sum_{i = 0}^{\\infty}(\\phi^iz^i - \\hat\\phi^iz^i) \\\\\n\n=\n \\frac{1}{\\sqrt{5}}(\\sum_{i = 0}^{\\infty}\\phi^iz^i - \\sum_{i = 0}^{\\infty}\\hat\\phi^iz^i) \\\\\n\n=\n \\frac{1}{\\sqrt{5}}(\\frac{1}{1 - \\phi{z}} - \\frac{1}{1 - \\hat\\phi{z}}) \\\\\n\n=\n \\mathcal{F}(z)\n\\end{eqnarray}\n$$\n\n\nd\n\n\nBecause $\\mathcal{F}(z) = \\sum_{i = 0}^{\\infty}F_iz^i$ and $\\mathcal{F}(z) = \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)z^i$, so we have $F_i = \\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)$. Since $|\\hat\\phi| \n 1$, we have $\\frac{|\\hat\\phi^i|}{\\sqrt{5}} \n \\frac{1}{\\sqrt{5}} \n \\frac{1}{2}$, which implies that $F_i = \\lfloor \\frac{\\phi^i}{\\sqrt{5}} + \\frac{1}{2} \\rfloor$, which is to say that the ith Fibonacci number $F_i$ is equal to $\\frac{\\phi^i}{\\sqrt{5}}$ rounded to the nearest integer.\n\n\n4-5\n\n\na\n\n\n4-6\n\n\na\n\n\nThe \"only if\" part is easy. If an array is Monge, then for all i, j, k and l such that $l \\leq i \n k \\leq m$ and $l \\leq j \n l \\leq n$ we have $A[i, j] + A[k, l] \\leq A[i, l] + A[k, j]$. So we let k = i + 1, l = j + 1, then we have $A[i, j] + A[i + 1, j + 1] \\leq A[i, j + 1] + A[i + 1, j]$. So we proved the \"only if\" part.\n\n\nThen let's prove the \"if\" part, we'll use induction separately on rows and columns. Let's get on columns first. The base case is already given, let's assume $A[i, j] + A[i + 1, j + k] \\leq A[i, j + k] + A[i + 1, j]$ for $k \\geq 1$ and $j + k \\leq n$. Then we need to prove $A[i, j] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j]$.\n\n\nAccording to the definition, we have $A[i, j + k] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j + k]$, thus $A[i, j] + A[i + 1, j + k] + A[i, j + k] + A[i + 1, j + k + 1] \\leq A[i, j + k] + A[i + 1, j] + A[i, j + k + 1] + A[i + 1, j + k]$. So we get $A[i, j] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j]$.\n\n\nSo the induction is correct, and similarly we can prove $A[i, j + 1] + A[i + 1, j + k] \\leq A[i, j + k] + A[i + 1, j + 1] \\, (j + k \\leq n), \\ldots, A[i, j + p] + A[i + 1, j + k] \\leq A[i, j + k] + A[i + 1, j + p] \\, (j + p \\leq j + k - 1, j + k \\leq n)$, thus we can see that all adjacent rows are Monge arrays. Similarly, we can use the induction on rows, so all adjacent columns are Monge arrays. \n\n\nFor any $m_1 x n_1$ array, if $m_1 = 2$ or $n_1 = 2$, then the subarray is a Monge array. And we need to prove the subarray is also a Monge array if $m_1 \\geq 3$ and $n_1 \\geq 3$.\n\n\nLet's assume the subarray starts at row $A[i_1, j_1]$ and ends at $A[i_2, j_2]$. Since any adjacent columns are Monge arrays, so we have:\n\n\n$$A[i_1, j_1] + A[i_2, j_1 + 1] \\leq A[i_1, j_1 + 1] + A[i_2, j_1]$$\n$$A[i_1, j_1 + 1] + A[i_2, j_1 + 2] \\leq A[i_1, j_1 + 2] + A[i_2, j_1 + 1]$$\n$$\\ldots$$\n$$A[i_1, j_2 - 1] + A[i_2, j_2] \\leq A[i_1, j_2] + A[i_2, j_2 - 1]$$\n\n\nThen we sum all inequations together, so we have:\n\n\n$$A[i_1, j_1] + A[i_2, j_2] \\leq A[i_1, j_2] + A[i_2, j_1]$$, which means the subarray $A[i_1, j_1]$ to $A[i_2, j_2]$ is a Monge array when $i_2 - i_1 \\geq 3$ and $j_2 - j_1 \\geq 3$. So for all subarrays in A, they are Monge arrays. So we proved the \"if\" part.\n\n\nb\n\n\n\\begin{matrix}\n37 \n 23 \n 22 \n 32 \\\\\n21 \n  6 \n  5 \n 10 \\\\\n53 \n 34 \n 30 \n 31 \\\\\n32 \n 13 \n  9 \n  6 \\\\\n43 \n 21 \n 15 \n  8 \\\\\n\\end{matrix}\n\n\nc\n\n\nLet's consider row i and row i + 1. Let $f(i) = j_1$ and $f(i + 1) = j_2$ and assume $f(i) \n f(i + 1)$, so we have $j_1 \n j_2$. Since A is a Monge array, so we have $A[i, j_2] + A[i + 1, j_1] \\leq A[i, j_1] + A[i + 1, j_2]$. But according to the definition of f(i), we have $A[i, j_2] \n A[i, j_1]$ and $A[i + 1, j_2] \n A[i + 1, j_1]$, combine them together: $A[i, j_2] + A[i + 1, j_1] \n A[i, j_1] + A[i + 1, j_2]$. Thus the assumption is wrong. So $f(i) \\leq f(i + 1)$. So $f(1) \\leq f(2) \\leq \\ldots \\leq f(m)$ for any m x n Monge array.\n\n\nd\n\n\nLet's assume $m = 2k, k = 1, 2, \\ldots$, from the previous question we have $f(2k - 2) \\leq f(2k - 1) \\leq f(2k)$. We want to get f(2k - 1) when f(2k - 2) and f(2k) are already known. We only need to compare the numbers from A[2k - 1][f(2k - 2)] to A[2k - 1][f(2k)], which costs at most $O(f(2k) - f(2k - 2) + 1)$. Thus:\n\n\n$$\n\\begin{eqnarray}\nT \n=\n \\sum_{k = 1}^{\\frac{m}{2}}O(f(2k) - f(2k - 2) + 1) \\\\\n\n=\n (f(2) - f(0) + 1) + (f(4) - f(2) + 1) + \\ldots + (f(m) - f(m - 2) + 1) \\\\\n\n=\n f(m) - f(0) + \\frac{m}{2} \\\\\n\n\\leq\n n - 0 + \\frac{m}{2} \\\\\n\n n + m \\\\\n\n=\n O(m + n)\n\\end{eqnarray}\n$$\n\n\ne\n\n\n$$\n\\begin{eqnarray}\nT(m) \n=\n T(\\frac{m}{2}) + O(m + n) \\\\\n\n=\n T(\\frac{m}{4}) + O(\\frac{m}{2} + n) + O(m + n) \\\\\n\n=\n T(\\frac{m}{8}) + O(\\frac{m}{4} + n) + O(\\frac{m}{2} + n) + O(m + n) \\\\\n\n=\n \\ldots \\\\\n\n=\n T(1) + \\sum_{i = 0}^{\\lg{m} - 1}O(\\frac{m}{2^i}) + \\sum_{i = 0}^{\\lg{m} - 1}O(n) \\\\\n\n T(1) + \\sum_{i = 0}^{\\infty}O(\\frac{m}{2^i}) + \\sum_{i = 0}^{\\lg{m} - 1}O(n) \\\\\n\n=\n T(1) + O(m\\frac{1}{1 - \\frac{1}{2}}) + O(n\\lg{m}) \\\\\n\n=\n O(1) + O(2m) + O(n\\lg{m}) \\\\\n\n=\n O(m + n\\lg{m})\n\\end{eqnarray}\n$$\n\n\ndef leftmost_min_element_in_each_row_of_monge_array(matrix):\n    # Column index of leftmost min element in each row\n    leftmost_min_element_in_each_row = [0] * len(matrix)\n\n    leftmost_min_element_divide_and_conquer(\n        matrix, leftmost_min_element_in_each_row, 0, len(matrix) - 1, 1)\n\n    return leftmost_min_element_in_each_row\n\n\ndef leftmost_min_element_divide_and_conquer(\n        matrix, leftmost_min_element_in_each_row, row_start, row_end, step):\n    if row_start == row_end:\n        leftmost_min_element_in_each_row[row_start] = \\\n            find_leftmost_min_element_in_row(\n                matrix, row_start, 0, len(matrix[row_start]) - 1)\n    else:\n        # Construct a submatrix consisting of the even-numbered rows\n        sub_matrix_row_start = row_start + step\n        sub_matrix_row_end = 0\n\n        if row_end % 2 == 0 or ((row_end - row_start) // step) % 2 == 0:\n            sub_matrix_row_end = row_end - step\n        elif ((row_end - row_start) // step) % 2 == 1:\n            sub_matrix_row_end = row_end\n\n        leftmost_min_element_divide_and_conquer(\n            matrix, leftmost_min_element_in_each_row,\n            sub_matrix_row_start, sub_matrix_row_end, step * 2)\n\n        leftmost_min_element_in_odd_numbered_rows(\n            matrix, leftmost_min_element_in_each_row, row_start, row_end, step)\n\n\ndef find_leftmost_min_element_in_row(matrix, row, column_start, column_end):\n    min_column_index = 0\n\n    for column in range(column_start, column_end + 1):\n        if matrix[row][column] \n matrix[row][min_column_index]:\n            min_column_index = column\n\n    return min_column_index\n\n\ndef leftmost_min_element_in_odd_numbered_rows(\n        matrix, leftmost_min_element_in_each_row, row_start, row_end, step):\n    for odd_numbered_row in range(row_start, row_end + 1, step * 2):\n        prev_even_numbered_row = odd_numbered_row - step\n        next_even_numbered_row = odd_numbered_row + step\n        column_start = -1\n        column_end = -1\n\n        if prev_even_numbered_row \n= row_start and \\\n                next_even_numbered_row \n= row_end:\n            column_start = \\\n                leftmost_min_element_in_each_row[prev_even_numbered_row]\n            column_end = \\\n                leftmost_min_element_in_each_row[next_even_numbered_row]\n        elif prev_even_numbered_row \n= row_start:\n            column_start = \\\n                leftmost_min_element_in_each_row[prev_even_numbered_row]\n            column_end = len(matrix[0]) - 1\n        elif next_even_numbered_row \n= row_end:\n            column_start = 0\n            column_end = \\\n                leftmost_min_element_in_each_row[next_even_numbered_row]\n\n        leftmost_min_element_in_each_row[odd_numbered_row] = \\\n            find_leftmost_min_element_in_row(\n                matrix, odd_numbered_row, column_start, column_end)", 
            "title": "Problems"
        }, 
        {
            "location": "/4-Divide-and-Conquer/Problems/#problems", 
            "text": "", 
            "title": "Problems"
        }, 
        {
            "location": "/4-Divide-and-Conquer/Problems/#4-1", 
            "text": "a  Here, we have a = 2, b = 2, and $f(n) = \\Theta(n^4)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{2}} = n$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 3$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{2}) = \\frac{n^4}{8} = cf(n)$, for $c = \\frac{1}{8}$. So, the solution to the recurrence is $T(n) = \\Theta(n^4)$.  b  Here, we have a = 1, $b = \\frac{10}{7}$, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_{\\frac{10}{7}}{1}} = 1$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 1$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = f(\\frac{7n}{10}) = \\frac{7n}{10} = cf(n)$, for $c = \\frac{7}{10}$. So, the solution to the recurrence is $T(n) = \\Theta(n)$.  c  Here, we have a = 16, b = 4, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{16}} = n^2$. So case 2 applies, so the solution to the recurrence is $T(n) = \\Theta(n^2\\lg{n})$.  d  Here, we have a = 7, b = 3, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{7}}$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 2 - \\log_3{7}$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 7f(\\frac{n}{3}) = \\frac{7n^2}{9} = cf(n)$, for $c = \\frac{7}{9}$. So, the solution to the recurrence is $T(n) = \\Theta(n^2)$.  e  Here, we have a = 7, b = 2, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{7}} = n^{\\lg{7}}$. Since $f(n) = O(n^{\\log_b{a} - \\epsilon})$, where $\\epsilon \\leq \\lg{\\frac{7}{4}}$, case 1 applies. So, the solution to the recurrence is $T(n) = \\Theta(n^{\\lg7})$.  f  Here, we have a = 2, b = 4, and $f(n) = \\Theta(\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = \\sqrt{n}$, case 2 applies. So, the solution to the recurrence is $T(n) = \\Theta(\\sqrt{n}\\lg{n})$.  g  First let's create a recursion tree for the recurrence $T(n) = T(n - 2) + n^2$.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$n^2$}\n    child {node {$(n - 2)^2$}\n        child {node {$(n - 4)^2$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$n^2$}[no edge from this parent]\n    child {node {$(n - 2)^2$}[no edge from this parent]\n        child {node {$(n - 4)^2$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}   The number of nodes at depth i is 1. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\frac{n - 1}{2} - 1$, has a cost of $(n - 2i)^2$. So the total cost over all nodes at depth i, is $(n - 2i)^2$. The bottom level, at depth $\\frac{n - 1}{2}$, has 1 node, which contributing cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}(n - 2i)^2 + \\Theta(1) \\\\ =  \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}(n^2 - 2ni + 4i^2) + \\Theta(1) \\\\ =  \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}n^2 - \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}2ni + \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}4i^2 + \\Theta(1) \\\\ =  \\frac{n - 1}{2}n^2 - 2n\\frac{(\\frac{n - 1}{2} - 1)(1 + \\frac{n - 1}{2} - 1)}{2} + 4\\frac{(\\frac{n - 1}{2} - 1)(\\frac{n - 1}{2} - 1 + 1)(2(\\frac{n - 1}{2} - 1) + 1)}{6} + \\Theta(1) \\\\ =  \\frac{n^2(n - 1)}{2} - \\frac{n(n - 1)(n - 3)}{4} + \\frac{(n - 1)(n - 2)(n - 3)}{6} + \\Theta(1) \\\\ =  \\frac{5n^3 - 6n^2 + 13n -12}{6} + \\Theta(1) \\\\ =  \\Theta(n^3)\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = \\Theta(n^3)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n^3$ and $T(n) \\leq c_2n^3$ for some constants $c_1   0$ and $c_2   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(n - 2) + n^2 \\\\ \\geq  c_1(n - 2)^3 + n^2 \\\\ =  c_1n^3 + (1 - 6c_1)n^2 + 4c_1(3n - 2) \\\\  c_1n^3\n\\end{eqnarray}\n$$  where the last step holds as long as $c_1 \\leq \\frac{1}{6}$.  So $T(n) = \\Omega(n^3)$.  And:  $$\n\\begin{eqnarray}\nT(n)  =  T(n - 2) + n^2 \\\\ \\leq  c_2(n - 2)^3 + n^2 \\\\ =  c_2n^3 + n((1 - 6c_2)n + 12c_2) - 8c_2 \\\\ \\leq  c_2n^3\n\\end{eqnarray}\n$$  where the last step holds as long as $c_2   \\frac{1}{6}$ and $n \\geq \\frac{12c_2}{6c_2 - 1}$.  So $T(n) = O(n^3)$, thus $T(n) = \\Theta(n^3)$.", 
            "title": "4-1"
        }, 
        {
            "location": "/4-Divide-and-Conquer/Problems/#4-2", 
            "text": "a  Here is the pseudocode of recursive binary search algorithm:  RECURSIVE-BINARY-SEARCH(A, v, low, high)\n\nif low  = high\n    middle = (low + high) / 2\n\n    if A[middle]   v\n        return RECURSIVE-BINARY-SEARCH(A, v, middle + 1, high)\n    else if A[middle]   v\n        return RECURSIVE-BINARY-SEARCH(A, v, low, middle - 1)\n    else\n        return middle\n\nreturn NIL  An array is passed by pointer  Before it halves the problem size, it needs to do some operations like comparing  low  and  high , calculating  middle . But they are constant operations, we can let it be $\\Theta(1)$.  So $T(n) = T(\\frac{n}{2}) + \\Theta(1)$. Here, we have a = 1, b = 2, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. So case 2 applies, thus $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(\\lg{n}) = \\Theta(\\lg{N})$.  An array is passed by copying  Each time it halves the problem size, it needs additional $\\Theta(N)$ operation to copy the array. So $T(n) = T(\\frac{n}{2}) + \\Theta(N) + \\Theta(1) = T(\\frac{n}{2}) + \\Theta(N) = T(\\frac{n}{4}) + \\Theta(N) + \\Theta(N) = \\ldots = T(1) + \\lg{n}\\Theta(N) = \\Theta(N\\lg{N})$.  An array is passed by range  Each time it halves the problem size, it needs additional $\\Theta(n)$ operation to copy the array. So $T(n) = T(\\frac{n}{2}) + \\Theta(n) + \\Theta(1) = T(\\frac{n}{2}) + \\Theta(n)$.  Here, we have a = 1, b = 2, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 1$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = f(\\frac{n}{2}) = \\Theta(\\frac{n}{2}) = c\\Theta(n)$, for $c = \\frac{1}{2}$. So, the solution to the recurrence is $T(n) = \\Theta(n) = \\Theta(N)$.  b  Here is the pseudocode of merge sor algorithm:  MERGE-SORT(A, p, r)\n\nif p   r\n    q = (p + r) / 2\n    MERGE-SORT(A, p ,q)\n    MERGE-SORT(A, q + 1, r)\n    MERGE(A, p, q, r)  An array is passed by pointer  We already know the solution that $T(n) = \\Theta(n\\lg{n}) = \\Theta(N\\lg{N})$.  An array is passed by copying  From the pseudocode we know it needs to pass the array 3 times to subroutine. So we have $T(n) = 2T(\\frac{n}{2}) + \\Theta(n) + 3\\Theta(N) = 2T(\\frac{n}{2}) + \\Theta(n) + \\Theta(N)$. Let's use the iterative method to solve it:  $$\n\\begin{eqnarray}\nT(n)  =  2T(\\frac{n}{2}) + \\Theta(n) + \\Theta(N) \\\\ =  2(2T(\\frac{n}{4}) + \\Theta(\\frac{n}{2}) + \\Theta(N)) + \\Theta(n) + \\Theta(N) \\\\ =  4T(\\frac{n}{4}) + 2\\Theta(\\frac{n}{2}) + 2\\Theta(N) + \\Theta(n) + \\Theta(N) \\\\ =  4T(\\frac{n}{4}) + 2\\Theta(n) + 3\\Theta(N) \\\\ =  \\ldots \\\\ =  2^iT(\\frac{n}{2^i}) + i\\Theta(n) + (1 + 2 + \\ldots + 2^{i - 1})\\Theta(N) \\text{ for } i = 1, 2, \\ldots, \\lg{n} \\\\ =  2^{\\lg{n}}T(1) + \\lg{n}\\Theta(n) + \\Theta(N)\\frac{1 - 2^{\\lg{n}}}{1 - 2} \\\\ =  nT(1) + \\Theta(n\\lg{n}) + \\Theta(Nn) \\\\ =  \\Theta(N^2)\n\\end{eqnarray}\n$$  An array is passed by range  It needs additional $3\\Theta(n)$ to copy the array. So $T(n) = 2T(\\frac{n}{2}) + \\Theta(n) + 3\\Theta(n) = 2T(\\frac{n}{2}) + \\Theta(n) = \\Theta(n\\lg{n}) = \\Theta(N\\lg{N})$.", 
            "title": "4-2"
        }, 
        {
            "location": "/4-Divide-and-Conquer/Problems/#4-3", 
            "text": "a  Here, we have a = 4, b = 3, and $f(n) = \\Theta(n\\lg{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_3{4}}$.  In problems 3-2-a, we proved that $\\lg^k{n} = O(n^\\epsilon)$ for $k \\geq 1$ and $\\epsilon   0$, so $\\lg{n} = O(n^{\\epsilon})$. Since $\\log_3{4} \\approx 1.2618595071429148$, so $f(n) = O(n^{\\log_ba - \\epsilon})$ for $\\epsilon \\leq 0.26$. So case 1 applies, the solution to the recurrence is $T(n) = \\Theta(n^{\\log_34})$.  b  Here, we have a = 3, b = 3, and $f(n) = \\Theta(\\frac{n}{\\lg{n}})$, and thus we have that $n^{\\log_ba} = n^{\\log_3{3}} = n$. So $f(n) = O(n^{\\log_ba})$. But it's not that easy to prove that $f(n) = O(n^{\\log_ba - \\epsilon})$.  Let's solve it by iterative method:  $$\n\\begin{eqnarray}\nT(n)  =  3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ =  3(3T(\\frac{n}{9}) + \\frac{\\frac{n}{3}}{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ =  9T(\\frac{n}{9}) + \\frac{n}{\\lg{n} - \\lg3} + \\frac{n}{\\lg{n}} \\\\ =  \\ldots \\\\ =  3^iT(\\frac{n}{3^i}) + \\sum_{i = 1}^{\\log_3{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg3} \\\\ =  nT(1) + \\sum_{i = 1}^{\\log_3{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg3} \\\\ =  nT(1) + \\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\lg{n} - i\\lg3} \\\\ =  nT(1) + \\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\frac{\\log_3{n}}{\\log_3{2}} - i\\frac{\\log_3{3}}{\\log_3{2}}} \\\\ =  nT(1) + \\log_3{2}\\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\log_3{n} - i} \\\\ =  nT(1) + \\log_3{2}\\sum_{i = 1}^{\\log_3{n}}\\frac{n}{i} \\\\ =  nT(1) + \\log_3{2}(n(\\ln{\\log_3{n}} + O(1))) \\text{ by equation (A.7)} \\\\ =  \\Theta(n\\lg{\\lg{n}})\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = \\Theta(n\\lg{\\lg{n}})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{\\lg{n}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}}$ for some constants $c_1   0$ and $c_2   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ \\leq  3(c_2\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ =  c_2n\\lg{(\\lg{n} - \\lg3)} + \\frac{n}{\\lg{n}}\n\\end{eqnarray}\n$$  It's not that easy to prove that $T(n) \\leq c_2n\\lg{\\lg{n}}$, let's guess $T(n) \\leq c_2n\\lg{\\lg{n}} - \\frac{n}{\\lg{3n}}$.  $$\n\\begin{eqnarray}\nT(n)  =  3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ \\leq  3(c_2\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}} - \\frac{\\frac{n}{3}}{\\lg{3\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ =  c_2n\\lg{(\\lg{n} - \\lg3)} - \\frac{n}{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\  c_2n\\lg{\\lg{n}}\n\\end{eqnarray}\n$$  So $T(n) = O(n\\lg{\\lg{n}})$. And:  $$\n\\begin{eqnarray}\nT(n)  =  3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ \\geq  3(c_1\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ =  c_1n\\lg{(\\lg{n} - \\lg3)} + \\frac{n}{\\lg{n}}\n\\end{eqnarray}\n$$  Let's reguess $T(n) \\geq c_1n\\lg{\\lg{3n}}$. So:  $$\n\\begin{eqnarray}\nT(n)  =  3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ \\geq  3(c_1\\frac{n}{3}\\lg{\\lg{3\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ =  c_1n\\lg{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\  c_1n\\lg{\\lg{n}}\n\\end{eqnarray}\n$$  So $T(n) = \\Omega(n\\lg{\\lg{n}})$. Thus = $T(n) \\ Theta(n\\lg{\\lg{n}})$.  c  Here, we have a = 4, b = 2, and $f(n) = \\Theta(n^2\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = \\sqrt{n}$. So $f(n) = \\Omega(n^{\\log_ba + \\epsilon})$ for $\\epsilon \\leq 2$. case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 4f(\\frac{n}{2}) = \\frac{\\sqrt{2}}{2}n^2\\sqrt{n} = cf(n)$, for $c = \\frac{\\sqrt{2}}{2}$. So, the solution to the recurrence is $T(n) = \\Theta(n^2\\sqrt{n})$.  d  If n is large enough, then we can ignore the  -2 . Then we can solve it by the master method. So we have a = 3, b = 3, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{3}} = n$. Case 2 applies, thus the solution to the recurrence is $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(n\\lg{n})$.  e  Let's try to solve a general form of the recurrence $T(n) = aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}}, a   1$.  $$\n\\begin{eqnarray}\nT(n)  =  aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\ =  a(aT(\\frac{n}{a^2}) + \\frac{\\frac{n}{a}}{\\lg{\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\ =  a^2T(\\frac{n}{a^2}) + \\frac{n}{\\lg{n} - \\lg{a}} + \\frac{n}{\\lg{n}} \\\\ =  \\ldots \\\\ =  a^iT(\\frac{n}{a^i}) + \\sum_{i = 1}^{\\log_a{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg{a}} \\\\ =  nT(1) + \\sum_{i = 1}^{\\log_a{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg{a}} \\\\ =  nT(1) + \\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\lg{n} - i\\lg{a}} \\\\ =  nT(1) + \\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\frac{\\log_a{n}}{\\log_a{2}} - i\\frac{\\log_a{a}}{\\log_a{2}}} \\\\ =  nT(1) + \\log_a{2}\\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\log_a{n} - i} \\\\ =  nT(1) + \\log_a{2}\\sum_{i = 1}^{\\log_a{n}}\\frac{n}{i} \\\\ =  nT(1) + \\log_a{2}(n(\\ln{\\log_a{n}} + O(1))) \\text{ by equation (A.7)} \\\\ =  \\Theta(n\\lg{\\lg{n}})\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = \\Theta(n\\lg{\\lg{n}})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{\\lg{n}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}}$ for some constants $c_1   0$ and $c_2   0$. Like the problem b, let's guess $T(n) \\geq c_1n\\lg{\\lg{an}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}} - \\frac{n}{\\lg{an}}$.  $$\n\\begin{eqnarray}\nT(n)  =  aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\ \\leq  a(c_2\\frac{n}{a}\\lg{\\lg{\\frac{n}{a}}} - \\frac{\\frac{n}{a}}{\\lg{a\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\ =  c_2n\\lg{(\\lg{n} - \\lg{a})} - \\frac{n}{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\  c_2n\\lg{\\lg{n}}\n\\end{eqnarray}\n$$  So $T(n) = O(n\\lg{\\lg{n}})$. And:  $$\n\\begin{eqnarray}\nT(n)  =  aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\ \\geq  a(c_1\\frac{n}{a}\\lg{\\lg{a\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\ =  c_1n\\lg{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\  c_1n\\lg{\\lg{n}}\n\\end{eqnarray}\n$$  So $T(n) = \\Omega(n\\lg{\\lg{n}})$. Thus = $T(n) = \\Theta(n\\lg{\\lg{n}})$.  The solution to recurrence $T(n) = 2T(\\frac{n}{2}) + \\frac{n}{\\lg{n}}$ is $\\Theta(n\\lg{\\lg{n}})$.  f  First let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n$ and assume that n is an exact power of 8.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\\tikzstyle{level 1}=[sibling distance=24mm]\n\\tikzstyle{level 2}=[sibling distance=8mm]\n\\tikzstyle{level 3}=[sibling distance=4mm]\n\n\\node (root){n}\n    child {node {$\\frac{n}{2}$}\n        child {node {$\\frac{n}{2^2}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^3}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2^2}$}\n        child {node {$\\frac{n}{2^3}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^5}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}}\n    child {node {$\\frac{n}{2^3}$}\n        child {node {$\\frac{n}{2^4}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^5}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}\n        child {node {$\\frac{n}{2^6}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=4 of root] {n}[no edge from this parent]\n    child {node {$\\frac{7n}{8}$}[no edge from this parent]\n        child {node {$(\\frac{7}{8})^2n$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(?)$}}}}};\n\\end{tikzpicture}\n\\end{document}   We've solved similar problems before. Not all branch reaches at the bottom at the same time. The right most branch reaches at the bottom first. The left most branch is the last one that reaches at the bottom.  And we can see the total cost over all nodes at depth i is $(\\frac{7}{8})^in$.  If the left most branch reaches at the bottom, and assume the current depth is k, then we assume other branches reach at depth k - 1 at the same time. So we have:  $$\n\\begin{eqnarray}\nT(n)  \\leq  \\sum_{i = 0}^{\\lg{n} - 1}(\\frac{7}{8})^in + T(1) \\\\ =  n\\frac{1 - (\\frac{7}{8})^{\\lg{n}}}{1 - \\frac{7}{8}} + T(1) \\\\ =  8n(1 - (\\frac{7}{8})^{\\lg{n}}) + T(1) \\\\  8n + T(1) \\\\ \\leq  9n \\\\ =  O(n)\n\\end{eqnarray}\n$$  where the last step holds as long as $n \\geq T(1)$.  Then if the right most branch reaches at the bottom, other branches have not reach at the bottom. But we assume they also reach at the bottom. So we have:  $$\n\\begin{eqnarray}\nT(n)  \\geq  \\sum_{i = 0}^{\\log_8{n} - 1}(\\frac{7}{8})^in + (\\frac{7}{8})^{\\log_8{n}}n \\\\ =  n\\frac{1 - (\\frac{7}{8})^{\\log_8{n} + 1}}{1 - \\frac{7}{8}} \\\\ =  8n(1 - (\\frac{7}{8})^{\\log_8{n} + 1}) \\\\ \\geq  8n(1 - (\\frac{7}{8})^{\\log_8{1} + 1}) \\\\ =  n \\\\ =  \\Omega(n)\n\\end{eqnarray}\n$$  Thus we devird a guess of $T(n) = \\Theta(n)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n$ and $T(n) \\leq c_2n$ for some constants $c_1   0$ and $c_2   0$. So:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n \\\\ \\geq  c_1\\frac{n}{2} + c_1\\frac{n}{4} + c_1\\frac{n}{8} + n \\\\ =  (1 + \\frac{7}{8}c_1)n \\\\ \\geq  c_1n \\\\ =  \\Omega(n)\n\\end{eqnarray}\n$$  where the last step holds as long as $c_1 \\leq 8$.  And:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n \\\\ \\leq  c_2\\frac{n}{2} + c_2\\frac{n}{4} + c_2\\frac{n}{8} + n \\\\ =  (1 + \\frac{7}{8}c_2)n \\\\ \\leq  c_2n \\\\ =  O(n)\n\\end{eqnarray}\n$$  where the last step holds as long as $c_2 \\geq 8$.  Thus $T(n) = \\Theta(n)$.  g  First let's create a recursion tree for the recurrence $T(n) = T(n - 1) + \\frac{1}{n}$.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$\\frac{1}{n}$}\n    child {node {$\\frac{1}{n - 1}$}\n        child {node {$\\frac{1}{n - 2}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$\\frac{1}{n}$}[no edge from this parent]\n    child {node {$\\frac{1}{n - 1}$}[no edge from this parent]\n        child {node {$\\frac{1}{n - 2}$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}   The number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, n - 2$, has a cost of $\\frac{1}{n - i}$. The bottom level, at depth n - 1, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sum_{i = 0}^{n - 2}\\frac{1}{n - i} + \\Theta(1) \\\\ =  \\sum_{i = 2}^{n}\\frac{1}{i} + \\Theta(1) \\\\ =  \\ln{n} - 1 + \\Theta(1) \\\\ =  \\Theta(n)\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = T(n - 1) + \\frac{1}{n}$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1\\lg{n}$ and $T(n) \\leq c_2\\lg{n}$ for some constants $c_1   0$ and $c_2   0$. So:  $$\n\\begin{eqnarray}\nT(n)  \\geq  c_1\\lg{(n - 1)} + \\frac{1}{n}\n\\end{eqnarray}\n$$  It's not obvious to see that $T(n) \\geq c_1\\lg{n}$. Let's try to guess $T(n) \\geq c_1\\lg{(n + 1)}$. So:  $$\n\\begin{eqnarray}\nT(n)  \\geq  c_1\\lg{(n - 1 + 1)} + \\frac{1}{n} \\\\ =  c_1\\lg{n} + \\frac{1}{n} \\\\  c_1\\lg{n} \\\\ =  \\Omega(\\lg{n})\n\\end{eqnarray}\n$$  So $T(n) = \\Omega(\\lg{n})$. And:  $$\n\\begin{eqnarray}\nT(n)  \\leq  c_2\\lg{(n - 1)} + \\frac{1}{n}\n\\end{eqnarray}\n$$  Let's try to reguess $T(n) \\leq c_2\\lg{n} - \\frac{1}{n + 1}$. So:  $$\n\\begin{eqnarray}\nT(n)  \\leq  c_2\\lg{(n - 1)} - \\frac{1}{n - 1 + 1} + \\frac{1}{n} \\\\ =  c_2\\lg{(n - 1)} \\\\  c_2\\lg{n} \\\\ =  O(\\lg{n})\n\\end{eqnarray}\n$$  So $T(n) = O(\\lg{n})$, thus $T(n) = \\Theta(\\lg{n})$.  h  First let's create a recursion tree for the recurrence $T(n) = T(n - 1) + \\lg{n}$.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$\\lg{n}$}\n    child {node {$\\lg{(n - 1)}$}\n        child {node {$\\lg{(n - 2)}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$\\lg{n}$}[no edge from this parent]\n    child {node {$\\lg{(n - 1)}$}[no edge from this parent]\n        child {node {$\\lg{(n - 2)}$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}   The number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, n - 2$, has a cost of $\\lg({n - i})$. The bottom level, at depth n - 1, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sum_{i = 0}^{n - 2}\\lg{(n - i)} + \\Theta(1) \\\\ =  \\sum_{i = 2}^{n}\\lg{n} + \\Theta(1) \\\\ =  \\lg{(n!)} + \\Theta(1) \\\\ =  \\Theta(n\\lg{n}) \\text{ (we have proved it in 3.2-3)}\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = T(n - 1) + \\lg{n}$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{n}$ and $T(n) \\leq c_2n\\lg{n}$ for some constants $c_1   0$ and $c_2   0$. So:  $$\n\\begin{eqnarray}\nT(n)  \\geq  c_1(n - 1)\\lg{(n - 1)} + \\lg{n}\n\\end{eqnarray}\n$$  Let's try to reguess $T(n) \\geq c_1(n + 1)\\lg{(n + 1)}$. So:  $$\n\\begin{eqnarray}\nT(n)  \\geq  c_1(n - 1 + 1)\\lg{(n - 1 + 1)} + \\lg{n} \\\\ =  c_1n\\lg{n} + \\lg{n} \\\\  c_1n\\lg{n} \\\\ =  \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$  So $T(n) = \\Omega(n\\lg{n})$. And:  $$\n\\begin{eqnarray}\nT(n)  \\leq  c_2(n - 1)\\lg{(n - 1)} + \\lg{n} \\\\  c_2(n - 1)\\lg{n} + \\lg{n} \\\\ =  \\lg{n}(c_2(n - 1) + 1) \\\\ \\leq  c_2n\\lg{n} \\\\ =  O(n\\lg{n})\n\\end{eqnarray}\n$$  where the last step holds as long as $c_2 \\geq 1$.  So $T(n) = O(n\\lg{n})$. Thus $T(n) = \\Theta(n\\lg{n})$.  i  First let's create a recursion tree for the recurrence $T(n) = T(n - 2) + \\frac{1}{\\lg{n}}$.  \\documentclass{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\n\\tikzset{\n    no edge from this parent/.style={\n        every child/.append style={\n        edge from parent/.style={draw=none}}},\n    level 4/.style={level distance=6mm}\n}\n\n\\begin{document}\n\\begin{tikzpicture}\n\n\\node (root){$\\frac{1}{\\lg{n}}$}\n    child {node {$\\frac{1}{\\lg{(n - 2)}}$}\n        child {node {$\\frac{1}{\\lg{(n - 4)}}$}\n            child {node {$\\vdots$}[no edge from this parent]\n                child {node {T(1)}}}}};\n\n\\node[right=1 of root] {$\\frac{1}{\\lg{n}}$}[no edge from this parent]\n    child {node {$\\frac{1}{\\lg{(n - 2)}}$}[no edge from this parent]\n        child {node {$\\frac{1}{\\lg{(n - 4)}}$}[no edge from this parent]\n            child {node {}[no edge from this parent]\n                child {node {$\\Theta(1)$}}}}};\n\\end{tikzpicture}\n\\end{document}   The number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, \\lg{(n - 1)} - 1$, has a cost of $\\frac{1}{\\lg{(n - 2^i)}}$. The bottom level, at depth $\\lg{(n - 1)}$, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sum_{i = 0}^{\\lg{(n - 1)} - 1}\\frac{1}{\\lg{(n - 2^i)}} + \\Theta(1)\n\\end{eqnarray}\n$$  But I don't know how to compute the sum.  j  Let's solve it by the iterative method and assume n is an exact power of 2, and n is also a perfect square.  $$\n\\begin{eqnarray}\nT(n)  =  \\sqrt{n}T(\\sqrt{n}) + n \\\\ =  n^{\\frac{1}{2}}T(n^{\\frac{1}{2}}) + n \\\\ =  n^{\\frac{1}{2}}((n^{\\frac{1}{2}})^{\\frac{1}{2}}T((n^{\\frac{1}{2}})^{\\frac{1}{2}}) + n^{\\frac{1}{2}}) + n \\\\ =  n^{\\frac{1}{2}}(n^{\\frac{1}{4}}T(n^{\\frac{1}{4}}) + n^{\\frac{1}{2}}) + n \\\\ =  n^{\\frac{1}{2} + \\frac{1}{4}}T(n^{\\frac{1}{4}}) + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\ =  n^{\\frac{1}{2} + \\frac{1}{4}}(n^{\\frac{1}{8}}T(n^{\\frac{1}{8}}) + n^{\\frac{1}{4}}) + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\ =  n^{\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8}}T(n^{\\frac{1}{8}}) + n^{\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{4}} + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\ =  \\ldots \\\\ =  n^{\\sum_{i = 1}^{\\lg{\\sqrt{n}}}\\frac{1}{2^i}}T(2) + \\lg{\\sqrt{n}} * n \\\\  n^{\\sum_{i = 1}^{\\infty}\\frac{1}{2^i}}T(2) + \\lg{\\sqrt{n}} * n \\\\ =  n^{\\frac{\\frac{1}{2}}{1 - \\frac{1}{2}}}T(2) + \\frac{1}{2}n\\lg{n} \\\\ =  nT(2) + \\frac{1}{2}n\\lg{n} \\\\ =  O(n\\lg{n})\n\\end{eqnarray}\n$$  Thus, we have derived a guess of $T(n) = O(n\\lg{n})$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn\\lg{n}$ for some constant c   0. So:  $$\n\\begin{eqnarray}\nT(n)  =  \\sqrt{n}T(\\sqrt{n}) + n \\\\ \\leq  \\sqrt{n}c\\sqrt{n}\\lg{\\sqrt{n}} + n \\\\ =  cn\\lg{\\sqrt{n}} + n \\\\ =  \\frac{1}{2}cn\\lg{n} + n\n\\end{eqnarray}\n$$  Because $\\frac{1}{2}cn\\lg{n} + n - cn\\lg{n} = n(1 - \\frac{1}{2}c\\lg{n}) \\leq n(1 - \\frac{1}{2}c)$ when $n \\geq 2$. So $n(1 - \\frac{1}{2}c) \\leq 0$ when $c \\geq 2$. So $T(n) \\leq cn\\lg{n}$ when $c \\geq 2$ and $n \\geq 2$. Thus, $T(n) = O(n\\lg{n})$.  But I don't know how to get the lower bound.", 
            "title": "4-3"
        }, 
        {
            "location": "/4-Divide-and-Conquer/Problems/#4-4", 
            "text": "a  $$\n\\begin{eqnarray}\nz + z\\mathcal{F}(z) + z^2\\mathcal{F}(Z)  =  z + z\\sum_{i = 0}^{\\infty}F_iz^i + z^2\\sum_{i = 0}^{\\infty}F_iz^i \\\\ =  z + (0 + z^2 + z^3 + 2z^4 + 3z^5 + \\ldots) + (0 + z^3 + z^4 + 2z^5 + \\ldots) \\\\ =  0 + z + z^2 + 2z^3 + 3z^4 + 5z^5 + \\ldots \\\\ =  \\mathcal{F}(z)\n\\end{eqnarray}\n$$  b  Because $\\mathcal{F}(z) = z + z\\mathcal{F}(z) + z^2\\mathcal{F}(z)$, so $\\mathcal{F}(z) - z\\mathcal{F}(z) - z^2\\mathcal{F}(z) = z$, thus $\\mathcal{F}(z)(1 - z - z^2) = z$, so $\\mathcal{F}(z) = \\frac{z}{1 - z - z^2}$.  And:  $$\n\\begin{eqnarray}\n\\frac{z}{(1 - \\phi{z})(1 - \\hat\\phi{z})}  =  \\frac{z}{1 - (\\phi + \\hat\\phi)z + \\phi\\hat\\phi{z^2}} \\\\ =  \\frac{z}{1 - (\\frac{1 - \\sqrt{5}}{2} + \\frac{1 + \\sqrt{5}}{2})z + \\frac{1 - \\sqrt{5}}{2}\\frac{1 + \\sqrt{5}}{2}z^2} \\\\ =  \\frac{z}{1 - z - z^2}\n\\end{eqnarray}\n$$  And:  $$\n\\begin{eqnarray}\n\\frac{1}{\\sqrt{5}}(\\frac{1}{1 - \\phi{z}} - \\frac{1}{1 - \\hat\\phi{z}})  =  \\frac{1}{\\sqrt{5}}\\frac{1 - \\hat\\phi{z} - 1 + \\phi{z}}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ =  \\frac{1}{\\sqrt{5}}\\frac{z(\\phi - \\hat\\phi)}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ =  \\frac{1}{\\sqrt{5}}\\frac{z(\\frac{1 + \\sqrt{5}}{2} - \\frac{1 - \\sqrt{5}}{2})}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ =  \\frac{1}{\\sqrt{5}}\\frac{z\\sqrt{5}}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ =  \\frac{z}{(1 - \\phi{z})(1 - \\hat\\phi{z})}\n\\end{eqnarray}\n$$  c  $$\n\\begin{eqnarray}\n\\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)z^i  =  \\frac{1}{\\sqrt{5}}\\sum_{i = 0}^{\\infty}(\\phi^iz^i - \\hat\\phi^iz^i) \\\\ =  \\frac{1}{\\sqrt{5}}(\\sum_{i = 0}^{\\infty}\\phi^iz^i - \\sum_{i = 0}^{\\infty}\\hat\\phi^iz^i) \\\\ =  \\frac{1}{\\sqrt{5}}(\\frac{1}{1 - \\phi{z}} - \\frac{1}{1 - \\hat\\phi{z}}) \\\\ =  \\mathcal{F}(z)\n\\end{eqnarray}\n$$  d  Because $\\mathcal{F}(z) = \\sum_{i = 0}^{\\infty}F_iz^i$ and $\\mathcal{F}(z) = \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)z^i$, so we have $F_i = \\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)$. Since $|\\hat\\phi|   1$, we have $\\frac{|\\hat\\phi^i|}{\\sqrt{5}}   \\frac{1}{\\sqrt{5}}   \\frac{1}{2}$, which implies that $F_i = \\lfloor \\frac{\\phi^i}{\\sqrt{5}} + \\frac{1}{2} \\rfloor$, which is to say that the ith Fibonacci number $F_i$ is equal to $\\frac{\\phi^i}{\\sqrt{5}}$ rounded to the nearest integer.", 
            "title": "4-4"
        }, 
        {
            "location": "/4-Divide-and-Conquer/Problems/#4-5", 
            "text": "a", 
            "title": "4-5"
        }, 
        {
            "location": "/4-Divide-and-Conquer/Problems/#4-6", 
            "text": "a  The \"only if\" part is easy. If an array is Monge, then for all i, j, k and l such that $l \\leq i   k \\leq m$ and $l \\leq j   l \\leq n$ we have $A[i, j] + A[k, l] \\leq A[i, l] + A[k, j]$. So we let k = i + 1, l = j + 1, then we have $A[i, j] + A[i + 1, j + 1] \\leq A[i, j + 1] + A[i + 1, j]$. So we proved the \"only if\" part.  Then let's prove the \"if\" part, we'll use induction separately on rows and columns. Let's get on columns first. The base case is already given, let's assume $A[i, j] + A[i + 1, j + k] \\leq A[i, j + k] + A[i + 1, j]$ for $k \\geq 1$ and $j + k \\leq n$. Then we need to prove $A[i, j] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j]$.  According to the definition, we have $A[i, j + k] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j + k]$, thus $A[i, j] + A[i + 1, j + k] + A[i, j + k] + A[i + 1, j + k + 1] \\leq A[i, j + k] + A[i + 1, j] + A[i, j + k + 1] + A[i + 1, j + k]$. So we get $A[i, j] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j]$.  So the induction is correct, and similarly we can prove $A[i, j + 1] + A[i + 1, j + k] \\leq A[i, j + k] + A[i + 1, j + 1] \\, (j + k \\leq n), \\ldots, A[i, j + p] + A[i + 1, j + k] \\leq A[i, j + k] + A[i + 1, j + p] \\, (j + p \\leq j + k - 1, j + k \\leq n)$, thus we can see that all adjacent rows are Monge arrays. Similarly, we can use the induction on rows, so all adjacent columns are Monge arrays.   For any $m_1 x n_1$ array, if $m_1 = 2$ or $n_1 = 2$, then the subarray is a Monge array. And we need to prove the subarray is also a Monge array if $m_1 \\geq 3$ and $n_1 \\geq 3$.  Let's assume the subarray starts at row $A[i_1, j_1]$ and ends at $A[i_2, j_2]$. Since any adjacent columns are Monge arrays, so we have:  $$A[i_1, j_1] + A[i_2, j_1 + 1] \\leq A[i_1, j_1 + 1] + A[i_2, j_1]$$\n$$A[i_1, j_1 + 1] + A[i_2, j_1 + 2] \\leq A[i_1, j_1 + 2] + A[i_2, j_1 + 1]$$\n$$\\ldots$$\n$$A[i_1, j_2 - 1] + A[i_2, j_2] \\leq A[i_1, j_2] + A[i_2, j_2 - 1]$$  Then we sum all inequations together, so we have:  $$A[i_1, j_1] + A[i_2, j_2] \\leq A[i_1, j_2] + A[i_2, j_1]$$, which means the subarray $A[i_1, j_1]$ to $A[i_2, j_2]$ is a Monge array when $i_2 - i_1 \\geq 3$ and $j_2 - j_1 \\geq 3$. So for all subarrays in A, they are Monge arrays. So we proved the \"if\" part.  b  \\begin{matrix}\n37   23   22   32 \\\\\n21    6    5   10 \\\\\n53   34   30   31 \\\\\n32   13    9    6 \\\\\n43   21   15    8 \\\\\n\\end{matrix}  c  Let's consider row i and row i + 1. Let $f(i) = j_1$ and $f(i + 1) = j_2$ and assume $f(i)   f(i + 1)$, so we have $j_1   j_2$. Since A is a Monge array, so we have $A[i, j_2] + A[i + 1, j_1] \\leq A[i, j_1] + A[i + 1, j_2]$. But according to the definition of f(i), we have $A[i, j_2]   A[i, j_1]$ and $A[i + 1, j_2]   A[i + 1, j_1]$, combine them together: $A[i, j_2] + A[i + 1, j_1]   A[i, j_1] + A[i + 1, j_2]$. Thus the assumption is wrong. So $f(i) \\leq f(i + 1)$. So $f(1) \\leq f(2) \\leq \\ldots \\leq f(m)$ for any m x n Monge array.  d  Let's assume $m = 2k, k = 1, 2, \\ldots$, from the previous question we have $f(2k - 2) \\leq f(2k - 1) \\leq f(2k)$. We want to get f(2k - 1) when f(2k - 2) and f(2k) are already known. We only need to compare the numbers from A[2k - 1][f(2k - 2)] to A[2k - 1][f(2k)], which costs at most $O(f(2k) - f(2k - 2) + 1)$. Thus:  $$\n\\begin{eqnarray}\nT  =  \\sum_{k = 1}^{\\frac{m}{2}}O(f(2k) - f(2k - 2) + 1) \\\\ =  (f(2) - f(0) + 1) + (f(4) - f(2) + 1) + \\ldots + (f(m) - f(m - 2) + 1) \\\\ =  f(m) - f(0) + \\frac{m}{2} \\\\ \\leq  n - 0 + \\frac{m}{2} \\\\  n + m \\\\ =  O(m + n)\n\\end{eqnarray}\n$$  e  $$\n\\begin{eqnarray}\nT(m)  =  T(\\frac{m}{2}) + O(m + n) \\\\ =  T(\\frac{m}{4}) + O(\\frac{m}{2} + n) + O(m + n) \\\\ =  T(\\frac{m}{8}) + O(\\frac{m}{4} + n) + O(\\frac{m}{2} + n) + O(m + n) \\\\ =  \\ldots \\\\ =  T(1) + \\sum_{i = 0}^{\\lg{m} - 1}O(\\frac{m}{2^i}) + \\sum_{i = 0}^{\\lg{m} - 1}O(n) \\\\  T(1) + \\sum_{i = 0}^{\\infty}O(\\frac{m}{2^i}) + \\sum_{i = 0}^{\\lg{m} - 1}O(n) \\\\ =  T(1) + O(m\\frac{1}{1 - \\frac{1}{2}}) + O(n\\lg{m}) \\\\ =  O(1) + O(2m) + O(n\\lg{m}) \\\\ =  O(m + n\\lg{m})\n\\end{eqnarray}\n$$  def leftmost_min_element_in_each_row_of_monge_array(matrix):\n    # Column index of leftmost min element in each row\n    leftmost_min_element_in_each_row = [0] * len(matrix)\n\n    leftmost_min_element_divide_and_conquer(\n        matrix, leftmost_min_element_in_each_row, 0, len(matrix) - 1, 1)\n\n    return leftmost_min_element_in_each_row\n\n\ndef leftmost_min_element_divide_and_conquer(\n        matrix, leftmost_min_element_in_each_row, row_start, row_end, step):\n    if row_start == row_end:\n        leftmost_min_element_in_each_row[row_start] = \\\n            find_leftmost_min_element_in_row(\n                matrix, row_start, 0, len(matrix[row_start]) - 1)\n    else:\n        # Construct a submatrix consisting of the even-numbered rows\n        sub_matrix_row_start = row_start + step\n        sub_matrix_row_end = 0\n\n        if row_end % 2 == 0 or ((row_end - row_start) // step) % 2 == 0:\n            sub_matrix_row_end = row_end - step\n        elif ((row_end - row_start) // step) % 2 == 1:\n            sub_matrix_row_end = row_end\n\n        leftmost_min_element_divide_and_conquer(\n            matrix, leftmost_min_element_in_each_row,\n            sub_matrix_row_start, sub_matrix_row_end, step * 2)\n\n        leftmost_min_element_in_odd_numbered_rows(\n            matrix, leftmost_min_element_in_each_row, row_start, row_end, step)\n\n\ndef find_leftmost_min_element_in_row(matrix, row, column_start, column_end):\n    min_column_index = 0\n\n    for column in range(column_start, column_end + 1):\n        if matrix[row][column]   matrix[row][min_column_index]:\n            min_column_index = column\n\n    return min_column_index\n\n\ndef leftmost_min_element_in_odd_numbered_rows(\n        matrix, leftmost_min_element_in_each_row, row_start, row_end, step):\n    for odd_numbered_row in range(row_start, row_end + 1, step * 2):\n        prev_even_numbered_row = odd_numbered_row - step\n        next_even_numbered_row = odd_numbered_row + step\n        column_start = -1\n        column_end = -1\n\n        if prev_even_numbered_row  = row_start and \\\n                next_even_numbered_row  = row_end:\n            column_start = \\\n                leftmost_min_element_in_each_row[prev_even_numbered_row]\n            column_end = \\\n                leftmost_min_element_in_each_row[next_even_numbered_row]\n        elif prev_even_numbered_row  = row_start:\n            column_start = \\\n                leftmost_min_element_in_each_row[prev_even_numbered_row]\n            column_end = len(matrix[0]) - 1\n        elif next_even_numbered_row  = row_end:\n            column_start = 0\n            column_end = \\\n                leftmost_min_element_in_each_row[next_even_numbered_row]\n\n        leftmost_min_element_in_each_row[odd_numbered_row] = \\\n            find_leftmost_min_element_in_row(\n                matrix, odd_numbered_row, column_start, column_end)", 
            "title": "4-6"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/", 
            "text": "5.1 The hiring problem\n\n\n5.1-1\n\n\nBecause we are always able to determine which candidate is best, that means we can compare any two candidates and know which is better. Thus we are able to sort the candidates based on this knowledge, which implies that we know a total order on the ranks of the candidates.\n\n\n5.1-2\n\n\nFirst we find the smallest number \np\n, such that \n2^p \n b - a\n. Then call \nRANDOM(0, 1)\n to generate \np\n bits, thus we have a random number \nr\n. If \nr + a \n= b\n, then that's what we want, otherwise we regenerate \nr\n. (\nLink 1\n, \nlink 2\n)\n\n\nimport math\nimport random\n\n\ndef random_number(a, b):\n    bits = math.ceil(math.log2(b - a + 1))\n\n    while True:\n        number = random_binay(bits)\n\n        if a + number \n= b:\n            return a + number\n\n\ndef random_binay(bits):\n    number = 0\n\n    for i in range(bits):\n        number = number * 2 + random.randint(0, 1)\n\n    return number\n\n\n\n\nThe running time of function \nrandom_binay\n is $O(\\lg{(b - a)})$. And it's possible that we need to call \nrandom_binay\n multiple times if \na + number \n b\n, but that doesn't affect the expected running time, since $cO(\\lg{(b - a)})$ is still $O((\\lg{(b - a)}))$. \n\n\nThus the expected running time is $O(\\lg{(b - a)})$.\n\n\n5.1-3\n\n\nWe can call \nBIASED-RANDOM\n twice to get two numbers \nx\n and \ny\n. The results would be \n0, 0\n, '1, 0', '0, 1', '1, 1', with probability \n(1 - p)(1 - p)\n, \np(1 - p)\n, \n(1 - p)p\n, \npp\n respectively. Since \np(1 - p) = (1 - p)p\n, we can treat one as \n0\n and the other as \n1\n. So we can generate 0 with probability 1/2 and 1 with probability 1/2.\n\n\nThe probability to generate \n0, 1\n and \n1, 0\n is $2p(1 - p)$, so we have to try $\\frac{1}{2p(1 - p)}$ times. Thus the expected running time is $O(\\frac{1}{p(1 - p)})$.\n\n\ndef unbiased_random():\n    while True:\n        x = BIASED-RANDOM()\n        y = BIASED-RANDOM()\n\n        if x != y:\n            return x", 
            "title": "5.1 The hiring problem"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/#51-the-hiring-problem", 
            "text": "", 
            "title": "5.1 The hiring problem"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/#51-1", 
            "text": "Because we are always able to determine which candidate is best, that means we can compare any two candidates and know which is better. Thus we are able to sort the candidates based on this knowledge, which implies that we know a total order on the ranks of the candidates.", 
            "title": "5.1-1"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/#51-2", 
            "text": "First we find the smallest number  p , such that  2^p   b - a . Then call  RANDOM(0, 1)  to generate  p  bits, thus we have a random number  r . If  r + a  = b , then that's what we want, otherwise we regenerate  r . ( Link 1 ,  link 2 )  import math\nimport random\n\n\ndef random_number(a, b):\n    bits = math.ceil(math.log2(b - a + 1))\n\n    while True:\n        number = random_binay(bits)\n\n        if a + number  = b:\n            return a + number\n\n\ndef random_binay(bits):\n    number = 0\n\n    for i in range(bits):\n        number = number * 2 + random.randint(0, 1)\n\n    return number  The running time of function  random_binay  is $O(\\lg{(b - a)})$. And it's possible that we need to call  random_binay  multiple times if  a + number   b , but that doesn't affect the expected running time, since $cO(\\lg{(b - a)})$ is still $O((\\lg{(b - a)}))$.   Thus the expected running time is $O(\\lg{(b - a)})$.", 
            "title": "5.1-2"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/#51-3", 
            "text": "We can call  BIASED-RANDOM  twice to get two numbers  x  and  y . The results would be  0, 0 , '1, 0', '0, 1', '1, 1', with probability  (1 - p)(1 - p) ,  p(1 - p) ,  (1 - p)p ,  pp  respectively. Since  p(1 - p) = (1 - p)p , we can treat one as  0  and the other as  1 . So we can generate 0 with probability 1/2 and 1 with probability 1/2.  The probability to generate  0, 1  and  1, 0  is $2p(1 - p)$, so we have to try $\\frac{1}{2p(1 - p)}$ times. Thus the expected running time is $O(\\frac{1}{p(1 - p)})$.  def unbiased_random():\n    while True:\n        x = BIASED-RANDOM()\n        y = BIASED-RANDOM()\n\n        if x != y:\n            return x", 
            "title": "5.1-3"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/", 
            "text": "5.2 Indicator random variables\n\n\n5.2-1\n\n\nWe hire exactly one time when the first candidate is the best. So the first candidate has a probability of $\\frac{1}{n}$ of being better qualified than other n - 1 candidates. Thus the probability of hiring exactly one time is $\\frac{1}{n}$.\n\n\nWe hire n times when each candidate is better than the previous candidate. In each turn, the candidate i has a probability of $\\frac{1}{i}$ of being better than the previous i - 1 candidates. Thus the probability is $1 * \\frac{1}{2} * \\frac{1}{3} * \\ldots * \\frac{1}{n} = \\frac{1}{n!}$.\n\n\n5.2-2\n\n\nWe hire exactly twice when the first candidate is not the best candidate, and the best candidate comes before all the candidates who are better than the first candidate.\n\n\nLet's assume the first candidate has a rank k among the candidates (k \n n, bigger is better). So there are n - k candidates that are better than the first candidate, including the best candidate. So the best candidate could only appear through position 2 to k + 1, which are k + 1 - 2 + 1 = k positions. And the best candidate has a probability of $\\frac{1}{k}$ to apppear in those k positions.\n\n\nBecause each candidate has a probability of $\\frac{1}{n}$ to appear in the first position, thus the probability of hiring twice is $\\frac{1}{n}\\frac{1}{1} + \\frac{1}{n}\\frac{1}{2} + \\frac{1}{n}\\frac{1}{3} \\ldots \\frac{1}{n}\\frac{1}{n - 1} = \\frac{1}{n}\\sum_{k = 1}^{n - 1}\\frac{1}{k} = \\frac{\\ln{(n - 1)} + O(1)}{n}$.\n\n\n5.2-3\n\n\nLet's define $X_i$ be the indicator random variable indicates the value of ith dice. Thus,\n\n\n$$\n\\begin{eqnarray}\nX_i \n=\n I\\lbrace\\text{the number of dice is k}\\rbrace \\\\\n\n=\n \\begin{cases}\n      1, \n \\text{if k = 1} \\\\\n      2, \n \\text{if k = 2} \\\\\n      3, \n \\text{if k = 3} \\\\\n      4, \n \\text{if k = 4} \\\\\n      5, \n \\text{if k = 5} \\\\\n      6, \n \\text{if k = 6} \\\\\n    \\end{cases}\n\\end{eqnarray}\n$$\n\n\nand $X = X_1 + X_2 + \\ldots + X_n$.\n\n\nThus the expected value in one dice is:\n\n\n$$\n\\begin{eqnarray}\nE[X_i] \n=\n E[I\\lbrace \\text{the number of dice is k} \\rbrace] \\\\\n\n=\n \\sum_{k = 1}^6 kPr\\lbrace X_i = k \\rbrace \\\\\n\n=\n \\frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) \\\\\n\n=\n 3.5\n\\end{eqnarray}\n$$\n\n\nThus, the expected value of the sum of n dice is:\n\n\n$$\n\\begin{eqnarray}\nE[X] \n=\n E\\big[\\sum_{i = 1}^n X_i\\big] \\\\\n\n=\n \\sum_{i = 1}^nE\\big[X_i\\big] \\\\\n\n=\n \\sum_{i = 1}^n 3.5 \\\\\n\n=\n 3.5n\n\\end{eqnarray}\n$$\n\n\n5.2-4\n\n\nEach person gets his own hat with probability $\\frac{1}{n}$. Let's define $X_i$ be the indicator random variable associated with the event in which the ith customer gets his own hat. Thus,\n\n\n$$\n\\begin{eqnarray}\nX_i \n=\n I\\lbrace\\text{customer i gets his own hat}\\rbrace \\\\\n\n=\n \\begin{cases}\n      1, \n \\text{if customer i gets his own hat} \\\\\n      0, \n \\text{if customer i doesn't get his own hat}\n    \\end{cases}\n\\end{eqnarray}\n$$\n\n\nand $X = X_1 + X_2 + \\ldots + X_n$.\n\n\nThus the expected number of customers who get back their own hat is:\n\n\n$$\n\\begin{eqnarray}\nE[X] \n=\n E\\big[\\sum_{i = 1}^n X_i\\big] \\\\\n\n=\n \\sum_{i = 1}^nE\\big[X_i\\big] \\\\\n\n=\n \\sum_{i = 1}^n \\frac{1}{n} * 1 \\\\\n\n=\n 1\n\\end{eqnarray}\n$$\n\n\n5.2-5\n\n\nLet's define $X_{ij}$ be the the indicator random variable associated with the event that if i \n j, then A[i] \n A[j]. Thus,\n\n\n$$\n\\begin{eqnarray}\nX_{ij} \n=\n I\\lbrace A[i] \n A[j]\\rbrace \\\\\n\n=\n \\begin{cases}\n      1, \n A[i] \n A[j] \\\\\n      0, \n A[i] \\leq A[j]\n    \\end{cases}\n\\end{eqnarray}\n$$\n\n\nand $Pr\\lbrace A[i] \n A[j] \\rbrace = Pr\\lbrace A[i] \\leq A[j] \\rbrace = \\frac{1}{2}$.\n\n\nThus the expected number of inversions is:\n\n\n$$\n\\begin{eqnarray}\nE[X] \n=\n E\\big[\\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n X_{ij}\\big] \\\\\n\n=\n \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n E\\big[X_{ij}\\big] \\\\\n\n=\n \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n (\\frac{1}{2} * 1 + 0) \\\\\n\n=\n \\frac{1}{2}\\sum_{i = 1}^{n - 1} (n - i) \\\\\n\n=\n \\frac{1}{2}\\sum_{i = 1}^{n - 1} i \\\\\n\n=\n \\frac{1}{2}\\frac{(1 + n - 1)(n - 1)}{2} \\\\\n\n=\n \\frac{n(n - 1)}{4}\n\\end{eqnarray}\n$$", 
            "title": "5.2 Indicator random variables"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-indicator-random-variables", 
            "text": "", 
            "title": "5.2 Indicator random variables"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-1", 
            "text": "We hire exactly one time when the first candidate is the best. So the first candidate has a probability of $\\frac{1}{n}$ of being better qualified than other n - 1 candidates. Thus the probability of hiring exactly one time is $\\frac{1}{n}$.  We hire n times when each candidate is better than the previous candidate. In each turn, the candidate i has a probability of $\\frac{1}{i}$ of being better than the previous i - 1 candidates. Thus the probability is $1 * \\frac{1}{2} * \\frac{1}{3} * \\ldots * \\frac{1}{n} = \\frac{1}{n!}$.", 
            "title": "5.2-1"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-2", 
            "text": "We hire exactly twice when the first candidate is not the best candidate, and the best candidate comes before all the candidates who are better than the first candidate.  Let's assume the first candidate has a rank k among the candidates (k   n, bigger is better). So there are n - k candidates that are better than the first candidate, including the best candidate. So the best candidate could only appear through position 2 to k + 1, which are k + 1 - 2 + 1 = k positions. And the best candidate has a probability of $\\frac{1}{k}$ to apppear in those k positions.  Because each candidate has a probability of $\\frac{1}{n}$ to appear in the first position, thus the probability of hiring twice is $\\frac{1}{n}\\frac{1}{1} + \\frac{1}{n}\\frac{1}{2} + \\frac{1}{n}\\frac{1}{3} \\ldots \\frac{1}{n}\\frac{1}{n - 1} = \\frac{1}{n}\\sum_{k = 1}^{n - 1}\\frac{1}{k} = \\frac{\\ln{(n - 1)} + O(1)}{n}$.", 
            "title": "5.2-2"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-3", 
            "text": "Let's define $X_i$ be the indicator random variable indicates the value of ith dice. Thus,  $$\n\\begin{eqnarray}\nX_i  =  I\\lbrace\\text{the number of dice is k}\\rbrace \\\\ =  \\begin{cases}\n      1,   \\text{if k = 1} \\\\\n      2,   \\text{if k = 2} \\\\\n      3,   \\text{if k = 3} \\\\\n      4,   \\text{if k = 4} \\\\\n      5,   \\text{if k = 5} \\\\\n      6,   \\text{if k = 6} \\\\\n    \\end{cases}\n\\end{eqnarray}\n$$  and $X = X_1 + X_2 + \\ldots + X_n$.  Thus the expected value in one dice is:  $$\n\\begin{eqnarray}\nE[X_i]  =  E[I\\lbrace \\text{the number of dice is k} \\rbrace] \\\\ =  \\sum_{k = 1}^6 kPr\\lbrace X_i = k \\rbrace \\\\ =  \\frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) \\\\ =  3.5\n\\end{eqnarray}\n$$  Thus, the expected value of the sum of n dice is:  $$\n\\begin{eqnarray}\nE[X]  =  E\\big[\\sum_{i = 1}^n X_i\\big] \\\\ =  \\sum_{i = 1}^nE\\big[X_i\\big] \\\\ =  \\sum_{i = 1}^n 3.5 \\\\ =  3.5n\n\\end{eqnarray}\n$$", 
            "title": "5.2-3"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-4", 
            "text": "Each person gets his own hat with probability $\\frac{1}{n}$. Let's define $X_i$ be the indicator random variable associated with the event in which the ith customer gets his own hat. Thus,  $$\n\\begin{eqnarray}\nX_i  =  I\\lbrace\\text{customer i gets his own hat}\\rbrace \\\\ =  \\begin{cases}\n      1,   \\text{if customer i gets his own hat} \\\\\n      0,   \\text{if customer i doesn't get his own hat}\n    \\end{cases}\n\\end{eqnarray}\n$$  and $X = X_1 + X_2 + \\ldots + X_n$.  Thus the expected number of customers who get back their own hat is:  $$\n\\begin{eqnarray}\nE[X]  =  E\\big[\\sum_{i = 1}^n X_i\\big] \\\\ =  \\sum_{i = 1}^nE\\big[X_i\\big] \\\\ =  \\sum_{i = 1}^n \\frac{1}{n} * 1 \\\\ =  1\n\\end{eqnarray}\n$$", 
            "title": "5.2-4"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-5", 
            "text": "Let's define $X_{ij}$ be the the indicator random variable associated with the event that if i   j, then A[i]   A[j]. Thus,  $$\n\\begin{eqnarray}\nX_{ij}  =  I\\lbrace A[i]   A[j]\\rbrace \\\\ =  \\begin{cases}\n      1,   A[i]   A[j] \\\\\n      0,   A[i] \\leq A[j]\n    \\end{cases}\n\\end{eqnarray}\n$$  and $Pr\\lbrace A[i]   A[j] \\rbrace = Pr\\lbrace A[i] \\leq A[j] \\rbrace = \\frac{1}{2}$.  Thus the expected number of inversions is:  $$\n\\begin{eqnarray}\nE[X]  =  E\\big[\\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n X_{ij}\\big] \\\\ =  \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n E\\big[X_{ij}\\big] \\\\ =  \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n (\\frac{1}{2} * 1 + 0) \\\\ =  \\frac{1}{2}\\sum_{i = 1}^{n - 1} (n - i) \\\\ =  \\frac{1}{2}\\sum_{i = 1}^{n - 1} i \\\\ =  \\frac{1}{2}\\frac{(1 + n - 1)(n - 1)}{2} \\\\ =  \\frac{n(n - 1)}{4}\n\\end{eqnarray}\n$$", 
            "title": "5.2-5"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/", 
            "text": "5.3 Randomized algorithms\n\n\n5.3-1\n\n\nWe can randomly swap the first element first, then start the for loop from the second element.\n\n\nRANDOMIZE-IN-PLACE(A)\nn = A.length\nswap A[1] with A[RANDOM(1, n)]\nfor i = 2 to n\n    swap A[i] with A[RANDOM(i, n)]\n\n\n\n\nAnd here is the initialization step: before the first loop iteration, i = 2. The loop invariant says that for each possible 1-permutation, the subarray A[1..1] contains this 1-permutation with probability (n - i + 1)!/n! = (n - 1)!/n! = 1/n. The subarray A[1..1] contains only one element, contains 1-permutation with probability 1/n, so the loop invariant holds prior to the first iteration.\n\n\nThe maintenance and termination steps remain the same.\n\n\n5.3-2\n\n\nNo, it cannot get any permutation, for example, the identity permutation.\n\n\n5.3-3\n\n\nThis method can produce $n^n$ permutations, but there are only at most n! permutations. So some permutations have duplicates. If each permutation has same number of duplicates, which means $n^n$ is divisible by n!, then the code can produce a uniform random permutation, otherwise, it can not.\n\n\nLet's consider n - 1, it's obvious that n - 1 is a divisor of n!. Then lt's prove n - 1 is not a divisor of $n^n$ for n \n 2.\n\n\nTo make it simple, let's compare $(n + 1)^{n + 1}$ and n for n \n 1. We can write $(n + 1)^{n + 1}$ as $a_{n + 1}n^{n + 1} + a_nn^n + \\ldots + a_1n + 1$. Thus $\\frac{(n + 1)^{n + 1}}{n} = a_{n + 1}n^n + a_nn^{n - 1} + \\ldots + a_1 + \\frac{1}{n}$, which is not an integer when n \n 1. So $(n + 1)^{n + 1}$ is not divisible by n when n \n 1, thus, $n^n$ is not divisible by n - 1 when n \n 2.\n\n\nSo $n^n$ is not divisible by n!, otherwise, n - 1 must also be a divisor of $n^n$. Thus, the code cannot produce a uniform random permutation.\n\n\n5.3-4\n\n\nThe code shifts each element in A by k positions by cyclic, k ranges from 1 to n, thus each element A[i] has a $\\frac{1}{n}$ probability of winding up in any particular position in B.\n\n\nThis is not a uniformly random because it cannot generate all possible permutations, it can only generate n permutations.\n\n\n5.3-5\n\n\nThere are $(n^3)^n$ permutations, and there are $A_{n^3}^{n}$ permutations that all elements are unique. Thus, the probability that all elements are unique is:\n\n\n$$\n\\begin{eqnarray}\nP \n=\n \\frac{A_{n^3}^{n}}{(n^3)^n} \\\\\n\n=\n \\frac{n^3(n^3 - 1)(n^3 - 2)\\ldots(n^3 - (n - 1))}{(n^3)^n} \\\\\n\n=\n 1\\frac{n^3 - 1}{n^3}\\frac{n^3 - 2}{n^3}\\ldots\\frac{n^3 - (n - 1)}{n^3} \\\\\n\n=\n (1 - \\frac{1}{n^3})(1 - \\frac{2}{n^3})\\ldots(1 - \\frac{n - 1}{n^3}) \\\\\n\n\\geq\n (1 - \\frac{n}{n^3})(1 - \\frac{n}{n^3})\\ldots(1 - \\frac{n}{n^3}) \\\\\n\n=\n (1 - \\frac{n}{n^3})^{n - 1} \\\\\n\n=\n (1 - \\frac{1}{n^2})^{n - 1} \\\\\n\n=\n 1 + (n - 1)(-\\frac{1}{n^2}) + \\frac{(n - 1)(n - 2)}{2}(-\\frac{1}{n^2})^2 + \\frac{(n - 1)(n - 2)(n - 3)}{3!}(-\\frac{1}{n^2})^3 + \\ldots \\\\\n\n=\n 1 - \\frac{n - 1}{n^2} + O(\\frac{1}{n^2}) - O(\\frac{1}{n^3}) + \\ldots \\\\\n\n 1 - \\frac{n - 1}{n^2} \\\\\n\n 1 - \\frac{n}{n^2} \\\\\n\n=\n 1 - \\frac{1}{n}\n\\end{eqnarray}\n$$\n\n\n5.3-6\n\n\nRegenerate the priorities array, until all elements are unique.\n\n\n5.3-7", 
            "title": "5.3 Randomized algorithms"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-randomized-algorithms", 
            "text": "", 
            "title": "5.3 Randomized algorithms"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-1", 
            "text": "We can randomly swap the first element first, then start the for loop from the second element.  RANDOMIZE-IN-PLACE(A)\nn = A.length\nswap A[1] with A[RANDOM(1, n)]\nfor i = 2 to n\n    swap A[i] with A[RANDOM(i, n)]  And here is the initialization step: before the first loop iteration, i = 2. The loop invariant says that for each possible 1-permutation, the subarray A[1..1] contains this 1-permutation with probability (n - i + 1)!/n! = (n - 1)!/n! = 1/n. The subarray A[1..1] contains only one element, contains 1-permutation with probability 1/n, so the loop invariant holds prior to the first iteration.  The maintenance and termination steps remain the same.", 
            "title": "5.3-1"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-2", 
            "text": "No, it cannot get any permutation, for example, the identity permutation.", 
            "title": "5.3-2"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-3", 
            "text": "This method can produce $n^n$ permutations, but there are only at most n! permutations. So some permutations have duplicates. If each permutation has same number of duplicates, which means $n^n$ is divisible by n!, then the code can produce a uniform random permutation, otherwise, it can not.  Let's consider n - 1, it's obvious that n - 1 is a divisor of n!. Then lt's prove n - 1 is not a divisor of $n^n$ for n   2.  To make it simple, let's compare $(n + 1)^{n + 1}$ and n for n   1. We can write $(n + 1)^{n + 1}$ as $a_{n + 1}n^{n + 1} + a_nn^n + \\ldots + a_1n + 1$. Thus $\\frac{(n + 1)^{n + 1}}{n} = a_{n + 1}n^n + a_nn^{n - 1} + \\ldots + a_1 + \\frac{1}{n}$, which is not an integer when n   1. So $(n + 1)^{n + 1}$ is not divisible by n when n   1, thus, $n^n$ is not divisible by n - 1 when n   2.  So $n^n$ is not divisible by n!, otherwise, n - 1 must also be a divisor of $n^n$. Thus, the code cannot produce a uniform random permutation.", 
            "title": "5.3-3"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-4", 
            "text": "The code shifts each element in A by k positions by cyclic, k ranges from 1 to n, thus each element A[i] has a $\\frac{1}{n}$ probability of winding up in any particular position in B.  This is not a uniformly random because it cannot generate all possible permutations, it can only generate n permutations.", 
            "title": "5.3-4"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-5", 
            "text": "There are $(n^3)^n$ permutations, and there are $A_{n^3}^{n}$ permutations that all elements are unique. Thus, the probability that all elements are unique is:  $$\n\\begin{eqnarray}\nP  =  \\frac{A_{n^3}^{n}}{(n^3)^n} \\\\ =  \\frac{n^3(n^3 - 1)(n^3 - 2)\\ldots(n^3 - (n - 1))}{(n^3)^n} \\\\ =  1\\frac{n^3 - 1}{n^3}\\frac{n^3 - 2}{n^3}\\ldots\\frac{n^3 - (n - 1)}{n^3} \\\\ =  (1 - \\frac{1}{n^3})(1 - \\frac{2}{n^3})\\ldots(1 - \\frac{n - 1}{n^3}) \\\\ \\geq  (1 - \\frac{n}{n^3})(1 - \\frac{n}{n^3})\\ldots(1 - \\frac{n}{n^3}) \\\\ =  (1 - \\frac{n}{n^3})^{n - 1} \\\\ =  (1 - \\frac{1}{n^2})^{n - 1} \\\\ =  1 + (n - 1)(-\\frac{1}{n^2}) + \\frac{(n - 1)(n - 2)}{2}(-\\frac{1}{n^2})^2 + \\frac{(n - 1)(n - 2)(n - 3)}{3!}(-\\frac{1}{n^2})^3 + \\ldots \\\\ =  1 - \\frac{n - 1}{n^2} + O(\\frac{1}{n^2}) - O(\\frac{1}{n^3}) + \\ldots \\\\  1 - \\frac{n - 1}{n^2} \\\\  1 - \\frac{n}{n^2} \\\\ =  1 - \\frac{1}{n}\n\\end{eqnarray}\n$$", 
            "title": "5.3-5"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-6", 
            "text": "Regenerate the priorities array, until all elements are unique.", 
            "title": "5.3-6"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-7", 
            "text": "", 
            "title": "5.3-7"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/", 
            "text": "5.4 Probabilistic analysis and further uses of indicator random variables\n\n\n5.4-1\n\n\nFor each people, the probability that he doesn't have the same birthday as me is $\\frac{n - 1}{n}$, for k people that all don't have the same birthday as me is $(\\frac{n - 1}{n})^k$, thus the probability that at least one people that has the same birthday as me is $1 - (\\frac{n - 1}{n})^k$. Let $1 - (\\frac{n - 1}{n})^k \\geq \\frac{1}{2}$, and we have $k \\geq 253$.\n\n\nThe probability that only one people has a birthday on July 4 is $k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1}$, and the probability that no one has a birthday on July 4 is $(\\frac{n - 1}{n})^k$, thus the probability that at least two people have a birthday on July 4 is $1 - k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1} - (\\frac{n - 1}{n})^k$, let $1 - k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1} - (\\frac{n - 1}{n})^k \\geq \\frac{1}{2}$, we have $k \\geq 613$.\n\n\n5.4-2\n\n\nThe expected number of ball tosses is $1 + \\sum_{k = 1}^{b} \\frac{b!}{(b - k)!b^k}$, it's a \nbirthday problem\n.\n\n\n5.4-3\n\n\nThe equation shows that $E[X] = \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}E[X_{ij}]$, so pairwise indenpendence is sufficient.\n\n\n5.4-4\n\n\nFor each pair (i, j, r) of the k people in the room, we define the indicator random variable $X_{ijr}$, for $1 \\leq i \n j \n r \\leq k$, by:\n\n\n$$\n\\begin{eqnarray}\nX_{ijr} \n=\n I\\lbrace\\text{person i, person j and person r have the same birthday}\\rbrace \\\\\n\n=\n \\begin{cases}\n      1, \n \\text{if person i, person j and person r have the same birthday} \\\\\n      0, \n \\text{otherwise}\n    \\end{cases}\n\\end{eqnarray}\n$$\n\n\nThe probability that i's birthday, j's birthday and r's birthday all fall on day s is $Pr\\lbrace b_i = s \\text{ and } b_j = s \\text{ and } b_r = s \\rbrace = Pr\\lbrace b_i = s \\rbrace Pr\\lbrace b_j = s \\rbrace Pr\\lbrace b_r = s \\rbrace = \\frac{1}{n^3}$.\n\n\nThus, the probability that they all fall on the same day is:\n\n\n$$\n\\begin{eqnarray}\nPr\\lbrace b_i = b_j = b_r \\rbrace \n=\n \\sum_{s = 1}^{n}Pr\\lbrace b_i = s \\text{ and } b_j = s \\text{ and } b_r = s \\rbrace \\\\\n\n=\n \\sum_{s = 1}^{n}\\frac{1}{n^3} \\\\\n\n=\n \\frac{1}{n^2}\n\\end{eqnarray}\n$$\n\n\nThus, $E[X_{ijr}] = Pr\\lbrace\\text{person i, person j and person r have the same birthday}\\rbrace = \\frac{1}{n^2}$.\n\n\nLetting X be the random variable that counts the number of pairs of inviduals having the same birthday, we have $X = \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}X_{ijr}$.\n\n\nTaking expectations of both sides and applying linearity of expectation, we obtain:\n\n\n$$\n\\begin{eqnarray}\nE[X] \n=\n E[\\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}X_{ijr}] \\\\\n\n=\n \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}E[X_{ijr}] \\\\\n\n=\n \\binom{k}{3}\\frac{1}{n^2} \\\\\n\n=\n \\frac{k(k - 1)(k - 2)}{6n^2}\n\\end{eqnarray}\n$$\n\n\nLet $\\frac{k(k - 1)(k - 2)}{6n^2} \\geq 1$, we have $k \\geq 94$.\n\n\n5.4-5\n\n\n$P = \\frac{A_n^k}{n^k}$.\n\n\nIt can be described by the birthday paradox that k people in the romm have unique birthday.\n\n\n5.4-6\n\n\nLet's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by:\n\n\n$$\n\\begin{eqnarray}\nX_i \n=\n I\\lbrace\\text{bin i is empty}\\rbrace \\\\\n\n=\n \\begin{cases}\n      1, \n \\text{if bin i is empty} \\\\\n      0, \n \\text{otherwise}\n    \\end{cases}\n\\end{eqnarray}\n$$\n\n\nWe have $E[X_i] = Pr\\lbrace \\text{bin i is empty} \\rbrace = (\\frac{n - 1}{n})^n$ (toss n balls into n - 1 bins).\n\n\nLet X be the random variable that counts the number of bins that are empty, we have $X = \\sum_{i = 1}^{n}X_i$. So:\n\n\n$$\n\\begin{eqnarray}\nE[X] \n=\n E[\\sum_{i = 1}^{n}X_i] \\\\\n\n=\n \\sum_{i = 1}^{n}E[X_i] \\\\\n\n=\n \\sum_{i = 1}^{n}(\\frac{n - 1}{n})^n \\\\\n\n=\n n(\\frac{n - 1}{n})^n \\\\\n\n=\n n(1 - \\frac{1}{n})^n \\\\\n\\end{eqnarray}\n$$\n\n\nThen let's check another question. Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by:\n\n\n$$\n\\begin{eqnarray}\nX_i \n=\n I\\lbrace\\text{bin i has exactly one ball}\\rbrace \\\\\n\n=\n \\begin{cases}\n      1, \n \\text{if bin i is has exactly one ball} \\\\\n      0, \n \\text{otherwise}\n    \\end{cases}\n\\end{eqnarray}\n$$\n\n\nWe have $E[X_i] = Pr\\lbrace \\text{bin i has exactly one ball} \\rbrace = n\\frac{1}{n}(\\frac{n - 1}{n})^{n - 1} = (1 - \\frac{1}{n})^{n - 1}$ (toss one ball into one bin, then toss n - 1 balls into n - 1 bins).\n\n\nLet X be the random variable that counts the number of bins that are empty, we have $X = \\sum_{i = 1}^{n}X_i$. So:\n\n\n$$\n\\begin{eqnarray}\nE[X] \n=\n E[\\sum_{i = 1}^{n}X_i] \\\\\n\n=\n \\sum_{i = 1}^{n}E[X_i] \\\\\n\n=\n \\sum_{i = 1}^{n}(1 - \\frac{1}{n})^{n - 1} \\\\\n\n=\n n(1 - \\frac{1}{n})^{n - 1}\n\\end{eqnarray}\n$$\n\n\n5.4-7", 
            "title": "5.4 Probabilistic analysis and further uses of indicator random variables"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-probabilistic-analysis-and-further-uses-of-indicator-random-variables", 
            "text": "", 
            "title": "5.4 Probabilistic analysis and further uses of indicator random variables"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-1", 
            "text": "For each people, the probability that he doesn't have the same birthday as me is $\\frac{n - 1}{n}$, for k people that all don't have the same birthday as me is $(\\frac{n - 1}{n})^k$, thus the probability that at least one people that has the same birthday as me is $1 - (\\frac{n - 1}{n})^k$. Let $1 - (\\frac{n - 1}{n})^k \\geq \\frac{1}{2}$, and we have $k \\geq 253$.  The probability that only one people has a birthday on July 4 is $k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1}$, and the probability that no one has a birthday on July 4 is $(\\frac{n - 1}{n})^k$, thus the probability that at least two people have a birthday on July 4 is $1 - k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1} - (\\frac{n - 1}{n})^k$, let $1 - k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1} - (\\frac{n - 1}{n})^k \\geq \\frac{1}{2}$, we have $k \\geq 613$.", 
            "title": "5.4-1"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-2", 
            "text": "The expected number of ball tosses is $1 + \\sum_{k = 1}^{b} \\frac{b!}{(b - k)!b^k}$, it's a  birthday problem .", 
            "title": "5.4-2"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-3", 
            "text": "The equation shows that $E[X] = \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}E[X_{ij}]$, so pairwise indenpendence is sufficient.", 
            "title": "5.4-3"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-4", 
            "text": "For each pair (i, j, r) of the k people in the room, we define the indicator random variable $X_{ijr}$, for $1 \\leq i   j   r \\leq k$, by:  $$\n\\begin{eqnarray}\nX_{ijr}  =  I\\lbrace\\text{person i, person j and person r have the same birthday}\\rbrace \\\\ =  \\begin{cases}\n      1,   \\text{if person i, person j and person r have the same birthday} \\\\\n      0,   \\text{otherwise}\n    \\end{cases}\n\\end{eqnarray}\n$$  The probability that i's birthday, j's birthday and r's birthday all fall on day s is $Pr\\lbrace b_i = s \\text{ and } b_j = s \\text{ and } b_r = s \\rbrace = Pr\\lbrace b_i = s \\rbrace Pr\\lbrace b_j = s \\rbrace Pr\\lbrace b_r = s \\rbrace = \\frac{1}{n^3}$.  Thus, the probability that they all fall on the same day is:  $$\n\\begin{eqnarray}\nPr\\lbrace b_i = b_j = b_r \\rbrace  =  \\sum_{s = 1}^{n}Pr\\lbrace b_i = s \\text{ and } b_j = s \\text{ and } b_r = s \\rbrace \\\\ =  \\sum_{s = 1}^{n}\\frac{1}{n^3} \\\\ =  \\frac{1}{n^2}\n\\end{eqnarray}\n$$  Thus, $E[X_{ijr}] = Pr\\lbrace\\text{person i, person j and person r have the same birthday}\\rbrace = \\frac{1}{n^2}$.  Letting X be the random variable that counts the number of pairs of inviduals having the same birthday, we have $X = \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}X_{ijr}$.  Taking expectations of both sides and applying linearity of expectation, we obtain:  $$\n\\begin{eqnarray}\nE[X]  =  E[\\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}X_{ijr}] \\\\ =  \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}E[X_{ijr}] \\\\ =  \\binom{k}{3}\\frac{1}{n^2} \\\\ =  \\frac{k(k - 1)(k - 2)}{6n^2}\n\\end{eqnarray}\n$$  Let $\\frac{k(k - 1)(k - 2)}{6n^2} \\geq 1$, we have $k \\geq 94$.", 
            "title": "5.4-4"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-5", 
            "text": "$P = \\frac{A_n^k}{n^k}$.  It can be described by the birthday paradox that k people in the romm have unique birthday.", 
            "title": "5.4-5"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-6", 
            "text": "Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by:  $$\n\\begin{eqnarray}\nX_i  =  I\\lbrace\\text{bin i is empty}\\rbrace \\\\ =  \\begin{cases}\n      1,   \\text{if bin i is empty} \\\\\n      0,   \\text{otherwise}\n    \\end{cases}\n\\end{eqnarray}\n$$  We have $E[X_i] = Pr\\lbrace \\text{bin i is empty} \\rbrace = (\\frac{n - 1}{n})^n$ (toss n balls into n - 1 bins).  Let X be the random variable that counts the number of bins that are empty, we have $X = \\sum_{i = 1}^{n}X_i$. So:  $$\n\\begin{eqnarray}\nE[X]  =  E[\\sum_{i = 1}^{n}X_i] \\\\ =  \\sum_{i = 1}^{n}E[X_i] \\\\ =  \\sum_{i = 1}^{n}(\\frac{n - 1}{n})^n \\\\ =  n(\\frac{n - 1}{n})^n \\\\ =  n(1 - \\frac{1}{n})^n \\\\\n\\end{eqnarray}\n$$  Then let's check another question. Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by:  $$\n\\begin{eqnarray}\nX_i  =  I\\lbrace\\text{bin i has exactly one ball}\\rbrace \\\\ =  \\begin{cases}\n      1,   \\text{if bin i is has exactly one ball} \\\\\n      0,   \\text{otherwise}\n    \\end{cases}\n\\end{eqnarray}\n$$  We have $E[X_i] = Pr\\lbrace \\text{bin i has exactly one ball} \\rbrace = n\\frac{1}{n}(\\frac{n - 1}{n})^{n - 1} = (1 - \\frac{1}{n})^{n - 1}$ (toss one ball into one bin, then toss n - 1 balls into n - 1 bins).  Let X be the random variable that counts the number of bins that are empty, we have $X = \\sum_{i = 1}^{n}X_i$. So:  $$\n\\begin{eqnarray}\nE[X]  =  E[\\sum_{i = 1}^{n}X_i] \\\\ =  \\sum_{i = 1}^{n}E[X_i] \\\\ =  \\sum_{i = 1}^{n}(1 - \\frac{1}{n})^{n - 1} \\\\ =  n(1 - \\frac{1}{n})^{n - 1}\n\\end{eqnarray}\n$$", 
            "title": "5.4-6"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-7", 
            "text": "", 
            "title": "5.4-7"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/", 
            "text": "Problems\n\n\n5-1\n\n\na\n\n\nLet's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by:\n\n\n$$\n\\begin{eqnarray}\nX_i \n=\n I\\lbrace\\text{the INCREMENT operation increases the counter}\\rbrace \\\\\n\n=\n \\begin{cases}\n      n_{i + 1} - n_i, \n \\text{the INCREMENT operation increases the counter} \\\\\n      0, \n \\text{otherwise}\n    \\end{cases}\n\\end{eqnarray}\n$$\n\n\nWe have:\n\n\n$$\n\\begin{eqnarray}\nE[X_i] \n=\n 0 * Pr\\lbrace \\text{the INCREMENT operation doesn't increase the counter} \\rbrace + (n_{i + 1} - n_i) * Pr\\lbrace \\text{the INCREMENT operation increases the counter} \\rbrace \\\\\n\n=\n 0 * (1 - \\frac{1}{n_{i + 1} - n_i}) + (n_{i + 1} - n_i) * \\frac{1}{n_{i + 1} - n_i} \\\\\n\n=\n 1\n\\end{eqnarray}\n$$\n\n\nLet X be the random variable as the expected value represented by the counter, we have $X = \\sum_{i = 1}^{n}X_i$. So:\n\n\n$$\n\\begin{eqnarray}\nE[X] \n=\n E[\\sum_{i = 1}^{n}X_i] \\\\\n\n=\n \\sum_{i = 1}^{n}E[X_i] \\\\\n\n=\n \\sum_{i = 1}^{n}1 \\\\\n\n=\n n\n\\end{eqnarray}\n$$\n\n\nb\n\n\n$Var(X_i) = E[X_i^2] - (E[X_i])^2 = 0^2 * (1 - \\frac{1}{n_{i + 1} - n_i}) + (n_{i + 1} - n_i)^2 * \\frac{1}{n_{i + 1} - n_i} - 1^2 = 100^2 * \\frac{1}{100} - 1^2 = 99$.\n\n\nBecause the random variables $X_i$ are uncorrelated, so $Var(X) = Var(\\sum_{i = 1}^{n}X_i) = \\sum_{i = 1}^{n}Var(X_i) = \\sum_{i = 1}^{n}99 = 99n$.\n\n\n5-2\n\n\na\n\n\nRANDOM-SEARCH(A, x)\n\nn = A.length\nlet B[1..n] be new array\n\nwhile B is not full\n    i = RANDOM(1, n)\n    B[i] = i\n\n    if A[i] = x\n        return i\n\nreturn -1\n\n\n\n\nb\n\n\nThis is similar like the question \"How many balls must we toss, an the average, until a given bin contains a ball?\", the answer is n.\n\n\nc\n\n\nEach time when we pick an index, we have the probability $\\frac{k}{n}$ such that we find x in A. So according to C.32, it takes $\\frac{1}{\\frac{k}{n}} = \\frac{n}{k}$ trials before we find x.\n\n\nd\n\n\nThis is similar like the question \"How many balls must we toss until every bin contains at least one ball?\", the answer is $n(\\ln{n} + O(1))$.\n\n\ne\n\n\nThe best-case running time is 1, and the worst-case running time is n, thus the average-case running time is $\\frac{n + 1}{2}$.\n\n\nf\n\n\nThe best-case running time is 1, and the worst-case running time is n - k + 1. The average-case running time is $\\frac{n + 1}{k + 1}$, here is the \ndiscussion\n.\n\n\ng\n\n\nIf there are no indices i such that A[i] = x, then it always scans all elements in A, thus the average-case and worst-case running time are both n.\n\n\nh\n\n\nThe worst-case running time is the same as DETERMINISTIC-SEARCH, the expected-case running time is the average-case running time in DETERMINISTIC-SEARCH.\n\n\ni\n\n\nThe DETERMINISTIC-SEARCH. It has better average-case running time, and the SCRAMBLE-SEARCH takes additional time to permute the array.", 
            "title": "Problems"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#problems", 
            "text": "", 
            "title": "Problems"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#5-1", 
            "text": "a  Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by:  $$\n\\begin{eqnarray}\nX_i  =  I\\lbrace\\text{the INCREMENT operation increases the counter}\\rbrace \\\\ =  \\begin{cases}\n      n_{i + 1} - n_i,   \\text{the INCREMENT operation increases the counter} \\\\\n      0,   \\text{otherwise}\n    \\end{cases}\n\\end{eqnarray}\n$$  We have:  $$\n\\begin{eqnarray}\nE[X_i]  =  0 * Pr\\lbrace \\text{the INCREMENT operation doesn't increase the counter} \\rbrace + (n_{i + 1} - n_i) * Pr\\lbrace \\text{the INCREMENT operation increases the counter} \\rbrace \\\\ =  0 * (1 - \\frac{1}{n_{i + 1} - n_i}) + (n_{i + 1} - n_i) * \\frac{1}{n_{i + 1} - n_i} \\\\ =  1\n\\end{eqnarray}\n$$  Let X be the random variable as the expected value represented by the counter, we have $X = \\sum_{i = 1}^{n}X_i$. So:  $$\n\\begin{eqnarray}\nE[X]  =  E[\\sum_{i = 1}^{n}X_i] \\\\ =  \\sum_{i = 1}^{n}E[X_i] \\\\ =  \\sum_{i = 1}^{n}1 \\\\ =  n\n\\end{eqnarray}\n$$  b  $Var(X_i) = E[X_i^2] - (E[X_i])^2 = 0^2 * (1 - \\frac{1}{n_{i + 1} - n_i}) + (n_{i + 1} - n_i)^2 * \\frac{1}{n_{i + 1} - n_i} - 1^2 = 100^2 * \\frac{1}{100} - 1^2 = 99$.  Because the random variables $X_i$ are uncorrelated, so $Var(X) = Var(\\sum_{i = 1}^{n}X_i) = \\sum_{i = 1}^{n}Var(X_i) = \\sum_{i = 1}^{n}99 = 99n$.", 
            "title": "5-1"
        }, 
        {
            "location": "/5-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#5-2", 
            "text": "a  RANDOM-SEARCH(A, x)\n\nn = A.length\nlet B[1..n] be new array\n\nwhile B is not full\n    i = RANDOM(1, n)\n    B[i] = i\n\n    if A[i] = x\n        return i\n\nreturn -1  b  This is similar like the question \"How many balls must we toss, an the average, until a given bin contains a ball?\", the answer is n.  c  Each time when we pick an index, we have the probability $\\frac{k}{n}$ such that we find x in A. So according to C.32, it takes $\\frac{1}{\\frac{k}{n}} = \\frac{n}{k}$ trials before we find x.  d  This is similar like the question \"How many balls must we toss until every bin contains at least one ball?\", the answer is $n(\\ln{n} + O(1))$.  e  The best-case running time is 1, and the worst-case running time is n, thus the average-case running time is $\\frac{n + 1}{2}$.  f  The best-case running time is 1, and the worst-case running time is n - k + 1. The average-case running time is $\\frac{n + 1}{k + 1}$, here is the  discussion .  g  If there are no indices i such that A[i] = x, then it always scans all elements in A, thus the average-case and worst-case running time are both n.  h  The worst-case running time is the same as DETERMINISTIC-SEARCH, the expected-case running time is the average-case running time in DETERMINISTIC-SEARCH.  i  The DETERMINISTIC-SEARCH. It has better average-case running time, and the SCRAMBLE-SEARCH takes additional time to permute the array.", 
            "title": "5-2"
        }, 
        {
            "location": "/6-Heapsort/6.1-Heaps/", 
            "text": "6.1 Heaps\n\n\n6.1-1\n\n\nThe height of a heap is defined to be the height of its root. The heap has minimum number of elements when height h contains only one element, and the heap has maximum number of elements when height contains $2^h$ elements.\n\n\nSo the minimum number of elements is $1 + 2^1 + 2^2 + \\ldots + 2^{h - 1} + 1 = 2^h$.\n\n\nThe maximum number of elements below root is $1 + 2^1 + 2^2 + \\ldots + 2^h = 2^{h + 1} - 1$.\n\n\n6.1-2\n\n\nIn the previous question we have proved if a heap has height h, then the number of elements n satisfies $2^h \\leq n \\leq 2^{h + 1} - 1$. So $2^h \\leq n \n 2^{h + 1}$, thus $h \\leq \\lg{n} \n h + 1$, so $h = \\lfloor \\lg{n} \\rfloor$.\n\n\n6.1-3\n\n\nIn a max-heap, for every node i other than the root, $A[PARENT(i)] \\geq A[i]$. So the root of a subtree contains the value that is greater than its left and right children. Since the left and right children are also the root of their own subtree, their values are greater than their own children's values. Thus, the root of the subtree contains the largest value occurring anywhere in that subtree.\n\n\n6.1-4\n\n\nIt cannot be a root of subtree, it must be a leaf.\n\n\n6.1-5\n\n\nIf the array is in increasing order, then for any index $i \\leq j$, we have $A[i] \\leq A[j]$, thus for any root index i of a subtree in heap, the left child index is 2i, and the right child index is 2i + 1, it's obvious that i \n 2i and i \n 2i + 1, thus $A[i] \\leq A[2i]$ and $A[i] \\leq A[2i + 1]$, so it's a min-heap.\n\n\nBut if the array is in descending order, it's not a min-heap.\n\n\n6.1-6\n\n\nThe subtree root index of 4 has right child with index 9, but A[4] \n A[9]. So it's not a max-heap.\n\n\n6.1-7\n\n\nIf a root of subtree has index i, then its left child index is 2i, and its right child index is 2i + 1. So it must satisfy $2i \\leq n$, $i \\leq \\lfloor \\frac{n}{2} \\rfloor$. So the maximum index of subtree root is $\\lfloor \\frac{n}{2} \\rfloor$, so the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\lfloor \\frac{n}{2} \\rfloor + 2, \\ldots, n$.", 
            "title": "6.1 Heaps"
        }, 
        {
            "location": "/6-Heapsort/6.1-Heaps/#61-heaps", 
            "text": "", 
            "title": "6.1 Heaps"
        }, 
        {
            "location": "/6-Heapsort/6.1-Heaps/#61-1", 
            "text": "The height of a heap is defined to be the height of its root. The heap has minimum number of elements when height h contains only one element, and the heap has maximum number of elements when height contains $2^h$ elements.  So the minimum number of elements is $1 + 2^1 + 2^2 + \\ldots + 2^{h - 1} + 1 = 2^h$.  The maximum number of elements below root is $1 + 2^1 + 2^2 + \\ldots + 2^h = 2^{h + 1} - 1$.", 
            "title": "6.1-1"
        }, 
        {
            "location": "/6-Heapsort/6.1-Heaps/#61-2", 
            "text": "In the previous question we have proved if a heap has height h, then the number of elements n satisfies $2^h \\leq n \\leq 2^{h + 1} - 1$. So $2^h \\leq n   2^{h + 1}$, thus $h \\leq \\lg{n}   h + 1$, so $h = \\lfloor \\lg{n} \\rfloor$.", 
            "title": "6.1-2"
        }, 
        {
            "location": "/6-Heapsort/6.1-Heaps/#61-3", 
            "text": "In a max-heap, for every node i other than the root, $A[PARENT(i)] \\geq A[i]$. So the root of a subtree contains the value that is greater than its left and right children. Since the left and right children are also the root of their own subtree, their values are greater than their own children's values. Thus, the root of the subtree contains the largest value occurring anywhere in that subtree.", 
            "title": "6.1-3"
        }, 
        {
            "location": "/6-Heapsort/6.1-Heaps/#61-4", 
            "text": "It cannot be a root of subtree, it must be a leaf.", 
            "title": "6.1-4"
        }, 
        {
            "location": "/6-Heapsort/6.1-Heaps/#61-5", 
            "text": "If the array is in increasing order, then for any index $i \\leq j$, we have $A[i] \\leq A[j]$, thus for any root index i of a subtree in heap, the left child index is 2i, and the right child index is 2i + 1, it's obvious that i   2i and i   2i + 1, thus $A[i] \\leq A[2i]$ and $A[i] \\leq A[2i + 1]$, so it's a min-heap.  But if the array is in descending order, it's not a min-heap.", 
            "title": "6.1-5"
        }, 
        {
            "location": "/6-Heapsort/6.1-Heaps/#61-6", 
            "text": "The subtree root index of 4 has right child with index 9, but A[4]   A[9]. So it's not a max-heap.", 
            "title": "6.1-6"
        }, 
        {
            "location": "/6-Heapsort/6.1-Heaps/#61-7", 
            "text": "If a root of subtree has index i, then its left child index is 2i, and its right child index is 2i + 1. So it must satisfy $2i \\leq n$, $i \\leq \\lfloor \\frac{n}{2} \\rfloor$. So the maximum index of subtree root is $\\lfloor \\frac{n}{2} \\rfloor$, so the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\lfloor \\frac{n}{2} \\rfloor + 2, \\ldots, n$.", 
            "title": "6.1-7"
        }, 
        {
            "location": "/6-Heapsort/6.2-Maintaining-the-heap-property/", 
            "text": "6.2 Maintaining the heap property\n\n\n6.2-1\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=30mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode] {27}\n      child {\n        node [circle,draw,treenode] {17}\n        child {\n            node [circle,draw,treenode] {16}\n            child {\n                node [circle,draw,treenode] {5}\n            }\n            child {\n                node [circle,draw,treenode] {7}\n            }}\n        child {\n            node [circle,draw,treenode] {13}\n            child {\n                node [circle,draw,treenode] {12}\n            }\n            child {\n                node [circle,draw,treenode] {14}\n            }\n        }\n      }\n      child {\n        node [circle,draw,treenode] {10}\n        child {\n            node [circle,draw,treenode] {9}\n            child {\n                node [circle,draw,treenode] {8}\n            }\n            child {\n                node [circle,draw,treenode,current] {3}\n            }\n        }\n        child {\n            node [circle,draw,treenode]  {1}\n            child {\n                node [circle,draw,treenode] {0}\n            }\n            child [missing]\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n\n\n\n\n6.2-2\n\n\nMIN-HEAPIFY(A, i)\nl = LEFT(i)\nr = RIGHT(i)\nif l \n= A.heap-size and A[l] \n A[i]\n    min = l\nelse\n    min = i\nif r \n= A.heap-size and A[r] \n A[min]\n    min = r\nif min != i\n    exchange A[i] with A[min]\n    MIN-HEAPIFY(A, min)\n\n\n\n\nThe running time is still $O(\\lg{n})$.\n\n\n6.2-3\n\n\nThe procedure terminates and the running time is O(1).\n\n\n6.2-4\n\n\nIn exercise 6.1-7 we know the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\ldots$, thus if i \n A.heap-size / 2, then the node at index i is a leaf, so both l \n= A.heap-size and r \n= A.heap-size fail, the procedure terminates.\n\n\n6.2-5\n\n\nMAX-HEAPIFY(A, i)\nlargest = -1\nroot = i\nwhile largest != root\n    l = LEFT(root)\n    r = RIGHT(root)\n    if l \n= A.heap-size and A[l] \n A[root]\n        largest = l\n    else\n        largest = root\n    if r \n= A.heap-size and A[r] \n A[largest]\n        largest = r\n    if largest != root:\n        exchange A[root] with A[largest]\n        root = largest\n        largest = -1\n\n\n\n\n6.2-6\n\n\nThe worst-case happens when it checks every node from the root down to a leaf. And the height h is $\\lfloor \\lg{n} \\rfloor \\geq \\lg{n}$, so the worst-case running time is $\\Omega(\\lg{n})$.", 
            "title": "6.2 Maintaining the heap property"
        }, 
        {
            "location": "/6-Heapsort/6.2-Maintaining-the-heap-property/#62-maintaining-the-heap-property", 
            "text": "", 
            "title": "6.2 Maintaining the heap property"
        }, 
        {
            "location": "/6-Heapsort/6.2-Maintaining-the-heap-property/#62-1", 
            "text": "\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=30mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode] {27}\n      child {\n        node [circle,draw,treenode] {17}\n        child {\n            node [circle,draw,treenode] {16}\n            child {\n                node [circle,draw,treenode] {5}\n            }\n            child {\n                node [circle,draw,treenode] {7}\n            }}\n        child {\n            node [circle,draw,treenode] {13}\n            child {\n                node [circle,draw,treenode] {12}\n            }\n            child {\n                node [circle,draw,treenode] {14}\n            }\n        }\n      }\n      child {\n        node [circle,draw,treenode] {10}\n        child {\n            node [circle,draw,treenode] {9}\n            child {\n                node [circle,draw,treenode] {8}\n            }\n            child {\n                node [circle,draw,treenode,current] {3}\n            }\n        }\n        child {\n            node [circle,draw,treenode]  {1}\n            child {\n                node [circle,draw,treenode] {0}\n            }\n            child [missing]\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}", 
            "title": "6.2-1"
        }, 
        {
            "location": "/6-Heapsort/6.2-Maintaining-the-heap-property/#62-2", 
            "text": "MIN-HEAPIFY(A, i)\nl = LEFT(i)\nr = RIGHT(i)\nif l  = A.heap-size and A[l]   A[i]\n    min = l\nelse\n    min = i\nif r  = A.heap-size and A[r]   A[min]\n    min = r\nif min != i\n    exchange A[i] with A[min]\n    MIN-HEAPIFY(A, min)  The running time is still $O(\\lg{n})$.", 
            "title": "6.2-2"
        }, 
        {
            "location": "/6-Heapsort/6.2-Maintaining-the-heap-property/#62-3", 
            "text": "The procedure terminates and the running time is O(1).", 
            "title": "6.2-3"
        }, 
        {
            "location": "/6-Heapsort/6.2-Maintaining-the-heap-property/#62-4", 
            "text": "In exercise 6.1-7 we know the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\ldots$, thus if i   A.heap-size / 2, then the node at index i is a leaf, so both l  = A.heap-size and r  = A.heap-size fail, the procedure terminates.", 
            "title": "6.2-4"
        }, 
        {
            "location": "/6-Heapsort/6.2-Maintaining-the-heap-property/#62-5", 
            "text": "MAX-HEAPIFY(A, i)\nlargest = -1\nroot = i\nwhile largest != root\n    l = LEFT(root)\n    r = RIGHT(root)\n    if l  = A.heap-size and A[l]   A[root]\n        largest = l\n    else\n        largest = root\n    if r  = A.heap-size and A[r]   A[largest]\n        largest = r\n    if largest != root:\n        exchange A[root] with A[largest]\n        root = largest\n        largest = -1", 
            "title": "6.2-5"
        }, 
        {
            "location": "/6-Heapsort/6.2-Maintaining-the-heap-property/#62-6", 
            "text": "The worst-case happens when it checks every node from the root down to a leaf. And the height h is $\\lfloor \\lg{n} \\rfloor \\geq \\lg{n}$, so the worst-case running time is $\\Omega(\\lg{n})$.", 
            "title": "6.2-6"
        }, 
        {
            "location": "/6-Heapsort/6.3-Building-a-heap/", 
            "text": "6.3 Building a heap\n\n\n6.3-1\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=20mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode] {5}\n      child {\n        node [circle,draw,treenode] {3}\n        child {\n            node [circle,draw,treenode,current] {10}\n            child {\n                node [circle,draw,treenode] {22}\n            }\n            child {\n                node [circle,draw,treenode] {9}\n            }}\n        child {\n            node [circle,draw,treenode] {84}\n        }\n      }\n      child {\n        node [circle,draw,treenode] {17}\n        child {\n            node [circle,draw,treenode] {19}\n        }\n        child {\n            node [circle,draw,treenode]  {6}\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.3-2\n\n\nIf we loop the index in a increasing order, when we call MAX-HEAPIFY(A, i), it's possible that the left subtree at root 2i and the right subtree at root 2i + 1 are not max-heap.\n\n\n6.3-3\n\n\nRemember that the height of a node is the distance from the node to a leaf, such that the height of a leaf is 0 (and the height of the root is the height of the tree).\n\n\nIn exercise 6.1-7, we proved that for a heap has n elements, the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\lfloor \\frac{n}{2} \\rfloor + 2, \\ldots, n$. So there are $\\lceil \\frac{n}{2} \\rceil$ leaves. And the height of leaves is 0, so when h = 0, $\\lceil \\frac{n}{2^{h + 1}} \\rceil = \\lceil \\frac{n}{2^{0 + 1}} \\rceil = \\lceil \\frac{n}{2} \\rceil$. The base case holds.\n\n\nNow let's consider the inductive step. Suppose there are at most $\\lceil \\frac{n}{2^h} \\rceil$ nodes for height h - 1. And let's cut the leaves for the tree with height h. So it has $n - \\lceil \\frac{n}{2} \\rceil = \\lfloor \\frac{n}{2} \\rfloor$ elements. And it's height becomes h - 1. By induction, the nodes at height h - 1 is $\\lceil \\frac{\\lfloor \\frac{n}{2} \\rfloor}{2^h} \\rceil \\leq \\lceil \\frac{\\frac{n}{2}}{2^h} \\rceil = \\lceil \\frac{n}{2^{h + 1}} \\rceil$, which is also the number of nodes at height h in the original tree.", 
            "title": "6.3 Building a heap"
        }, 
        {
            "location": "/6-Heapsort/6.3-Building-a-heap/#63-building-a-heap", 
            "text": "", 
            "title": "6.3 Building a heap"
        }, 
        {
            "location": "/6-Heapsort/6.3-Building-a-heap/#63-1", 
            "text": "\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=20mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode] {5}\n      child {\n        node [circle,draw,treenode] {3}\n        child {\n            node [circle,draw,treenode,current] {10}\n            child {\n                node [circle,draw,treenode] {22}\n            }\n            child {\n                node [circle,draw,treenode] {9}\n            }}\n        child {\n            node [circle,draw,treenode] {84}\n        }\n      }\n      child {\n        node [circle,draw,treenode] {17}\n        child {\n            node [circle,draw,treenode] {19}\n        }\n        child {\n            node [circle,draw,treenode]  {6}\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}", 
            "title": "6.3-1"
        }, 
        {
            "location": "/6-Heapsort/6.3-Building-a-heap/#63-2", 
            "text": "If we loop the index in a increasing order, when we call MAX-HEAPIFY(A, i), it's possible that the left subtree at root 2i and the right subtree at root 2i + 1 are not max-heap.", 
            "title": "6.3-2"
        }, 
        {
            "location": "/6-Heapsort/6.3-Building-a-heap/#63-3", 
            "text": "Remember that the height of a node is the distance from the node to a leaf, such that the height of a leaf is 0 (and the height of the root is the height of the tree).  In exercise 6.1-7, we proved that for a heap has n elements, the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\lfloor \\frac{n}{2} \\rfloor + 2, \\ldots, n$. So there are $\\lceil \\frac{n}{2} \\rceil$ leaves. And the height of leaves is 0, so when h = 0, $\\lceil \\frac{n}{2^{h + 1}} \\rceil = \\lceil \\frac{n}{2^{0 + 1}} \\rceil = \\lceil \\frac{n}{2} \\rceil$. The base case holds.  Now let's consider the inductive step. Suppose there are at most $\\lceil \\frac{n}{2^h} \\rceil$ nodes for height h - 1. And let's cut the leaves for the tree with height h. So it has $n - \\lceil \\frac{n}{2} \\rceil = \\lfloor \\frac{n}{2} \\rfloor$ elements. And it's height becomes h - 1. By induction, the nodes at height h - 1 is $\\lceil \\frac{\\lfloor \\frac{n}{2} \\rfloor}{2^h} \\rceil \\leq \\lceil \\frac{\\frac{n}{2}}{2^h} \\rceil = \\lceil \\frac{n}{2^{h + 1}} \\rceil$, which is also the number of nodes at height h in the original tree.", 
            "title": "6.3-3"
        }, 
        {
            "location": "/6-Heapsort/6.4-The-heapsort-algorithm/", 
            "text": "6.4 The heapsort algorithm\n\n\n6.4-1\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=20mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode] {5}\n      child {\n        node [circle,draw,treenode] {13}\n        child {\n            node [circle,draw,treenode] {25}\n            child {\n                node [circle,draw,treenode] {8}\n            }\n            child {\n                node [circle,draw,treenode] {4}\n            }}\n        child {\n            node [circle,draw,treenode] {7}\n        }\n      }\n      child {\n        node [circle,draw,treenode] {2}\n        child {\n            node [circle,draw,treenode] {17}\n        }\n        child {\n            node [circle,draw,treenode]  {20}\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4-2\n\n\nInitialization\n: Prior to the first iteration of the loop, i = n, and we called \nBUILD-MAX-HEAP\n on A, thus the subarray \nA[1..n]\n is a max-heap containing the n smallest elements of \nA[1..n]\n, and the subarray \nA[n + 1..n]\n is an empty array, thus it contains n - n = 0 largest elements of \nA[1..n]\n, sorted.\n\n\nMaintenance\n: Let's assume prior to the ith iteration of the loop, the subarray \nA[1..i]\n is a max-heap containing the i smallest elements of \nA[1..n]\n, and the subarray \nA[i + 1,..n]\n contains the n - i largest elements of \nA[1..n]\n, sorted.\n\n\nIn the ith loop, \nA[1]\n is the biggest number in the subarray \nA[1..i]\n, in line 3, it exchanges \nA[1]\n and \nA[i]\n, making the subarray \nA[i..n]\n contains the n - i + 1 largest elements of \nA[1..n]\n, sorted. In line 4-5, it calls \nMAX-HEAPIFY\n on A, so it makes the subarray \nA[1..i - 1]\n to a max-heap contains the i - 1 smallest elements of \nA[1..n]\n.\n\n\nTermination\n: At termination, i = 1. By the loop invariant, the subarray \nA[1..1]\n is a max-heap containing the 1 smallest elements of \nA[1..n]\n. And the subarray \nA[2..n]\n contains the n - 1 largest elements of \nA[1..n]\n, sorted. So the array \nA[1..n]\n is sorted.\n\n\n6.4-3\n\n\nIf the array is in increasing order, then \nBUILD-MAX-HEAP\n takes O(n), and it changes A to a max-heap. Then it calls \nMAX-HEAPIFY(A, 1)\n n - 1 times, at each iteration, there are i - 1 elements in the current max-heap, so the running time of for loop is $\\sum_{i = n - 1}^{1}\\lg{i} = \\lg{((n - 1)!)} = \\Theta(n\\lg{n})$ (exercise 3.2-3). So the total running time is $O(n) + \\Theta(n\\lg{n}) = \\Theta(n\\lg{n})$.\n\n\nIf the array is in decreasing order, it takes O(n) to call \nBUILD-MAX-HEAP\n, but it won't change the array. But it still needs $\\Theta(n\\lg{n})$ running time to call \nMAX-HEAPIFY(A, 1)\n. The total running time is also $\\Theta(n\\lg{n})$.\n\n\n6.4-4\n\n\nThe worst-case happens when we need to call \nMAX-HEAPIFY(A, 1)\n in each iteration with the cost $\\lg{(i - 1)}$. And the answer is given in the above question.\n\n\n6.4-5\n\n\nYou can find the answer \nhere\n.", 
            "title": "6.4 The heapsort algorithm"
        }, 
        {
            "location": "/6-Heapsort/6.4-The-heapsort-algorithm/#64-the-heapsort-algorithm", 
            "text": "", 
            "title": "6.4 The heapsort algorithm"
        }, 
        {
            "location": "/6-Heapsort/6.4-The-heapsort-algorithm/#64-1", 
            "text": "\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=20mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode] {5}\n      child {\n        node [circle,draw,treenode] {13}\n        child {\n            node [circle,draw,treenode] {25}\n            child {\n                node [circle,draw,treenode] {8}\n            }\n            child {\n                node [circle,draw,treenode] {4}\n            }}\n        child {\n            node [circle,draw,treenode] {7}\n        }\n      }\n      child {\n        node [circle,draw,treenode] {2}\n        child {\n            node [circle,draw,treenode] {17}\n        }\n        child {\n            node [circle,draw,treenode]  {20}\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}", 
            "title": "6.4-1"
        }, 
        {
            "location": "/6-Heapsort/6.4-The-heapsort-algorithm/#64-2", 
            "text": "Initialization : Prior to the first iteration of the loop, i = n, and we called  BUILD-MAX-HEAP  on A, thus the subarray  A[1..n]  is a max-heap containing the n smallest elements of  A[1..n] , and the subarray  A[n + 1..n]  is an empty array, thus it contains n - n = 0 largest elements of  A[1..n] , sorted.  Maintenance : Let's assume prior to the ith iteration of the loop, the subarray  A[1..i]  is a max-heap containing the i smallest elements of  A[1..n] , and the subarray  A[i + 1,..n]  contains the n - i largest elements of  A[1..n] , sorted.  In the ith loop,  A[1]  is the biggest number in the subarray  A[1..i] , in line 3, it exchanges  A[1]  and  A[i] , making the subarray  A[i..n]  contains the n - i + 1 largest elements of  A[1..n] , sorted. In line 4-5, it calls  MAX-HEAPIFY  on A, so it makes the subarray  A[1..i - 1]  to a max-heap contains the i - 1 smallest elements of  A[1..n] .  Termination : At termination, i = 1. By the loop invariant, the subarray  A[1..1]  is a max-heap containing the 1 smallest elements of  A[1..n] . And the subarray  A[2..n]  contains the n - 1 largest elements of  A[1..n] , sorted. So the array  A[1..n]  is sorted.", 
            "title": "6.4-2"
        }, 
        {
            "location": "/6-Heapsort/6.4-The-heapsort-algorithm/#64-3", 
            "text": "If the array is in increasing order, then  BUILD-MAX-HEAP  takes O(n), and it changes A to a max-heap. Then it calls  MAX-HEAPIFY(A, 1)  n - 1 times, at each iteration, there are i - 1 elements in the current max-heap, so the running time of for loop is $\\sum_{i = n - 1}^{1}\\lg{i} = \\lg{((n - 1)!)} = \\Theta(n\\lg{n})$ (exercise 3.2-3). So the total running time is $O(n) + \\Theta(n\\lg{n}) = \\Theta(n\\lg{n})$.  If the array is in decreasing order, it takes O(n) to call  BUILD-MAX-HEAP , but it won't change the array. But it still needs $\\Theta(n\\lg{n})$ running time to call  MAX-HEAPIFY(A, 1) . The total running time is also $\\Theta(n\\lg{n})$.", 
            "title": "6.4-3"
        }, 
        {
            "location": "/6-Heapsort/6.4-The-heapsort-algorithm/#64-4", 
            "text": "The worst-case happens when we need to call  MAX-HEAPIFY(A, 1)  in each iteration with the cost $\\lg{(i - 1)}$. And the answer is given in the above question.", 
            "title": "6.4-4"
        }, 
        {
            "location": "/6-Heapsort/6.4-The-heapsort-algorithm/#64-5", 
            "text": "You can find the answer  here .", 
            "title": "6.4-5"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/", 
            "text": "6.5 Priority queues\n\n\n6.5-1\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=30mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode,current] {15}\n      child {\n        node [circle,draw,treenode] {13}\n        child {\n            node [circle,draw,treenode] {5}\n            child {\n                node [circle,draw,treenode] {4}\n            }\n            child {\n                node [circle,draw,treenode] {0}\n            }}\n        child {\n            node [circle,draw,treenode] {12}\n            child {\n                node [circle,draw,treenode] {6}\n            }\n            child {\n                node [circle,draw,treenode] {2}\n            }\n        }\n      }\n      child {\n        node [circle,draw,treenode] {9}\n        child {\n            node [circle,draw,treenode] {8}\n            child {\n                node [circle,draw,treenode] {1}\n            }\n            child [missing] {}\n        }\n        child {\n            node [circle,draw,treenode]  {7}\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n\n\n\n\n6.5-2\n\n\n\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=30mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode] {15}\n      child {\n        node [circle,draw,treenode] {13}\n        child {\n            node [circle,draw,treenode] {5}\n            child {\n                node [circle,draw,treenode] {4}\n            }\n            child {\n                node [circle,draw,treenode] {0}\n            }}\n        child {\n            node [circle,draw,treenode] {12}\n            child {\n                node [circle,draw,treenode] {6}\n            }\n            child {\n                node [circle,draw,treenode] {2}\n            }\n        }\n      }\n      child {\n        node [circle,draw,treenode] {9}\n        child {\n            node [circle,draw,treenode] {8}\n            child {\n                node [circle,draw,treenode] {1}\n            }\n            child [missing] {}\n        }\n        child {\n            node [circle,draw,treenode]  {7}\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\n\n\n\n\n\n\n\n\n6.5-3\n\n\nHEAP-MINIMUM(A)\n    return A[1]\n\n\n\n\nHEAP-EXTRACT-MIN(A)\n    if A.heap-size \n 1\n        error \nheap underflow\n\n    min = A[1]\n    A[1] = A[A.heap-size]\n    A.heap-size = A.heap-size - 1\n    MIN-HEAPIFY(A, 1)\n    return max\n\n\n\n\nHEAP-DECREASE-KEY(A, i, key)\n    if key \n A[i]\n        error \nnew key is bigger than current key\n\n    A[i] = key\n    while i \n 1 and A[PARENT(i)] \n A[i]\n        exchange A[i] with A[PARENT(i)]\n        i = PARENT(i)\n\n\n\n\nMIN-HEAP-INSERT(A, key)\n    A.heap-size = A.heap-size + 1\n    A[A.heap-size] = +\u221e\n    HEAP-DECREASE-KEY(A, A.heap-size, key)\n\n\n\n\n6.5-4\n\n\nSetting the key of the inserted node to negative infinity to make sure it can pass the truth check in line 1 of \nHEAP-INCREASE-KEY(A, i, key)\n.\n\n\n6.5-5\n\n\nInitialization\n: Prior to the first iteration of the loop, the subarray \nA[1..A.heap-size]\n is satifies the max-heap before we set \nA[i] = key\n, after we set \nA[i] = key\n, it's possible that \nA[i]\n would be greater than its parent. And we cannot set a smaller key to \nA[i]\n, so there may be only one violation.\n\n\nMaintenance\n: In the for loop, we are exchanging \nA[i]\n and \nA[PARENT(i)]\n, before the exchange, the subtree rooted at \nA[i]\n is a max-heap, but it may be greater than its parent, so there may be on violation.\n\n\nTermination\n: When it's terminated, i maybe 1 or \nA[i]\n is smaller thant its parent, either makes A a max-heap.\n\n\n6.5-6\n\n\nHEAP-INCREASE-KEY(A, i, key)\nif key \n A[i]\n    error \nnew key is smaller than current key\n\nwhile i \n 1 and A[PARENT(i)] \n key\n    A[i] = A[PARENT(i)]\n    i = PARENT(i)\nA[i] = key\n\n\n\n\n6.5-7\n\n\nImplement a first-in, first-out queue with a min priority queue. Call \nMIN-HEAP-INSERT\n method to enqueue with a priority, for example, use timestamp as priority. Call \nHEAP-MINIMUM\n to dequeue.\n\n\nImplement a stack with a max priority queue. Call \nMAX-HEAP-INSERT\n method to push with a priority, for example, use timestamp as priority. Call \nHEAP-MAXIMUM\n to pop.\n\n\n6.5-8\n\n\nHEAP-DELETE(A, i)\nA.heap-size \n 1\n    error \nheap underflow\n\nA[i] = A[A.heap-size]\nA.heap-size = A.heap-size - 1\nMAX-HEAPIFY(A, i)\n\n\n\n\n6.5-9\n\n\nFirst we create a k-elements min heap from the first element in each sorted list, this requires $O(k) + O(n) = O(n)$.\n\n\nThen we keep extracting the min value from min heap, and insert the next value after the min value in the original list, this requires $O(\\lg{k})$. And there are n elements, so it requires $O(n\\lg{k})$.\n\n\nThus the running time is $O(n) + O(n\\lg{k}) = O(n\\lg{k})$.\n\n\ndef merge_sorted_lists(lists):\n    sorted_list = []\n    min_heap_elements = []\n\n    for i in range(len(lists)):\n        min_heap_elements.append(MinHeapElement(i, 1, lists[i][0]))\n\n    min_heap = MinHeap(min_heap_elements)\n\n    while not min_heap.is_empty():\n        min_element = min_heap.extract_min()\n        sorted_list.append(min_element.value)\n\n        list_index = min_element.list_index\n        next_index = min_element.next_index\n\n        if next_index \n len(lists[list_index]):\n            next_element = MinHeapElement(\n                list_index, next_index + 1, lists[list_index][next_index])\n\n            min_heap.insert(next_element)\n\n    return sorted_list\n\n\nclass MinHeapElement():\n    def __init__(self, list_index, next_index, value):\n        self.list_index = list_index\n        self.next_index = next_index\n        self.value = value\n\n\nclass MinHeap():\n    def __init__(self, elements):\n        self.elements = elements\n        self.heap_size = len(elements)\n\n        self.build_min_heap()\n\n    def extract_min(self):\n        assert not self.is_empty()\n\n        minimum = self.elements[0]\n        self.elements[0] = self.elements[self.heap_size - 1]\n        self.heap_size -= 1\n\n        self.min_heapify(0)\n\n        return minimum\n\n    def insert(self, element):\n        self.heap_size += 1\n\n        if len(self.elements) \n self.heap_size:\n            self.elements.append(element)\n        else:\n            self.elements[self.heap_size - 1] = element\n\n        self.decrease_element(self.heap_size - 1, element)\n\n    def decrease_element(self, i, element):\n        assert i \n self.heap_size\n        assert element.value \n= self.elements[i].value\n\n        while i \n 0 and self.elements[(i - 1) // 2].value \n element.value:\n            self.elements[i] = self.elements[(i - 1) // 2]\n            i = (i - 1) // 2\n\n        self.elements[i] = element\n\n    def is_empty(self):\n        return self.heap_size == 0\n\n    def min_heapify(self, i):\n        left = 2 * i + 1\n        right = 2 * i + 2\n        minimum = i\n\n        if (left \n= self.heap_size - 1 and\n                self.elements[left].value \n self.elements[i].value):\n            minimum = left\n\n        if (right \n= self.heap_size - 1 and\n                self.elements[right].value \n self.elements[minimum].value):\n            minimum = right\n\n        if minimum != i:\n            self.elements[i], self.elements[minimum] = \\\n                self.elements[minimum], self.elements[i]\n\n            self.min_heapify(minimum)\n\n    def build_min_heap(self):\n        for i in range((self.heap_size - 1) // 2, -1, -1):\n            self.min_heapify(i)", 
            "title": "6.5 Priority queues"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-priority-queues", 
            "text": "", 
            "title": "6.5 Priority queues"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-1", 
            "text": "\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=30mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode,current] {15}\n      child {\n        node [circle,draw,treenode] {13}\n        child {\n            node [circle,draw,treenode] {5}\n            child {\n                node [circle,draw,treenode] {4}\n            }\n            child {\n                node [circle,draw,treenode] {0}\n            }}\n        child {\n            node [circle,draw,treenode] {12}\n            child {\n                node [circle,draw,treenode] {6}\n            }\n            child {\n                node [circle,draw,treenode] {2}\n            }\n        }\n      }\n      child {\n        node [circle,draw,treenode] {9}\n        child {\n            node [circle,draw,treenode] {8}\n            child {\n                node [circle,draw,treenode] {1}\n            }\n            child [missing] {}\n        }\n        child {\n            node [circle,draw,treenode]  {7}\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}", 
            "title": "6.5-1"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-2", 
            "text": "\\documentclass{standalone}\n\\usepackage{tikz}\n\n\\begin{document}\n\\begin{tikzpicture}[level/.style={sibling distance=30mm/#1},\ntreenode/.style={align=center, inner sep=0pt, text width=1.2em, text centered},\ncurrent/.style={fill=gray}]\n    \\node [circle,draw,treenode] {15}\n      child {\n        node [circle,draw,treenode] {13}\n        child {\n            node [circle,draw,treenode] {5}\n            child {\n                node [circle,draw,treenode] {4}\n            }\n            child {\n                node [circle,draw,treenode] {0}\n            }}\n        child {\n            node [circle,draw,treenode] {12}\n            child {\n                node [circle,draw,treenode] {6}\n            }\n            child {\n                node [circle,draw,treenode] {2}\n            }\n        }\n      }\n      child {\n        node [circle,draw,treenode] {9}\n        child {\n            node [circle,draw,treenode] {8}\n            child {\n                node [circle,draw,treenode] {1}\n            }\n            child [missing] {}\n        }\n        child {\n            node [circle,draw,treenode]  {7}\n        }\n    };\n    \\end{tikzpicture}\n\\end{document}", 
            "title": "6.5-2"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-3", 
            "text": "HEAP-MINIMUM(A)\n    return A[1]  HEAP-EXTRACT-MIN(A)\n    if A.heap-size   1\n        error  heap underflow \n    min = A[1]\n    A[1] = A[A.heap-size]\n    A.heap-size = A.heap-size - 1\n    MIN-HEAPIFY(A, 1)\n    return max  HEAP-DECREASE-KEY(A, i, key)\n    if key   A[i]\n        error  new key is bigger than current key \n    A[i] = key\n    while i   1 and A[PARENT(i)]   A[i]\n        exchange A[i] with A[PARENT(i)]\n        i = PARENT(i)  MIN-HEAP-INSERT(A, key)\n    A.heap-size = A.heap-size + 1\n    A[A.heap-size] = +\u221e\n    HEAP-DECREASE-KEY(A, A.heap-size, key)", 
            "title": "6.5-3"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-4", 
            "text": "Setting the key of the inserted node to negative infinity to make sure it can pass the truth check in line 1 of  HEAP-INCREASE-KEY(A, i, key) .", 
            "title": "6.5-4"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-5", 
            "text": "Initialization : Prior to the first iteration of the loop, the subarray  A[1..A.heap-size]  is satifies the max-heap before we set  A[i] = key , after we set  A[i] = key , it's possible that  A[i]  would be greater than its parent. And we cannot set a smaller key to  A[i] , so there may be only one violation.  Maintenance : In the for loop, we are exchanging  A[i]  and  A[PARENT(i)] , before the exchange, the subtree rooted at  A[i]  is a max-heap, but it may be greater than its parent, so there may be on violation.  Termination : When it's terminated, i maybe 1 or  A[i]  is smaller thant its parent, either makes A a max-heap.", 
            "title": "6.5-5"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-6", 
            "text": "HEAP-INCREASE-KEY(A, i, key)\nif key   A[i]\n    error  new key is smaller than current key \nwhile i   1 and A[PARENT(i)]   key\n    A[i] = A[PARENT(i)]\n    i = PARENT(i)\nA[i] = key", 
            "title": "6.5-6"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-7", 
            "text": "Implement a first-in, first-out queue with a min priority queue. Call  MIN-HEAP-INSERT  method to enqueue with a priority, for example, use timestamp as priority. Call  HEAP-MINIMUM  to dequeue.  Implement a stack with a max priority queue. Call  MAX-HEAP-INSERT  method to push with a priority, for example, use timestamp as priority. Call  HEAP-MAXIMUM  to pop.", 
            "title": "6.5-7"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-8", 
            "text": "HEAP-DELETE(A, i)\nA.heap-size   1\n    error  heap underflow \nA[i] = A[A.heap-size]\nA.heap-size = A.heap-size - 1\nMAX-HEAPIFY(A, i)", 
            "title": "6.5-8"
        }, 
        {
            "location": "/6-Heapsort/6.5-Priority-queues/#65-9", 
            "text": "First we create a k-elements min heap from the first element in each sorted list, this requires $O(k) + O(n) = O(n)$.  Then we keep extracting the min value from min heap, and insert the next value after the min value in the original list, this requires $O(\\lg{k})$. And there are n elements, so it requires $O(n\\lg{k})$.  Thus the running time is $O(n) + O(n\\lg{k}) = O(n\\lg{k})$.  def merge_sorted_lists(lists):\n    sorted_list = []\n    min_heap_elements = []\n\n    for i in range(len(lists)):\n        min_heap_elements.append(MinHeapElement(i, 1, lists[i][0]))\n\n    min_heap = MinHeap(min_heap_elements)\n\n    while not min_heap.is_empty():\n        min_element = min_heap.extract_min()\n        sorted_list.append(min_element.value)\n\n        list_index = min_element.list_index\n        next_index = min_element.next_index\n\n        if next_index   len(lists[list_index]):\n            next_element = MinHeapElement(\n                list_index, next_index + 1, lists[list_index][next_index])\n\n            min_heap.insert(next_element)\n\n    return sorted_list\n\n\nclass MinHeapElement():\n    def __init__(self, list_index, next_index, value):\n        self.list_index = list_index\n        self.next_index = next_index\n        self.value = value\n\n\nclass MinHeap():\n    def __init__(self, elements):\n        self.elements = elements\n        self.heap_size = len(elements)\n\n        self.build_min_heap()\n\n    def extract_min(self):\n        assert not self.is_empty()\n\n        minimum = self.elements[0]\n        self.elements[0] = self.elements[self.heap_size - 1]\n        self.heap_size -= 1\n\n        self.min_heapify(0)\n\n        return minimum\n\n    def insert(self, element):\n        self.heap_size += 1\n\n        if len(self.elements)   self.heap_size:\n            self.elements.append(element)\n        else:\n            self.elements[self.heap_size - 1] = element\n\n        self.decrease_element(self.heap_size - 1, element)\n\n    def decrease_element(self, i, element):\n        assert i   self.heap_size\n        assert element.value  = self.elements[i].value\n\n        while i   0 and self.elements[(i - 1) // 2].value   element.value:\n            self.elements[i] = self.elements[(i - 1) // 2]\n            i = (i - 1) // 2\n\n        self.elements[i] = element\n\n    def is_empty(self):\n        return self.heap_size == 0\n\n    def min_heapify(self, i):\n        left = 2 * i + 1\n        right = 2 * i + 2\n        minimum = i\n\n        if (left  = self.heap_size - 1 and\n                self.elements[left].value   self.elements[i].value):\n            minimum = left\n\n        if (right  = self.heap_size - 1 and\n                self.elements[right].value   self.elements[minimum].value):\n            minimum = right\n\n        if minimum != i:\n            self.elements[i], self.elements[minimum] = \\\n                self.elements[minimum], self.elements[i]\n\n            self.min_heapify(minimum)\n\n    def build_min_heap(self):\n        for i in range((self.heap_size - 1) // 2, -1, -1):\n            self.min_heapify(i)", 
            "title": "6.5-9"
        }, 
        {
            "location": "/6-Heapsort/Problems/", 
            "text": "Problems\n\n\n6-1\n\n\na\n\n\nLet \nA = [1, 2, 3, 4]\n, if we use \nBUILD-MAX-HEAP\n, we have \nA = [4, 2, 3, 1]\n. If we use \nBUILD-MAX-HEAP'\n, we have \nA = [4, 3, 2, 1]\n. That's a counterexample.\n\n\nb\n\n\nThe worst-case happens when A is in increasing order, in each iteration, \nMAX-HEAP-INSERT(A, A[i])\n requires $O(\\lg{k})$ to heapify the heap (move \nA[i]\n from bottom to top), where k is the number of elements in heap. Thus the running time is $\\sum_{i = 2}^{n}\\lg{i} = \\lg{(n!)} = \\Theta(n\\lg{n})$.\n\n\n6-2\n\n\na\n\n\nFor a node with index i (i = 1, 2, ..., n), its kth child index is \nd(i - 1) + k + 1\n in array, and its parent's index is $\\lceil \\frac{i - 1}{d} \\rceil$.\n\n\nb\n\n\nThe max number of elements in d-ary heap is $d^0 + d^1 + \\ldots + d^h = \\sum_{i = 0}^{h}d^h = \\frac{d^{h + 1} - 1}{d - 1}$, the min number of elements in d-ary heap is $d^0 + d^1 + \\ldots + d^{h - 1} + 1 = \\sum_{i = 0}^{h}d^{h - 1} + 1 = \\frac{d^h - 1}{d - 1} + 1$. Thus, $\\frac{d^h - 1}{d - 1} + 1 \\leq n \\leq \\frac{d^{h + 1} - 1}{d - 1}$.\n\n\nFor a d-ary heap, we have $d \\geq 2$. Let's check the left part first. $\\frac{d^h - 1}{d - 1} + 1 = \\frac{d^h - 1 + (d - 2)}{d - 1} \\geq \\frac{d^h - 1}{d - 1}$, so $h \\leq \\log_d{(n(d - 1) + 1)}$. And:\n\n\n$$\n\\begin{eqnarray}\n\\log_d{(n(d - 1) + 1)} - (\\log_d{(n(d - 1))} + 1)  \n=\n \\log_d{(\\frac{1}{d}(1 + \\frac{1}{n(d - 1)}))} \\\\\n\n\\leq\n \\log_d{(\\frac{1}{d}(1 + \\frac{1}{d - 1}))} \\\\\n\n=\n \\log_d{\\frac{1}{d - 1}} \\\\\n\n\\leq\n \\log_d{\\frac{1}{\\frac{d}{2}}} \\\\\n\n=\n \\log_d{\\frac{2}{d}} \\\\\n\n=\n \\log_d2 - 1 \\\\\n\n\\leq\n \\log_2{2} - 1 \\\\\n\n=\n 0\n\\end{eqnarray}\n$$\n\n\nSo $\\log_d{(n(d - 1) + 1)} \\leq \\log_d{(n(d - 1))} + 1$, if n = 1 and d = 2, we have $\\log_d{(n(d - 1) + 1)} = \\log_d{(n(d - 1))} + 1$, for a d-ary heap, n is usually not 1, so we can say $\\log_d{(n(d - 1) + 1)} \n \\log_d{(n(d - 1))} + 1$. Thus we have $h \n \\log_d{(n(d - 1))} + 1$.\n\n\nThen let's check the right part. $\\frac{d^{h + 1} - 1}{d - 1} \n \\frac{d^{h + 1}}{d - 1}$, so $h \n \\log_d{(n(d - 1))} - 1$.\n\n\nSo $h = \\lfloor \\log_d{(n(d - 1))} \\rfloor$.\n\n\nc\n\n\nIt's similar like \nEXTRACT-MAX\n 2-ary heap. The running time of \nEXTRACT-MAX\n for d-ary heap is mainly the running time of \nMAX-HEAPIFY\n. We need to change \nMAX-HEAPIFY\n a little bit.\n\n\nMAX-HEAPIFY(A, i)\nlargest = i\nfor i = 1 to d\n    child = CHILD(i)\n    if child \n= A.heap-size and A[child] \n A[largest]\n        largest = child\nif largest != i\n    exchange A[i] with A[largest]\n    MAX-HEAPIFY(A, largest)\n\n\n\n\nSo the running time of \nMAX-HEAPIFY\n for d-ary heap is $O(dh) = O(d\\log_d{(n(d - 1))})$.\n\n\nd\n\n\nThe \nINSERT\n method only compares node i with its parent, so the running time is $O(h) = O(\\log_d{(n(d - 1))})$.\n\n\ne\n\n\nSince the \nINSERT\n method calls \nINCREASE-KEY\n method, the running time of \nINCREASE-KEY\n is also $O(h) = O(\\log_d{(n(d - 1))})$.\n\n\n6-3\n\n\na\n\n\n$$\n\\begin{matrix}\n2 \n 3 \n 4 \n 5 \\\\\n8 \n 9 \n 12 \n 14 \\\\\n16 \n \\infty \n \\infty \n \\infty \\\\\n\\infty \n \\infty \n \\infty \n \\infty\n\\end{matrix}\n$$\n\n\nb\n\n\nIf $Y[1, 1] = \\infty$, then no cell can has a value greater than \nY[1, 1]\n, other cells are all $\\infty$, so Y is empty.\n\n\nY[m, n]\n is the largest in Y, if it's not $\\infty$, then others are also not $\\infty$, so Y is full.\n\n\nc\n\n\nY[1, 1]\n is the smallest, after we extract it, we need to make Y to a Young tableau again.\n\n\nFirst we start at \nY[1, 1]\n, then we compare its right element and below element, if right element is smaller than below element, we put right element to \nY[1, 1]\n, which makes \nY[1, 1]\n to \nY[1, m]\n sorted, but \nY[1, 2]\n to \nY[m, n]\n might be not a Young tableau.\n\n\nif right element is greater than below element, we put below element to \nY[1, 1]\n, which makes \nY[1, 1]\n to \nY[1, n]\n sorted, but \nY[2, 1]\n to \nY[m, n]\n might be not a Young tableau.\n\n\nEXTRACT-MIN(Y)\nsmallest = Y[1, 1]\nY[1, 1] = \u221e\nYOUNGIFY(Y, 1, 1)\nreturn smallest\n\nYOUNGIFY(Y, row, column)\nsmallest_row = row\nsmallest_column = column\nif row + 1 \n= m and Y[row + 1, column] \n Y[row, column]\n    smallest_row = row + 1\nif column + 1 \n= n and Y[row, column + 1] \n Y[smallest_row, smallest_column]\n    smallest_row = row\n    smallest_column = column + 1\nif smallest_row != row or smallest_column != column\n    exchange Y[row, column] with Y[smallest_row, smallest_column]\n    YOUNGIFY(Y, smallest_row, smallest_column)\n\n\n\n\nAfter each recursive procedure, the \nYOUNGIFY\n method cuts a row or a column, which reduces the problem size to (m - 1) x n or m x (n - 1). Notice that both (m - 1) + n = m + n - 1 and m + (n - 1) = m + n - 1, so m + n is decreased by 1 in each recursive procedure. so T(p) = T(p - 1) + O(1), it's obvious to know that T(p) = O(p) = O(m + n).\n\n\nd\n\n\nSimilar like \nEXTRACT-MIN\n, we first put the new element to \nY[m, n]\n, then youngify the tableau from \nY[m, n]\n. The running time is O(m + n).\n\n\nINSERT(Y, key)\nY[m][n] = key\nYOUNGIFY-INSERT(Y, m, n)\n\nYOUNGIFY-INSERT(Y, row, column)\nlargest_row = row\nlargest_column = column\nif row - 1 \n= 1 and Y[row - 1][column] \n Y[row][column]\n    largest_row = row - 1\nif column - 1 \n= 1 and Y[row][column - 1] \n Y[largest_row][largest_column]\n    largest_row = row\n    largest_column = column - 1\nif largest_row != row or largest_column != column\n    exchange Y[row, column] with Y[largest_row, largest_column]\n    YOUNGIFY-INSERT(Y, largest_row, largest_column)\n\n\n\n\ne\n\n\nThe \nINSERT\n operation takes O(n + n) = O(n) time, and there are $n^2$ elements, so we can insert each element into the n x n Young tableau, thus the running time is $n^2O(n) = O(n^3)$.\n\n\nf\n\n\nWe start from \nY[m, 1]\n, and compare \nY[m, 1]\n with target value, if \nY[m, 1]\n is greater than target value, we move to \nY[m - 1, 1]\n, if it's smaller than target value, we move to \nY[m, 2]\n, otherwise, we find the target value.\n\n\nFIND(Y, key)\nrow = m\ncolumn = 1\nwhile row \n= 1 and column \n= n\n    if Y[row][column] == key\n        return True\n    else if Y[row][column] \n key\n        column += 1\n    else if Y[row][column] \n key\n        row -= 1\nreturn False\n\n\n\n\nSimilar like \nYOUNGIFY\n, it reduces the problem size to (m - 1) * n or m * (n - 1), so the running time is O(m + n).", 
            "title": "Problems"
        }, 
        {
            "location": "/6-Heapsort/Problems/#problems", 
            "text": "", 
            "title": "Problems"
        }, 
        {
            "location": "/6-Heapsort/Problems/#6-1", 
            "text": "a  Let  A = [1, 2, 3, 4] , if we use  BUILD-MAX-HEAP , we have  A = [4, 2, 3, 1] . If we use  BUILD-MAX-HEAP' , we have  A = [4, 3, 2, 1] . That's a counterexample.  b  The worst-case happens when A is in increasing order, in each iteration,  MAX-HEAP-INSERT(A, A[i])  requires $O(\\lg{k})$ to heapify the heap (move  A[i]  from bottom to top), where k is the number of elements in heap. Thus the running time is $\\sum_{i = 2}^{n}\\lg{i} = \\lg{(n!)} = \\Theta(n\\lg{n})$.", 
            "title": "6-1"
        }, 
        {
            "location": "/6-Heapsort/Problems/#6-2", 
            "text": "a  For a node with index i (i = 1, 2, ..., n), its kth child index is  d(i - 1) + k + 1  in array, and its parent's index is $\\lceil \\frac{i - 1}{d} \\rceil$.  b  The max number of elements in d-ary heap is $d^0 + d^1 + \\ldots + d^h = \\sum_{i = 0}^{h}d^h = \\frac{d^{h + 1} - 1}{d - 1}$, the min number of elements in d-ary heap is $d^0 + d^1 + \\ldots + d^{h - 1} + 1 = \\sum_{i = 0}^{h}d^{h - 1} + 1 = \\frac{d^h - 1}{d - 1} + 1$. Thus, $\\frac{d^h - 1}{d - 1} + 1 \\leq n \\leq \\frac{d^{h + 1} - 1}{d - 1}$.  For a d-ary heap, we have $d \\geq 2$. Let's check the left part first. $\\frac{d^h - 1}{d - 1} + 1 = \\frac{d^h - 1 + (d - 2)}{d - 1} \\geq \\frac{d^h - 1}{d - 1}$, so $h \\leq \\log_d{(n(d - 1) + 1)}$. And:  $$\n\\begin{eqnarray}\n\\log_d{(n(d - 1) + 1)} - (\\log_d{(n(d - 1))} + 1)   =  \\log_d{(\\frac{1}{d}(1 + \\frac{1}{n(d - 1)}))} \\\\ \\leq  \\log_d{(\\frac{1}{d}(1 + \\frac{1}{d - 1}))} \\\\ =  \\log_d{\\frac{1}{d - 1}} \\\\ \\leq  \\log_d{\\frac{1}{\\frac{d}{2}}} \\\\ =  \\log_d{\\frac{2}{d}} \\\\ =  \\log_d2 - 1 \\\\ \\leq  \\log_2{2} - 1 \\\\ =  0\n\\end{eqnarray}\n$$  So $\\log_d{(n(d - 1) + 1)} \\leq \\log_d{(n(d - 1))} + 1$, if n = 1 and d = 2, we have $\\log_d{(n(d - 1) + 1)} = \\log_d{(n(d - 1))} + 1$, for a d-ary heap, n is usually not 1, so we can say $\\log_d{(n(d - 1) + 1)}   \\log_d{(n(d - 1))} + 1$. Thus we have $h   \\log_d{(n(d - 1))} + 1$.  Then let's check the right part. $\\frac{d^{h + 1} - 1}{d - 1}   \\frac{d^{h + 1}}{d - 1}$, so $h   \\log_d{(n(d - 1))} - 1$.  So $h = \\lfloor \\log_d{(n(d - 1))} \\rfloor$.  c  It's similar like  EXTRACT-MAX  2-ary heap. The running time of  EXTRACT-MAX  for d-ary heap is mainly the running time of  MAX-HEAPIFY . We need to change  MAX-HEAPIFY  a little bit.  MAX-HEAPIFY(A, i)\nlargest = i\nfor i = 1 to d\n    child = CHILD(i)\n    if child  = A.heap-size and A[child]   A[largest]\n        largest = child\nif largest != i\n    exchange A[i] with A[largest]\n    MAX-HEAPIFY(A, largest)  So the running time of  MAX-HEAPIFY  for d-ary heap is $O(dh) = O(d\\log_d{(n(d - 1))})$.  d  The  INSERT  method only compares node i with its parent, so the running time is $O(h) = O(\\log_d{(n(d - 1))})$.  e  Since the  INSERT  method calls  INCREASE-KEY  method, the running time of  INCREASE-KEY  is also $O(h) = O(\\log_d{(n(d - 1))})$.", 
            "title": "6-2"
        }, 
        {
            "location": "/6-Heapsort/Problems/#6-3", 
            "text": "a  $$\n\\begin{matrix}\n2   3   4   5 \\\\\n8   9   12   14 \\\\\n16   \\infty   \\infty   \\infty \\\\\n\\infty   \\infty   \\infty   \\infty\n\\end{matrix}\n$$  b  If $Y[1, 1] = \\infty$, then no cell can has a value greater than  Y[1, 1] , other cells are all $\\infty$, so Y is empty.  Y[m, n]  is the largest in Y, if it's not $\\infty$, then others are also not $\\infty$, so Y is full.  c  Y[1, 1]  is the smallest, after we extract it, we need to make Y to a Young tableau again.  First we start at  Y[1, 1] , then we compare its right element and below element, if right element is smaller than below element, we put right element to  Y[1, 1] , which makes  Y[1, 1]  to  Y[1, m]  sorted, but  Y[1, 2]  to  Y[m, n]  might be not a Young tableau.  if right element is greater than below element, we put below element to  Y[1, 1] , which makes  Y[1, 1]  to  Y[1, n]  sorted, but  Y[2, 1]  to  Y[m, n]  might be not a Young tableau.  EXTRACT-MIN(Y)\nsmallest = Y[1, 1]\nY[1, 1] = \u221e\nYOUNGIFY(Y, 1, 1)\nreturn smallest\n\nYOUNGIFY(Y, row, column)\nsmallest_row = row\nsmallest_column = column\nif row + 1  = m and Y[row + 1, column]   Y[row, column]\n    smallest_row = row + 1\nif column + 1  = n and Y[row, column + 1]   Y[smallest_row, smallest_column]\n    smallest_row = row\n    smallest_column = column + 1\nif smallest_row != row or smallest_column != column\n    exchange Y[row, column] with Y[smallest_row, smallest_column]\n    YOUNGIFY(Y, smallest_row, smallest_column)  After each recursive procedure, the  YOUNGIFY  method cuts a row or a column, which reduces the problem size to (m - 1) x n or m x (n - 1). Notice that both (m - 1) + n = m + n - 1 and m + (n - 1) = m + n - 1, so m + n is decreased by 1 in each recursive procedure. so T(p) = T(p - 1) + O(1), it's obvious to know that T(p) = O(p) = O(m + n).  d  Similar like  EXTRACT-MIN , we first put the new element to  Y[m, n] , then youngify the tableau from  Y[m, n] . The running time is O(m + n).  INSERT(Y, key)\nY[m][n] = key\nYOUNGIFY-INSERT(Y, m, n)\n\nYOUNGIFY-INSERT(Y, row, column)\nlargest_row = row\nlargest_column = column\nif row - 1  = 1 and Y[row - 1][column]   Y[row][column]\n    largest_row = row - 1\nif column - 1  = 1 and Y[row][column - 1]   Y[largest_row][largest_column]\n    largest_row = row\n    largest_column = column - 1\nif largest_row != row or largest_column != column\n    exchange Y[row, column] with Y[largest_row, largest_column]\n    YOUNGIFY-INSERT(Y, largest_row, largest_column)  e  The  INSERT  operation takes O(n + n) = O(n) time, and there are $n^2$ elements, so we can insert each element into the n x n Young tableau, thus the running time is $n^2O(n) = O(n^3)$.  f  We start from  Y[m, 1] , and compare  Y[m, 1]  with target value, if  Y[m, 1]  is greater than target value, we move to  Y[m - 1, 1] , if it's smaller than target value, we move to  Y[m, 2] , otherwise, we find the target value.  FIND(Y, key)\nrow = m\ncolumn = 1\nwhile row  = 1 and column  = n\n    if Y[row][column] == key\n        return True\n    else if Y[row][column]   key\n        column += 1\n    else if Y[row][column]   key\n        row -= 1\nreturn False  Similar like  YOUNGIFY , it reduces the problem size to (m - 1) * n or m * (n - 1), so the running time is O(m + n).", 
            "title": "6-3"
        }, 
        {
            "location": "/7-Quicksort/7.1-Description-of-quicksort/", 
            "text": "7.1 Description of quicksort\n\n\n7.1-1\n\n\n\\documentclass{standalone}\n\n\\usepackage{colortbl}\n\\makeatletter\n\\newcolumntype{W}{!{\\smash{\\vrule\n\\@width 4\\arrayrulewidth\n\\@height\\dimexpr\\ht\\@arstrutbox+2pt\\relax\n\\@depth\\dimexpr\\dp\\@arstrutbox+2pt\\relax}}}\n\\makeatother\n\\definecolor{gray}{rgb}{.5,.5,.5}\n\\definecolor{lightgray}{rgb}{.8,.8,.8}\n\\begin{document}\n\\begin{tabular}{c|c|cWc|cWc|c|c|c|c|c|cWc|}\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{$p$}\n\n\\multicolumn{1}{c}{$i$}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{$j$}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{$r$}\\\\\n\n\n\\cellcolor{lightgray}9\n\n\\cellcolor{lightgray}5\n\n\\cellcolor{gray}13\n\n\\cellcolor{gray}19\n\n12\n\n8\n\n7\n\n4\n\n21\n\n2\n\n6\n\n11\n\\end{tabular}\n\\end{document}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.1-2\n\n\nWhen all elements in the array \nA[p..r]\n have the same value, it returns \nr\n.\n\n\nPARTITION(A, p, r)\nx = A[r]\ni = p - 1\nsame_value_count = 0\nfor j = p to r - 1\n    if A[j] \n= x\n        if A[j] == x\n            same_value_count = same_value_count + 1\n        i = i + 1\n        exchange A[i] with A[j]\nexchange A[i + 1] with A[r]\nif same_value_count = r - p\n    return Math.floor((p + 2) / 2)\nelse\n    return i + 1\n\n\n\n\n7.1-3\n\n\nThe \nPARTITION\n takes $\\Theta(r - p)$ because it iterates the array from j to p. So the \nPARTITION\n on a subarray of size n is $\\Theta(n)$.\n\n\n7.1-4\n\n\nQUICKSORT(A, p, r)\nif p \n r\n    q = PARTITION(A, p, r)\n    QUICKSORT(A, p, q - 1)\n    QUICKSORT(A, q + 1, r)\n\nPARTITION(A, p, r)\nx = A[r]\ni = p - 1\nfor j = p to r - 1\n    if A[j] \n= x\n        i = i + 1\n        exchange A[i] with A[j]\nexchange A[i + 1] with A[r]\nreturn i + 1", 
            "title": "7.1 Description of quicksort"
        }, 
        {
            "location": "/7-Quicksort/7.1-Description-of-quicksort/#71-description-of-quicksort", 
            "text": "", 
            "title": "7.1 Description of quicksort"
        }, 
        {
            "location": "/7-Quicksort/7.1-Description-of-quicksort/#71-1", 
            "text": "\\documentclass{standalone}\n\n\\usepackage{colortbl}\n\\makeatletter\n\\newcolumntype{W}{!{\\smash{\\vrule\n\\@width 4\\arrayrulewidth\n\\@height\\dimexpr\\ht\\@arstrutbox+2pt\\relax\n\\@depth\\dimexpr\\dp\\@arstrutbox+2pt\\relax}}}\n\\makeatother\n\\definecolor{gray}{rgb}{.5,.5,.5}\n\\definecolor{lightgray}{rgb}{.8,.8,.8}\n\\begin{document}\n\\begin{tabular}{c|c|cWc|cWc|c|c|c|c|c|cWc|}\n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{$p$} \n\\multicolumn{1}{c}{$i$} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{$j$} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{$r$}\\\\ \n\\cellcolor{lightgray}9 \n\\cellcolor{lightgray}5 \n\\cellcolor{gray}13 \n\\cellcolor{gray}19 \n12 \n8 \n7 \n4 \n21 \n2 \n6 \n11\n\\end{tabular}\n\\end{document}", 
            "title": "7.1-1"
        }, 
        {
            "location": "/7-Quicksort/7.1-Description-of-quicksort/#71-2", 
            "text": "When all elements in the array  A[p..r]  have the same value, it returns  r .  PARTITION(A, p, r)\nx = A[r]\ni = p - 1\nsame_value_count = 0\nfor j = p to r - 1\n    if A[j]  = x\n        if A[j] == x\n            same_value_count = same_value_count + 1\n        i = i + 1\n        exchange A[i] with A[j]\nexchange A[i + 1] with A[r]\nif same_value_count = r - p\n    return Math.floor((p + 2) / 2)\nelse\n    return i + 1", 
            "title": "7.1-2"
        }, 
        {
            "location": "/7-Quicksort/7.1-Description-of-quicksort/#71-3", 
            "text": "The  PARTITION  takes $\\Theta(r - p)$ because it iterates the array from j to p. So the  PARTITION  on a subarray of size n is $\\Theta(n)$.", 
            "title": "7.1-3"
        }, 
        {
            "location": "/7-Quicksort/7.1-Description-of-quicksort/#71-4", 
            "text": "QUICKSORT(A, p, r)\nif p   r\n    q = PARTITION(A, p, r)\n    QUICKSORT(A, p, q - 1)\n    QUICKSORT(A, q + 1, r)\n\nPARTITION(A, p, r)\nx = A[r]\ni = p - 1\nfor j = p to r - 1\n    if A[j]  = x\n        i = i + 1\n        exchange A[i] with A[j]\nexchange A[i + 1] with A[r]\nreturn i + 1", 
            "title": "7.1-4"
        }, 
        {
            "location": "/7-Quicksort/7.2-Performance-of-quicksort/", 
            "text": "7.2 Performance of quicksort\n\n\n7.2-1\n\n\nWe start by assuming that this bound holds for all possitive m \n n, in particular for m = n - 1, yielding $T(n - 1) \\leq c(n - 1)^2$. Substituting into the recurrence yields:\n\n\n$T(n) = T(n - 1) + \\Theta(n) \\leq c(n - 1)^2 + dn = cn^2 + (d-2c)n + c \\leq cn^2$\n\n\nwhere the last step holds as long as $c \n \\frac{d}{2}$ and $n \\geq \\frac{c}{2c - d}$.\n\n\n7.2-2\n\n\nWhen all elements of array A have the same value, the \nPARTITION\n method returns \nr\n, thus it reduces the problem size to n - 1 and 0, which yields the recurrence in the above exercise. So the running time is $\\Theta(n^2)$.\n\n\n7.2-3\n\n\nWhen the array A contains decreasing sorted elements, the \nPARTITION\n method returns \np\n, thus it reduces the problem size to 0 and n - 1, same like the above exercise. The running time is $\\Theta(n^2)$.\n\n\n7.2-4\n\n\nSuppose there are k exceptions in an almost-sorted array, in \nINSERTION-SORT\n, we need to move the k exceptions to right position, each takes at most $O(n)$, so the total running time is O(n + kn). When k is small enough, \nINSERTION-SORT\n tends to be linear sort.\n\n\nIn \nQUICKSORT\n, we need to split the array into two parts, since the array is almost-sorted, it's quite possible that it will create 0 size subarray, thus the running time tends to be $O(n^2)$.\n\n\nSo for an almost-sorted array, \nINSERTION-SORT\n would tend to beat \nQUICKSORT\n.\n\n\n7.2-5\n\n\nWe have proved it in exercise 4.4-9. When $0 \n \\alpha \\leq \\frac{1}{2}$, the minimum depth of a leaf is $\\log_{\\alpha}{\\frac{1}{n}} = -\\log_{\\alpha}{n} = -\\frac{\\lg{n}}{\\lg{\\alpha}}$. The maximum depth is $\\log_{1 - \\alpha}{\\frac{1}{n}} = -\\log_{1 - \\alpha}{n} = -\\frac{\\lg{n}}{\\lg{1 - \\alpha}}$.\n\n\n7.2-6\n\n\nThe splits at each level quicksort are in the proportion $1 - \\alpha$ and $\\alpha$, thus the probability that \nPARTITION\n produces a more balanced split is $\\frac{(1 - \\alpha - \\alpha)n}{n} = 1 - 2\\alpha$.", 
            "title": "7.2 Performance of quicksort"
        }, 
        {
            "location": "/7-Quicksort/7.2-Performance-of-quicksort/#72-performance-of-quicksort", 
            "text": "", 
            "title": "7.2 Performance of quicksort"
        }, 
        {
            "location": "/7-Quicksort/7.2-Performance-of-quicksort/#72-1", 
            "text": "We start by assuming that this bound holds for all possitive m   n, in particular for m = n - 1, yielding $T(n - 1) \\leq c(n - 1)^2$. Substituting into the recurrence yields:  $T(n) = T(n - 1) + \\Theta(n) \\leq c(n - 1)^2 + dn = cn^2 + (d-2c)n + c \\leq cn^2$  where the last step holds as long as $c   \\frac{d}{2}$ and $n \\geq \\frac{c}{2c - d}$.", 
            "title": "7.2-1"
        }, 
        {
            "location": "/7-Quicksort/7.2-Performance-of-quicksort/#72-2", 
            "text": "When all elements of array A have the same value, the  PARTITION  method returns  r , thus it reduces the problem size to n - 1 and 0, which yields the recurrence in the above exercise. So the running time is $\\Theta(n^2)$.", 
            "title": "7.2-2"
        }, 
        {
            "location": "/7-Quicksort/7.2-Performance-of-quicksort/#72-3", 
            "text": "When the array A contains decreasing sorted elements, the  PARTITION  method returns  p , thus it reduces the problem size to 0 and n - 1, same like the above exercise. The running time is $\\Theta(n^2)$.", 
            "title": "7.2-3"
        }, 
        {
            "location": "/7-Quicksort/7.2-Performance-of-quicksort/#72-4", 
            "text": "Suppose there are k exceptions in an almost-sorted array, in  INSERTION-SORT , we need to move the k exceptions to right position, each takes at most $O(n)$, so the total running time is O(n + kn). When k is small enough,  INSERTION-SORT  tends to be linear sort.  In  QUICKSORT , we need to split the array into two parts, since the array is almost-sorted, it's quite possible that it will create 0 size subarray, thus the running time tends to be $O(n^2)$.  So for an almost-sorted array,  INSERTION-SORT  would tend to beat  QUICKSORT .", 
            "title": "7.2-4"
        }, 
        {
            "location": "/7-Quicksort/7.2-Performance-of-quicksort/#72-5", 
            "text": "We have proved it in exercise 4.4-9. When $0   \\alpha \\leq \\frac{1}{2}$, the minimum depth of a leaf is $\\log_{\\alpha}{\\frac{1}{n}} = -\\log_{\\alpha}{n} = -\\frac{\\lg{n}}{\\lg{\\alpha}}$. The maximum depth is $\\log_{1 - \\alpha}{\\frac{1}{n}} = -\\log_{1 - \\alpha}{n} = -\\frac{\\lg{n}}{\\lg{1 - \\alpha}}$.", 
            "title": "7.2-5"
        }, 
        {
            "location": "/7-Quicksort/7.2-Performance-of-quicksort/#72-6", 
            "text": "The splits at each level quicksort are in the proportion $1 - \\alpha$ and $\\alpha$, thus the probability that  PARTITION  produces a more balanced split is $\\frac{(1 - \\alpha - \\alpha)n}{n} = 1 - 2\\alpha$.", 
            "title": "7.2-6"
        }, 
        {
            "location": "/7-Quicksort/7.3-A-randomized-version-of-quicksort/", 
            "text": "7.3 A randomized version of quicksort\n\n\n7.3-1\n\n\nThe worst-case is a special case, analyzing the expected running time can be more resonable that reflects the complexity of the algorithm.\n\n\n7.3-2\n\n\nThe worst-case satifies $T(n) = T(n - 1) + \\Theta(1)$, thus $T(n) = \\Theta(n)$. The best case satifies $T(n) = 2T(\\frac{n}{2}) + \\Theta(1)$, thus $T(n) = \\Theta(n)$.", 
            "title": "7.3 A randomized version of quicksort"
        }, 
        {
            "location": "/7-Quicksort/7.3-A-randomized-version-of-quicksort/#73-a-randomized-version-of-quicksort", 
            "text": "", 
            "title": "7.3 A randomized version of quicksort"
        }, 
        {
            "location": "/7-Quicksort/7.3-A-randomized-version-of-quicksort/#73-1", 
            "text": "The worst-case is a special case, analyzing the expected running time can be more resonable that reflects the complexity of the algorithm.", 
            "title": "7.3-1"
        }, 
        {
            "location": "/7-Quicksort/7.3-A-randomized-version-of-quicksort/#73-2", 
            "text": "The worst-case satifies $T(n) = T(n - 1) + \\Theta(1)$, thus $T(n) = \\Theta(n)$. The best case satifies $T(n) = 2T(\\frac{n}{2}) + \\Theta(1)$, thus $T(n) = \\Theta(n)$.", 
            "title": "7.3-2"
        }, 
        {
            "location": "/7-Quicksort/7.4-Analysis-of-quicksort/", 
            "text": "7.4 Analysis of quicksort\n\n\n7.4-1\n\n\nWe guess that $T(n) \\geq cn^2$ for some constant c. Substituting this guess into recurrence, we obtain:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n \\max_{0 \\leq q \\leq n - 1}(T(q) + T(n - q - 1)) + \\Theta(n) \\\\\n\n\\geq\n \\max_{0 \\leq q \\leq n - 1}(cq^2 + c(n - q - 1)^2) + \\Theta(n) \\\\\n\n=\n c\\max_{0 \\leq q \\leq n - 1}(q^2 + (n - q - 1)^2) + \\Theta(n) \\\\\n\n=\n c\\max_{0 \\leq q \\leq n - 1}(2q^2 - 2(n - 1)q + (n - 1)^2) + \\Theta(n)\n\\end{eqnarray}\n$$\n\n\n$f(q) = 2q^2 - 2(n - 1)q + (n - 1)^2$ is monotonically decreasing at $[0, \\frac{n - 1}{2}]$, and monotonically increasing at $[\\frac{n - 1}{2}, n - 1]$. So $\\max_{0 \\leq q \\leq n - 1}(2q^2 - 2(n - 1)q + (n - 1)^2) = f(0) = f(n) = (n - 1)^2$. Thus:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\geq\n c(n - 1)^2 + \\Theta(n) \\\\\n\n=\n c(n - 1)^2 + dn \\\\\n\n=\n cn^2 + (d - 2c)n + c \\\\\n\n cn^2\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $ d \\geq 2c$.\n\n\nSo $T(n) = \\Omega(n^2)$\n\n\n7.4-2\n\n\nThe recurrence for the best-case is $T(n) = 2T(\\frac{n}{2}) + \\Theta(n)$. Let's solve it with the master method. We have $a = 2, b = 2, f(n) = \\Theta(n)$, and thus we have that $n^{\\log_b{a}} = n^{\\log_2{2}} = n = \\Theta(n)$. Since $f(n) = \\Theta(n)$, we can apply case 2 of the master theorem and conclude that the solution is $T(n) = \\Theta(n\\lg{n})$. Thus, $T(n) = \\Omega(n\\lg{n})$.\n\n\n7.4-3\n\n\nWe've already knew it in the first exercise.\n\n\n7.4-4\n\n\nWe have:\n\n\n$$\n\\begin{eqnarray}\nE[X] \n=\n \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n \\frac{2}{j - i + 1} \\\\\n\n=\n \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{2}{k + 1} \\\\\n\n\\geq\n \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{2}{2k} \\\\\n\n=\n \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{1}{k} \\\\\n\n=\n \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{i} \\frac{1}{k} \\\\\n\n=\n \\sum_{i = 1}^{n - 1}(\\ln{i} + O(1)) \n \\text{(A.7)} \\\\\n\n=\n \\ln{((n - 1)!)} + (n - 1)O(1) \\\\\n\n=\n \\Omega(n\\lg{n}) + O(n) \n \\text{(Exercise 3.2-3)} \\\\\n\n=\n \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\n7.4-5\n\n\nQuicksort splits the array into two parts, let's say $T(n) = T(\\alpha{n}) + T((1 - \\alpha)n) + \\Theta(n)$. In exercise 7.2-5, we know when $0 \\leq \\alpha \\leq \\frac{1}{2}$, the minimum depth of a leaf is $\\log_{\\alpha}{\\frac{1}{n}}$ and the maximum depth is $\\log_{1 - \\alpha}{\\frac{1}{n}}$, when $\\frac{1}{2} \n \\alpha \n 1$, it's opposite. To make it simple, let the recursion tree depth be $\\log_{\\beta}n$, since the new algorithm terminates when a subarray contains fewer than k elements, so we have $\\frac{n}{\\beta^{i}} = k$ and $i = \\log_{\\beta}{\\frac{n}{k}}$, so the depth is $O(\\lg{\\frac{n}{k}})$. Since each recursion tree level costs $O(n)$, the running time of quicksort is $O(n\\lg{\\frac{n}{k}})$.\n\n\nThe last step is sorting the array by insertion sort. Let's assume n is exactly divisible by k, thus, after quicksort, the $\\frac{n}{k}$ subarrays (each with length $\\frac{n}{k}$) is sorted, but it's not sorted within the subarray. It requires $O(k^2)$ to sort each subarray, so it requires $\\frac{n}{k}k^2 = nk$ to sort entir array.\n\n\nSo the running time is $O(nk + n\\lg{\\frac{n}{k}})$.\n\n\nIn order to beat quicksort, we need to find a k such that $O(nk + n\\lg{\\frac{n}{k}}) \n O(n\\lg{n})$, let's compare $nk + n\\lg{\\frac{n}{k}}$ and $n\\lg{n}$.\n\n\n$$\n\\begin{eqnarray}\nnk + n\\lg{\\frac{n}{k}} - n\\lg{n} \n=\n nk + n\\lg{\\frac{1}{k}} \\\\\n\n=\n nk - n\\lg{k} \\\\\n\n=\n n(k - \\lg{k})\n\\end{eqnarray}\n$$\n\n\nBut k grows faster than $\\lg{k}$, so we cannot pick a good k here. Thus we can multiply a constant c to $n\\lg{n}$. Thus:\n\n\n$$\n\\begin{eqnarray}\nnk + n\\lg{\\frac{n}{k}} - cn\\lg{n} \n=\n nk + n\\lg{n} - n\\lg{k} - cn\\lg{n} \\\\\n\n=\n n(\\lg{2^k} + \\lg{n} - \\lg{k} - \\lg{n^c}) \\\\\n\n=\n n\\lg{\\frac{2^k}{kn^{c - 1}}}\n\\end{eqnarray}\n$$\n\n\nSo we only need to find a k that satifies $\\frac{2^k}{kn^{c - 1}} \n 1$, which is possible.\n\n\nIn practice, we need to do some tests to find a proper k.\n\n\n7.4-6", 
            "title": "7.4 Analysis of quicksort"
        }, 
        {
            "location": "/7-Quicksort/7.4-Analysis-of-quicksort/#74-analysis-of-quicksort", 
            "text": "", 
            "title": "7.4 Analysis of quicksort"
        }, 
        {
            "location": "/7-Quicksort/7.4-Analysis-of-quicksort/#74-1", 
            "text": "We guess that $T(n) \\geq cn^2$ for some constant c. Substituting this guess into recurrence, we obtain:  $$\n\\begin{eqnarray}\nT(n)  =  \\max_{0 \\leq q \\leq n - 1}(T(q) + T(n - q - 1)) + \\Theta(n) \\\\ \\geq  \\max_{0 \\leq q \\leq n - 1}(cq^2 + c(n - q - 1)^2) + \\Theta(n) \\\\ =  c\\max_{0 \\leq q \\leq n - 1}(q^2 + (n - q - 1)^2) + \\Theta(n) \\\\ =  c\\max_{0 \\leq q \\leq n - 1}(2q^2 - 2(n - 1)q + (n - 1)^2) + \\Theta(n)\n\\end{eqnarray}\n$$  $f(q) = 2q^2 - 2(n - 1)q + (n - 1)^2$ is monotonically decreasing at $[0, \\frac{n - 1}{2}]$, and monotonically increasing at $[\\frac{n - 1}{2}, n - 1]$. So $\\max_{0 \\leq q \\leq n - 1}(2q^2 - 2(n - 1)q + (n - 1)^2) = f(0) = f(n) = (n - 1)^2$. Thus:  $$\n\\begin{eqnarray}\nT(n)  \\geq  c(n - 1)^2 + \\Theta(n) \\\\ =  c(n - 1)^2 + dn \\\\ =  cn^2 + (d - 2c)n + c \\\\  cn^2\n\\end{eqnarray}\n$$  where the last step holds as long as $ d \\geq 2c$.  So $T(n) = \\Omega(n^2)$", 
            "title": "7.4-1"
        }, 
        {
            "location": "/7-Quicksort/7.4-Analysis-of-quicksort/#74-2", 
            "text": "The recurrence for the best-case is $T(n) = 2T(\\frac{n}{2}) + \\Theta(n)$. Let's solve it with the master method. We have $a = 2, b = 2, f(n) = \\Theta(n)$, and thus we have that $n^{\\log_b{a}} = n^{\\log_2{2}} = n = \\Theta(n)$. Since $f(n) = \\Theta(n)$, we can apply case 2 of the master theorem and conclude that the solution is $T(n) = \\Theta(n\\lg{n})$. Thus, $T(n) = \\Omega(n\\lg{n})$.", 
            "title": "7.4-2"
        }, 
        {
            "location": "/7-Quicksort/7.4-Analysis-of-quicksort/#74-3", 
            "text": "We've already knew it in the first exercise.", 
            "title": "7.4-3"
        }, 
        {
            "location": "/7-Quicksort/7.4-Analysis-of-quicksort/#74-4", 
            "text": "We have:  $$\n\\begin{eqnarray}\nE[X]  =  \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n \\frac{2}{j - i + 1} \\\\ =  \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{2}{k + 1} \\\\ \\geq  \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{2}{2k} \\\\ =  \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{1}{k} \\\\ =  \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{i} \\frac{1}{k} \\\\ =  \\sum_{i = 1}^{n - 1}(\\ln{i} + O(1))   \\text{(A.7)} \\\\ =  \\ln{((n - 1)!)} + (n - 1)O(1) \\\\ =  \\Omega(n\\lg{n}) + O(n)   \\text{(Exercise 3.2-3)} \\\\ =  \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$", 
            "title": "7.4-4"
        }, 
        {
            "location": "/7-Quicksort/7.4-Analysis-of-quicksort/#74-5", 
            "text": "Quicksort splits the array into two parts, let's say $T(n) = T(\\alpha{n}) + T((1 - \\alpha)n) + \\Theta(n)$. In exercise 7.2-5, we know when $0 \\leq \\alpha \\leq \\frac{1}{2}$, the minimum depth of a leaf is $\\log_{\\alpha}{\\frac{1}{n}}$ and the maximum depth is $\\log_{1 - \\alpha}{\\frac{1}{n}}$, when $\\frac{1}{2}   \\alpha   1$, it's opposite. To make it simple, let the recursion tree depth be $\\log_{\\beta}n$, since the new algorithm terminates when a subarray contains fewer than k elements, so we have $\\frac{n}{\\beta^{i}} = k$ and $i = \\log_{\\beta}{\\frac{n}{k}}$, so the depth is $O(\\lg{\\frac{n}{k}})$. Since each recursion tree level costs $O(n)$, the running time of quicksort is $O(n\\lg{\\frac{n}{k}})$.  The last step is sorting the array by insertion sort. Let's assume n is exactly divisible by k, thus, after quicksort, the $\\frac{n}{k}$ subarrays (each with length $\\frac{n}{k}$) is sorted, but it's not sorted within the subarray. It requires $O(k^2)$ to sort each subarray, so it requires $\\frac{n}{k}k^2 = nk$ to sort entir array.  So the running time is $O(nk + n\\lg{\\frac{n}{k}})$.  In order to beat quicksort, we need to find a k such that $O(nk + n\\lg{\\frac{n}{k}})   O(n\\lg{n})$, let's compare $nk + n\\lg{\\frac{n}{k}}$ and $n\\lg{n}$.  $$\n\\begin{eqnarray}\nnk + n\\lg{\\frac{n}{k}} - n\\lg{n}  =  nk + n\\lg{\\frac{1}{k}} \\\\ =  nk - n\\lg{k} \\\\ =  n(k - \\lg{k})\n\\end{eqnarray}\n$$  But k grows faster than $\\lg{k}$, so we cannot pick a good k here. Thus we can multiply a constant c to $n\\lg{n}$. Thus:  $$\n\\begin{eqnarray}\nnk + n\\lg{\\frac{n}{k}} - cn\\lg{n}  =  nk + n\\lg{n} - n\\lg{k} - cn\\lg{n} \\\\ =  n(\\lg{2^k} + \\lg{n} - \\lg{k} - \\lg{n^c}) \\\\ =  n\\lg{\\frac{2^k}{kn^{c - 1}}}\n\\end{eqnarray}\n$$  So we only need to find a k that satifies $\\frac{2^k}{kn^{c - 1}}   1$, which is possible.  In practice, we need to do some tests to find a proper k.", 
            "title": "7.4-5"
        }, 
        {
            "location": "/7-Quicksort/7.4-Analysis-of-quicksort/#74-6", 
            "text": "", 
            "title": "7.4-6"
        }, 
        {
            "location": "/7-Quicksort/Problems/", 
            "text": "Problems\n\n\n7-1\n\n\na\n\n\n\\documentclass{standalone}\n\n\\usepackage{colortbl}\n\\makeatletter\n\\newcolumntype{W}{!{\\smash{\\vrule\n\\@width 4\\arrayrulewidth\n\\@height\\dimexpr\\ht\\@arstrutbox+2pt\\relax\n\\@depth\\dimexpr\\dp\\@arstrutbox+2pt\\relax}}}\n\\makeatother\n\\definecolor{gray}{rgb}{.5,.5,.5}\n\\definecolor{lightgray}{rgb}{.8,.8,.8}\n\\begin{document}\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|cWc|c|c|c}\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{$p$}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{$ji$}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{$r$}\n\n\\multicolumn{1}{c}{}\\\\\n\n\n\\cellcolor{lightgray}6\n\n\\cellcolor{lightgray}2\n\n\\cellcolor{lightgray}9\n\n\\cellcolor{lightgray}5\n\n\\cellcolor{lightgray}12\n\n\\cellcolor{lightgray}8\n\n\\cellcolor{lightgray}7\n\n\\cellcolor{lightgray}4\n\n\\cellcolor{lightgray}11\n\n\\cellcolor{gray}19\n\n\\cellcolor{gray}13\n\n\\cellcolor{gray}21\n\n\\end{tabular}\n\\end{document}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb\n\n\nWe repeat \nj = j - 1\n from \nr + 1\n and repeat \ni = i + 1\n from \np - 1\n, so \ni\n moves from left to right and \nj\n moves from right to left, the while loop terminates when \ni \n= j\n, thus, it will not access an element of A outside the subarray \nA[p..r]\n. But will it keep increasing \ni\n such that \ni\n is bigger than \nr\n? No, it's not possible, because we choose \nA[p]\n as pivot. So in the first iteration, \ni\n stops at \np\n, and if \ni \n j\n, we exchange \nA[i]\n with \nA[j]\n, so the pivot is exchanged to right, thus it makes sure it stops keep increasing \ni\n when it meets the pivot in right.\n\n\nc\n\n\nSuppose the array is in increasing order, so it keeps increasing \nj\n until \nj = p\n because \nA[p] \n= x\n. So \nj\n could be \np\n. In the while loop, \nj\n will be \nr\n first, because there are at least two elements in the array, it will keep increasing \nj\n or exchange \nA[i]\n with \nA[j]\n, if it increase \nj\n, we get \nj \n r\n, if we exchange \nA[i]\n with \nA[j]\n, then it also increase \nj\n in the next iteration, either makes \nj \n r\n. So \np \n= j \n r\n.\n\n\nd\n\n\nj\n stopes decreasing when \nA[j] \n= x\n and \ni\n stops increasing when \nA[i] \n= x\n, after exchanging \nA[i]\n with \nA[j]\n, it makes sure every element of \nA[p..i]\n is less than or equal to every element of \nA[j..r]\n. When it terminates, we have \nj - i \n= 1\n, so every element of \nA[p..j]\n is less than or equal to every element of \nA[j + 1..r]\n.\n\n\ne\n\n\nQUICKSORT(A, p, r)\nif p \n r\n    q = HOARE-PARTITION(A, p, r)\n    QUICKSORT(A, p, q)\n    QUICKSORT(A, q + 1, r)\n\nHOARE-PARTITION(A, p, r)\nx = A[p]\ni = p - 1\nj = r + 1\nwhile True\n    repeat\n        j = j - 1\n    until A[j] \n= x\n    repeat\n        i = i + 1\n    until A[i] \n= x\n    if i \n j\n        exchange A[i] with A[j]\n    else return j\n\n\n\n\n7-2\n\n\na\n\n\nIf all elements values are equal, then randomize the array won't make a difference. In exercise 7.2-2 we know the running time is $\\Theta(n^2)$.\n\n\nb\n\n\nPARTITION'(A, p, r)\nx = A[r]\ni = p - 1\nt = p - 1\nfor j = p to r - 1\n    if A[j] \n x\n        t = t + 1\n        i = i + 1\n        exchange A[t] with A[i]\n\n        if t != j:\n            exchange A[j] with A[i]\n    else if A[j] == x\n        t = t + 1\n        exchange A[t] with A[j]\nexchange A[t + 1] with A[r]\nreturn i + 1, t + 1\n\n\n\n\nThe running time is obviously $\\Theta(r - p)$.\n\n\nc\n\n\nRANDOMIZED-QUICKSORT'(A, p, r)\nif p \n r\n    q, t = RANDOMIZED-PARTITION'(A, p r)\n    RANDOMIZED-QUICKSORT'(A, p, q - 1)\n    RANDOMIZED-QUICKSORT'(A, t + 1, r)\n\nRANDOMIZED-PARTITION'(A, p, r)\ni = RANDOM(p, r)\nexchange A[r] with A[i]\nreturn PARTITION'(A, p, r)\n\n\n\n\nQUICKSORT'(A, p, r)\nif p \n r\n    q, t = RANDOMIZED-PARTITION'(A, p r)\n    QUICKSORT'(A, p, q - 1)\n    QUICKSORT'(A, t + 1, r)\n\n\n\n\nd\n\n\nIf we use \nQUICKSORT'\n, then the worst-case becomes a best-case. If all elements of the array are the same, then the \nPARTITION'\n on the whole array splits the problem size to \n[p, p - 1]\n and \n[r + 1, r]\n, then the subproblem terminates. The running time is $\\Theta(n)$.\n\n\nSo we can avoid the assumption that all elements are distinct, because if an array with size n contains duplicate elements, the running time is less than the running time of the array with distinct elements.\n\n\n7-3\n\n\na\n\n\nWe choose the last element as pivot, but we randomize the array before that. It has the probability $\\frac{1}{n}$ that an element will be put to the last position in array. So any particular element is chosen as the pivot is $\\frac{1}{n}$. And $E[X_i] = \\frac{1}{n}$.\n\n\nb\n\n\nFirst let's check the expected running time when element i is chosen as pivot. The \nPARTITION\n method takes $\\Theta(n)$, and the problem size is splited to \n[p, i - 1]\n and \n[i + 1, r]\n, the running time of the two subproblems are $T(i - 1 - p + 1) = T(i - p)$ and $T(r - (i + 1) + 1) = T(r - i)$. Thus the running time is $E[X_i(T(i - p) + T(r - i) + \\Theta(n))]$, where p = 1, r = n.\n\n\nThus $E[T(n)] = E[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))]$.\n\n\nc\n\n\n$$\n\\begin{eqnarray}\nE[T(n)] \n=\n E[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))] \\\\\n\n=\n E[\\sum_{q = 1}^n \\frac{1}{n}(T(q - 1) + T(n - q) + \\Theta(n))] \\\\\n\n=\n \\frac{1}{n}E[T(0) + T(n - 1) + \\Theta(n) + T(1) + T(n - 2) + \\Theta(n) + \\ldots + T(n - 1) + T(0) + \\Theta(n)] \\\\\n\n=\n \\frac{1}{n}E[(T(0) + T(1) + \\ldots + T(n - 1)) + (T(n - 1) + T(n - 2) + \\ldots + T(0)) + n\\Theta(n)] \\\\\n\n=\n \\frac{1}{n}E[2\\sum_{q = 0}^{n - 1}T(q) + n\\Theta(n)] \\\\\n\n=\n \\frac{2}{n}E[\\sum_{q = 0}^{n - 1}T(q)] + \\Theta(n) \\\\\n\n=\n \\frac{2}{n}\\sum_{q = 0}^{n - 1}E[T(q)] + \\Theta(n) \\\\\n\n=\n \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\frac{2}{n}(E[T(0)] + E[T(1)]) + \\Theta(n) \\\\\n\n=\n \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\Theta(n)\n\\end{eqnarray}\n$$\n\n\nd\n\n\n$$\n\\begin{eqnarray}\n\\sum_{k = 2}^{n - 1} k\\lg{k} \n=\n \\sum_{k = 2}^{\\lceil \\frac{n}{2} \\rceil - 1} k\\lg{k} + \\sum_{k = \\lceil \\frac{n}{2} \\rceil}^{n - 1} k\\lg{k} \\\\\n\n \\sum_{k = 2}^{\\frac{n}{2} + 1 - 1} k\\lg{k} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{k} \\\\\n\n=\n \\sum_{k = 2}^{\\frac{n}{2}} k\\lg{k} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{k} \\\\\n\n\\leq\n \\sum_{k = 2}^{\\frac{n}{2}} k\\lg{\\frac{n}{2}} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{n} \\\\\n\n=\n (\\lg{n} - 1)\\frac{(2 + \\frac{n}{2})(\\frac{n}{2} - 2 + 1)}{2} + \\lg{n}\\frac{(\\frac{n}{2} + n - 1)(n - 1 - \\frac{n}{2} + 1)}{2} \\\\\n\n=\n (\\lg{n} - 1)(\\frac{n^2}{8} + \\frac{n}{4} - 1) + \\lg{n}(\\frac{3n^2}{8} - \\frac{n}{4}) \\\\\n\n=\n \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 -\\lg{n} - \\frac{n}{4} + 1 \\\\\n\n=\n \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 - \\lg3 - \\frac{3}{4} + 1 \\\\\n\n \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2\n\\end{eqnarray}\n$$\n\n\ne\n\n\nWe stary by assuming that $E[T(n)] \\leq an\\lg{n}$ for all positive m \n n, yielding $E[T(q)] \\leq aq\\lg{q}$. Substituting into the equation:\n\n\n$$\n\\begin{eqnarray}\nE[T(n)]\n=\n \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\Theta(n) \\\\\n\n\\leq\n \\frac{2}{n}\\sum_{q = 2}^{n - 1} aq\\lg{q} + \\Theta(n) \\\\\n\n=\n \\frac{2a}{n}\\sum_{q = 2}^{n - 1} q\\lg{q} + \\Theta(n) \\\\\n\n\\leq\n \\frac{2a}{n}(\\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2) + \\Theta(n) \\\\\n\n=\n an\\lg{n} - \\frac{a}{4}n + \\Theta(n) \\\\\n\n\\leq\n an\\lg{n} - \\frac{a}{4}n + cn \\\\\n\n=\n an\\lg{n} - (\\frac{a}{4} - c)n \\\\\n\n\\leq\n an\\lg{n}\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $\\frac{a}{4} \\geq c$.\n\n\nThus $E[T(n)] = \\Theta(n\\lg{n})$.\n\n\n7-4\n\n\na\n\n\nThe \nPARTITION\n method splits the array into two parts, then it sorts the left part first. At last it assigns the right part start index as the new value of p, so in the next iteration, it starts to sort the right part. Thus the whole array is sorted.\n\n\nb\n\n\nFor example, when all elements in the array are same. The \nPARTITION\n method returns n, so the size of subproblems are n - 1 and 0, and it continues to call \nTAIL-RECURSIVE-QUICKSORT\n with size n - 1, but each recursive call only reduces the problem size by 1, thus the stack depth becomes $\\Theta(n)$.\n\n\nc\n\n\nAfter we split the array into two parts, we call \nTAIL-RECURSIVE-QUICKSORT\n on the smaller part. The max size of smaller part is $\\frac{n}{2}$, thus the max stack depth is $\\Theta(\\lg{n})$.\n\n\nTAIL-RECURSIVE-QUICKSORT(A, p, r)\nwhile p \n r\n    // Partition and sort left subarray.\n    q = PARTITION(A, p, r)\n    if q - p \n r - q\n        TAIL-RECURSIVE-QUICKSORT(A, q + 1, r)\n        r = q - 1\n    else\n        TAIL-RECURSIVE-QUICKSORT(A, p, q - 1)\n        p = q + 1\n\n\n\n\n7-5\n\n\na\n\n\nIf an element at index i is chosen as median, we need to pick one element in \nA[1..i - 1]\n and one element in \nA[i + 1..n]\n, and the picking order is not unique, so there are $A_3^{3}$ permutations. Thus the probability is $p_i = \\frac{(i - 1)(n - i)A_3^{3}}{A_n{3}} = \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)}$.\n\n\nb\n\n\nThe $p_i$ of oridinary implementation is $\\frac{1}{n}$. So:\n\n\n$$\n\\begin{eqnarray}\n\\frac{p_{\\lfloor \\frac{n + 1}{2} \\rfloor}\\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)}}{\\frac{1}{n}} \n=\n \\frac{\\lim_{n \\to \\infty}\\frac{6(\\lfloor \\frac{n + 1}{2} \\rfloor - 1)(n - \\lfloor \\frac{n + 1}{2} \\rfloor)}{n(n - 1)(n - 2)}}{\\frac{1}{n}} \\\\\n\n=\n \\lim_{n \\to \\infty}\\frac{6(\\frac{n + 1}{2} - 1)(n - \\frac{n + 1}{2})}{(n - 1)(n - 2)} \\\\\n\n=\n \\lim_{n \\to \\infty}\\frac{6(\\frac{n - 1}{2})^2}{(n - 1)(n - 2)} \\\\\n\n=\n \\frac{3}{2}\n\\end{eqnarray}\n$$\n\n\nc\n\n\nThe likehood of getting a good split of the ordinary implementation is:\n\n\n$$\n\\begin{eqnarray}\n\\lim_{n \\to \\infty} \\sum_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{1}{n} \n=\n \\lim_{n \\to \\infty} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{1}{n} di \\quad (\\int \\frac{1}{n} di = \\frac{i}{n}) \\\\\n\n=\n \\lim_{n \\to \\infty} \\frac{\\frac{2n}{3}}{n} - \\frac{\\frac{n}{3}}{n} \\\\\n\n=\n \\lim_{n \\to \\infty} (\\frac{2}{3} - \\frac{1}{3}) \\\\\n\n=\n \\frac{1}{3}\n\\end{eqnarray}\n$$\n\n\nThe likehood of getting a good split of the new implementation is:\n\n\n$$\n\\begin{eqnarray}\n\\lim_{n \\to \\infty} \\sum_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)} \n=\n \\lim_{n \\to \\infty} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)} di \\\\\n\n=\n \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} (i - 1)(n - i) di \\quad (\\int (i - 1)(n - i) di = -\\frac{1}{3}i^3 + \\frac{n + 1}{2}i^2 - ni) \\\\\n\n=\n \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)} (-\\frac{1}{3}(\\frac{2n}{3})^3 + \\frac{n + 1}{2}(\\frac{2n}{3})^2 - n\\frac{2n}{3} - (-\\frac{1}{3}(\\frac{n}{3})^3 + \\frac{n + 1}{2}(\\frac{n}{3})^2 - n\\frac{n}{3})) \\\\\n\n=\n \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)}(-\\frac{7}{81}n^3 + \\frac{1}{6}n^3 - \\frac{1}{6}n^2) \\\\\n\n=\n 6(-\\frac{7}{81} + \\frac{1}{6}) \\\\\n\n=\n \\frac{13}{27}\n\\end{eqnarray}\n$$\n\n\nAnd the amount of improvement is $\\frac{\\frac{13}{27}}{\\frac{1}{3}} = \\frac{13}{9}$.\n\n\nd\n\n\nThe best-case of quicksort happens when the middle element is chosen as the pivot. And the running time is $\\Omega(n\\lg{n})$. Event the median-of-3 method picks the middle element as the pivot each time, the running time is still $\\Omega(n\\lg{n})$, thus it affects only the constant factor.\n\n\n7-6\n\n\na\n\n\nThe idea is similar like problem 7-2. Given an interval as pivot, we treat the intervals which overlap with the pivot are \"equal element values\". The \nPARTITION\n method returns two indices q and t, where $p \\leq q \\leq t \\leq r$, such that:\n\n\n\n\nall intervals of \nA[q..t]\n overlap with \nA[t]\n,\n\n\nthe right ending of each interval of \nA[p..q - 1]\n is less than the left ending of \nA[t]\n,\n\n\nthe left ending of each interval of \nA[t + 1..r]\n is greater than the right ending of \nA[t]\n.\n\n\n\n\nFUZZY-SORTING-OF-INTERVALS(A, p, r)\nif p \n r\n    q, t = RANDOMIZED-PARTITION(A, p r)\n    FUZZY-SORTING-OF-INTERVALS(A, p, q - 1)\n    FUZZY-SORTING-OF-INTERVALS(A, t + 1, r)\n\nRANDOMIZED-PARTITION(A, p, r)\ni = RANDOM(p, r)\nexchange A[r] with A[i]\nreturn PARTITION(A, p, r)\n\nPARTITION(A, p, r)\nx = A[r]\ni = p - 1\nt = p - 1\nfor j = p to r - 1\n    if A[j] is before x\n        t = t + 1\n        i = i + 1\n        exchange A[t] with A[i]\n\n        if t != j:\n            exchange A[j] with A[i]\n    else if A[j] overlaps with x\n        t = t + 1\n        exchange A[t] with A[j]\nexchange A[t + 1] with A[r]\nreturn i + 1, t + 1\n\n\n\n\nb\n\n\nIt's just a variation of problem 7-2, expected we use different comparision checks, so the expected running time is $\\Theta(n\\lg{n})$ in general.\n\n\nWhen all of the intervals overlap, we treat them like all of the intervals are equal, thus like the analysis in problem 7-2, the expected running time is $\\Theta(n)$.", 
            "title": "Problems"
        }, 
        {
            "location": "/7-Quicksort/Problems/#problems", 
            "text": "", 
            "title": "Problems"
        }, 
        {
            "location": "/7-Quicksort/Problems/#7-1", 
            "text": "a  \\documentclass{standalone}\n\n\\usepackage{colortbl}\n\\makeatletter\n\\newcolumntype{W}{!{\\smash{\\vrule\n\\@width 4\\arrayrulewidth\n\\@height\\dimexpr\\ht\\@arstrutbox+2pt\\relax\n\\@depth\\dimexpr\\dp\\@arstrutbox+2pt\\relax}}}\n\\makeatother\n\\definecolor{gray}{rgb}{.5,.5,.5}\n\\definecolor{lightgray}{rgb}{.8,.8,.8}\n\\begin{document}\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|cWc|c|c|c}\n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{$p$} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{$ji$} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{$r$} \n\\multicolumn{1}{c}{}\\\\ \n\\cellcolor{lightgray}6 \n\\cellcolor{lightgray}2 \n\\cellcolor{lightgray}9 \n\\cellcolor{lightgray}5 \n\\cellcolor{lightgray}12 \n\\cellcolor{lightgray}8 \n\\cellcolor{lightgray}7 \n\\cellcolor{lightgray}4 \n\\cellcolor{lightgray}11 \n\\cellcolor{gray}19 \n\\cellcolor{gray}13 \n\\cellcolor{gray}21 \n\\end{tabular}\n\\end{document}                  b  We repeat  j = j - 1  from  r + 1  and repeat  i = i + 1  from  p - 1 , so  i  moves from left to right and  j  moves from right to left, the while loop terminates when  i  = j , thus, it will not access an element of A outside the subarray  A[p..r] . But will it keep increasing  i  such that  i  is bigger than  r ? No, it's not possible, because we choose  A[p]  as pivot. So in the first iteration,  i  stops at  p , and if  i   j , we exchange  A[i]  with  A[j] , so the pivot is exchanged to right, thus it makes sure it stops keep increasing  i  when it meets the pivot in right.  c  Suppose the array is in increasing order, so it keeps increasing  j  until  j = p  because  A[p]  = x . So  j  could be  p . In the while loop,  j  will be  r  first, because there are at least two elements in the array, it will keep increasing  j  or exchange  A[i]  with  A[j] , if it increase  j , we get  j   r , if we exchange  A[i]  with  A[j] , then it also increase  j  in the next iteration, either makes  j   r . So  p  = j   r .  d  j  stopes decreasing when  A[j]  = x  and  i  stops increasing when  A[i]  = x , after exchanging  A[i]  with  A[j] , it makes sure every element of  A[p..i]  is less than or equal to every element of  A[j..r] . When it terminates, we have  j - i  = 1 , so every element of  A[p..j]  is less than or equal to every element of  A[j + 1..r] .  e  QUICKSORT(A, p, r)\nif p   r\n    q = HOARE-PARTITION(A, p, r)\n    QUICKSORT(A, p, q)\n    QUICKSORT(A, q + 1, r)\n\nHOARE-PARTITION(A, p, r)\nx = A[p]\ni = p - 1\nj = r + 1\nwhile True\n    repeat\n        j = j - 1\n    until A[j]  = x\n    repeat\n        i = i + 1\n    until A[i]  = x\n    if i   j\n        exchange A[i] with A[j]\n    else return j", 
            "title": "7-1"
        }, 
        {
            "location": "/7-Quicksort/Problems/#7-2", 
            "text": "a  If all elements values are equal, then randomize the array won't make a difference. In exercise 7.2-2 we know the running time is $\\Theta(n^2)$.  b  PARTITION'(A, p, r)\nx = A[r]\ni = p - 1\nt = p - 1\nfor j = p to r - 1\n    if A[j]   x\n        t = t + 1\n        i = i + 1\n        exchange A[t] with A[i]\n\n        if t != j:\n            exchange A[j] with A[i]\n    else if A[j] == x\n        t = t + 1\n        exchange A[t] with A[j]\nexchange A[t + 1] with A[r]\nreturn i + 1, t + 1  The running time is obviously $\\Theta(r - p)$.  c  RANDOMIZED-QUICKSORT'(A, p, r)\nif p   r\n    q, t = RANDOMIZED-PARTITION'(A, p r)\n    RANDOMIZED-QUICKSORT'(A, p, q - 1)\n    RANDOMIZED-QUICKSORT'(A, t + 1, r)\n\nRANDOMIZED-PARTITION'(A, p, r)\ni = RANDOM(p, r)\nexchange A[r] with A[i]\nreturn PARTITION'(A, p, r)  QUICKSORT'(A, p, r)\nif p   r\n    q, t = RANDOMIZED-PARTITION'(A, p r)\n    QUICKSORT'(A, p, q - 1)\n    QUICKSORT'(A, t + 1, r)  d  If we use  QUICKSORT' , then the worst-case becomes a best-case. If all elements of the array are the same, then the  PARTITION'  on the whole array splits the problem size to  [p, p - 1]  and  [r + 1, r] , then the subproblem terminates. The running time is $\\Theta(n)$.  So we can avoid the assumption that all elements are distinct, because if an array with size n contains duplicate elements, the running time is less than the running time of the array with distinct elements.", 
            "title": "7-2"
        }, 
        {
            "location": "/7-Quicksort/Problems/#7-3", 
            "text": "a  We choose the last element as pivot, but we randomize the array before that. It has the probability $\\frac{1}{n}$ that an element will be put to the last position in array. So any particular element is chosen as the pivot is $\\frac{1}{n}$. And $E[X_i] = \\frac{1}{n}$.  b  First let's check the expected running time when element i is chosen as pivot. The  PARTITION  method takes $\\Theta(n)$, and the problem size is splited to  [p, i - 1]  and  [i + 1, r] , the running time of the two subproblems are $T(i - 1 - p + 1) = T(i - p)$ and $T(r - (i + 1) + 1) = T(r - i)$. Thus the running time is $E[X_i(T(i - p) + T(r - i) + \\Theta(n))]$, where p = 1, r = n.  Thus $E[T(n)] = E[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))]$.  c  $$\n\\begin{eqnarray}\nE[T(n)]  =  E[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))] \\\\ =  E[\\sum_{q = 1}^n \\frac{1}{n}(T(q - 1) + T(n - q) + \\Theta(n))] \\\\ =  \\frac{1}{n}E[T(0) + T(n - 1) + \\Theta(n) + T(1) + T(n - 2) + \\Theta(n) + \\ldots + T(n - 1) + T(0) + \\Theta(n)] \\\\ =  \\frac{1}{n}E[(T(0) + T(1) + \\ldots + T(n - 1)) + (T(n - 1) + T(n - 2) + \\ldots + T(0)) + n\\Theta(n)] \\\\ =  \\frac{1}{n}E[2\\sum_{q = 0}^{n - 1}T(q) + n\\Theta(n)] \\\\ =  \\frac{2}{n}E[\\sum_{q = 0}^{n - 1}T(q)] + \\Theta(n) \\\\ =  \\frac{2}{n}\\sum_{q = 0}^{n - 1}E[T(q)] + \\Theta(n) \\\\ =  \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\frac{2}{n}(E[T(0)] + E[T(1)]) + \\Theta(n) \\\\ =  \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\Theta(n)\n\\end{eqnarray}\n$$  d  $$\n\\begin{eqnarray}\n\\sum_{k = 2}^{n - 1} k\\lg{k}  =  \\sum_{k = 2}^{\\lceil \\frac{n}{2} \\rceil - 1} k\\lg{k} + \\sum_{k = \\lceil \\frac{n}{2} \\rceil}^{n - 1} k\\lg{k} \\\\  \\sum_{k = 2}^{\\frac{n}{2} + 1 - 1} k\\lg{k} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{k} \\\\ =  \\sum_{k = 2}^{\\frac{n}{2}} k\\lg{k} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{k} \\\\ \\leq  \\sum_{k = 2}^{\\frac{n}{2}} k\\lg{\\frac{n}{2}} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{n} \\\\ =  (\\lg{n} - 1)\\frac{(2 + \\frac{n}{2})(\\frac{n}{2} - 2 + 1)}{2} + \\lg{n}\\frac{(\\frac{n}{2} + n - 1)(n - 1 - \\frac{n}{2} + 1)}{2} \\\\ =  (\\lg{n} - 1)(\\frac{n^2}{8} + \\frac{n}{4} - 1) + \\lg{n}(\\frac{3n^2}{8} - \\frac{n}{4}) \\\\ =  \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 -\\lg{n} - \\frac{n}{4} + 1 \\\\ =  \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 - \\lg3 - \\frac{3}{4} + 1 \\\\  \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2\n\\end{eqnarray}\n$$  e  We stary by assuming that $E[T(n)] \\leq an\\lg{n}$ for all positive m   n, yielding $E[T(q)] \\leq aq\\lg{q}$. Substituting into the equation:  $$\n\\begin{eqnarray}\nE[T(n)] =  \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\Theta(n) \\\\ \\leq  \\frac{2}{n}\\sum_{q = 2}^{n - 1} aq\\lg{q} + \\Theta(n) \\\\ =  \\frac{2a}{n}\\sum_{q = 2}^{n - 1} q\\lg{q} + \\Theta(n) \\\\ \\leq  \\frac{2a}{n}(\\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2) + \\Theta(n) \\\\ =  an\\lg{n} - \\frac{a}{4}n + \\Theta(n) \\\\ \\leq  an\\lg{n} - \\frac{a}{4}n + cn \\\\ =  an\\lg{n} - (\\frac{a}{4} - c)n \\\\ \\leq  an\\lg{n}\n\\end{eqnarray}\n$$  where the last step holds as long as $\\frac{a}{4} \\geq c$.  Thus $E[T(n)] = \\Theta(n\\lg{n})$.", 
            "title": "7-3"
        }, 
        {
            "location": "/7-Quicksort/Problems/#7-4", 
            "text": "a  The  PARTITION  method splits the array into two parts, then it sorts the left part first. At last it assigns the right part start index as the new value of p, so in the next iteration, it starts to sort the right part. Thus the whole array is sorted.  b  For example, when all elements in the array are same. The  PARTITION  method returns n, so the size of subproblems are n - 1 and 0, and it continues to call  TAIL-RECURSIVE-QUICKSORT  with size n - 1, but each recursive call only reduces the problem size by 1, thus the stack depth becomes $\\Theta(n)$.  c  After we split the array into two parts, we call  TAIL-RECURSIVE-QUICKSORT  on the smaller part. The max size of smaller part is $\\frac{n}{2}$, thus the max stack depth is $\\Theta(\\lg{n})$.  TAIL-RECURSIVE-QUICKSORT(A, p, r)\nwhile p   r\n    // Partition and sort left subarray.\n    q = PARTITION(A, p, r)\n    if q - p   r - q\n        TAIL-RECURSIVE-QUICKSORT(A, q + 1, r)\n        r = q - 1\n    else\n        TAIL-RECURSIVE-QUICKSORT(A, p, q - 1)\n        p = q + 1", 
            "title": "7-4"
        }, 
        {
            "location": "/7-Quicksort/Problems/#7-5", 
            "text": "a  If an element at index i is chosen as median, we need to pick one element in  A[1..i - 1]  and one element in  A[i + 1..n] , and the picking order is not unique, so there are $A_3^{3}$ permutations. Thus the probability is $p_i = \\frac{(i - 1)(n - i)A_3^{3}}{A_n{3}} = \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)}$.  b  The $p_i$ of oridinary implementation is $\\frac{1}{n}$. So:  $$\n\\begin{eqnarray}\n\\frac{p_{\\lfloor \\frac{n + 1}{2} \\rfloor}\\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)}}{\\frac{1}{n}}  =  \\frac{\\lim_{n \\to \\infty}\\frac{6(\\lfloor \\frac{n + 1}{2} \\rfloor - 1)(n - \\lfloor \\frac{n + 1}{2} \\rfloor)}{n(n - 1)(n - 2)}}{\\frac{1}{n}} \\\\ =  \\lim_{n \\to \\infty}\\frac{6(\\frac{n + 1}{2} - 1)(n - \\frac{n + 1}{2})}{(n - 1)(n - 2)} \\\\ =  \\lim_{n \\to \\infty}\\frac{6(\\frac{n - 1}{2})^2}{(n - 1)(n - 2)} \\\\ =  \\frac{3}{2}\n\\end{eqnarray}\n$$  c  The likehood of getting a good split of the ordinary implementation is:  $$\n\\begin{eqnarray}\n\\lim_{n \\to \\infty} \\sum_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{1}{n}  =  \\lim_{n \\to \\infty} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{1}{n} di \\quad (\\int \\frac{1}{n} di = \\frac{i}{n}) \\\\ =  \\lim_{n \\to \\infty} \\frac{\\frac{2n}{3}}{n} - \\frac{\\frac{n}{3}}{n} \\\\ =  \\lim_{n \\to \\infty} (\\frac{2}{3} - \\frac{1}{3}) \\\\ =  \\frac{1}{3}\n\\end{eqnarray}\n$$  The likehood of getting a good split of the new implementation is:  $$\n\\begin{eqnarray}\n\\lim_{n \\to \\infty} \\sum_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)}  =  \\lim_{n \\to \\infty} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)} di \\\\ =  \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} (i - 1)(n - i) di \\quad (\\int (i - 1)(n - i) di = -\\frac{1}{3}i^3 + \\frac{n + 1}{2}i^2 - ni) \\\\ =  \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)} (-\\frac{1}{3}(\\frac{2n}{3})^3 + \\frac{n + 1}{2}(\\frac{2n}{3})^2 - n\\frac{2n}{3} - (-\\frac{1}{3}(\\frac{n}{3})^3 + \\frac{n + 1}{2}(\\frac{n}{3})^2 - n\\frac{n}{3})) \\\\ =  \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)}(-\\frac{7}{81}n^3 + \\frac{1}{6}n^3 - \\frac{1}{6}n^2) \\\\ =  6(-\\frac{7}{81} + \\frac{1}{6}) \\\\ =  \\frac{13}{27}\n\\end{eqnarray}\n$$  And the amount of improvement is $\\frac{\\frac{13}{27}}{\\frac{1}{3}} = \\frac{13}{9}$.  d  The best-case of quicksort happens when the middle element is chosen as the pivot. And the running time is $\\Omega(n\\lg{n})$. Event the median-of-3 method picks the middle element as the pivot each time, the running time is still $\\Omega(n\\lg{n})$, thus it affects only the constant factor.", 
            "title": "7-5"
        }, 
        {
            "location": "/7-Quicksort/Problems/#7-6", 
            "text": "a  The idea is similar like problem 7-2. Given an interval as pivot, we treat the intervals which overlap with the pivot are \"equal element values\". The  PARTITION  method returns two indices q and t, where $p \\leq q \\leq t \\leq r$, such that:   all intervals of  A[q..t]  overlap with  A[t] ,  the right ending of each interval of  A[p..q - 1]  is less than the left ending of  A[t] ,  the left ending of each interval of  A[t + 1..r]  is greater than the right ending of  A[t] .   FUZZY-SORTING-OF-INTERVALS(A, p, r)\nif p   r\n    q, t = RANDOMIZED-PARTITION(A, p r)\n    FUZZY-SORTING-OF-INTERVALS(A, p, q - 1)\n    FUZZY-SORTING-OF-INTERVALS(A, t + 1, r)\n\nRANDOMIZED-PARTITION(A, p, r)\ni = RANDOM(p, r)\nexchange A[r] with A[i]\nreturn PARTITION(A, p, r)\n\nPARTITION(A, p, r)\nx = A[r]\ni = p - 1\nt = p - 1\nfor j = p to r - 1\n    if A[j] is before x\n        t = t + 1\n        i = i + 1\n        exchange A[t] with A[i]\n\n        if t != j:\n            exchange A[j] with A[i]\n    else if A[j] overlaps with x\n        t = t + 1\n        exchange A[t] with A[j]\nexchange A[t + 1] with A[r]\nreturn i + 1, t + 1  b  It's just a variation of problem 7-2, expected we use different comparision checks, so the expected running time is $\\Theta(n\\lg{n})$ in general.  When all of the intervals overlap, we treat them like all of the intervals are equal, thus like the analysis in problem 7-2, the expected running time is $\\Theta(n)$.", 
            "title": "7-6"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/", 
            "text": "8.1 Lower bounds for sorting\n\n\n8.1-1\n\n\nIt's $\\Theta(n)$, when the array is already sorted.\n\n\n8.1-2\n\n\nBy A.11 we have $\\int_0^n \\lg{k} dk \\leq \\sum_{k = 1}^n \\lg{k} \\leq \\int_1^{n + 1} \\lg{k} dk$.\n\n\nAnd $\\int \\lg{k} dk = k\\lg{k} - \\frac{1}{\\ln2}k + c = k\\lg{k} - k\\lg{e} + c$, but k could not be 0, so we cannot calculate the left part. Notice that when k = 1, $\\lg{k} = 0$, so $\\sum_{k = 1}^n \\lg{k} = \\sum_{k = 2}^n \\lg{k}$ for $n \\geq 2$. Thus we have $\\int_1^n \\lg{k} dk \\leq \\sum_{k = 2}^n \\lg{k} \\leq \\int_2^{n + 1} \\lg{k} dk$.\n\n\nFor the left part:\n\n\n$$\n\\begin{eqnarray}\n\\int_1^n \\lg{k} dk \n=\n n\\lg{n} - n\\lg{e} + c - (1\\lg1 - 1\\lg{e} + c) \\\\\n\n=\n n(\\lg{n} - \\lg{e}) + \\lg{e} \\\\\n\n\\geq\n n(\\lg{n} - \\frac{1}{2}\\lg{n}) + \\lg{e} \\quad (\\text{when } n \\geq e^2) \\\\\n\n=\n \\frac{1}{2}n\\lg{n} + \\lg{e} \\\\\n\n \\frac{1}{2}n\\lg{n} \\\\\n\n=\n \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nFor the right part:\n\n\n$$\n\\begin{eqnarray}\n\\int_2^{n + 1} \\lg{k} dk \n=\n (n + 1)\\lg{(n + 1)} - (n + 1)\\lg{e} + c - (2\\lg2 - 2\\lg{e} + c) \\\\\n\n=\n (n + 1)\\lg{\\frac{n + 1}{e}} + 2(\\lg{e} - 1) \\\\\n\n\\leq\n (n + 1)\\lg{\\frac{n + 1}{e}} + (n + 1)\\lg{e} \\quad (\\text{when } n \\geq 1 - \\frac{1}{\\lg{e}}) \\\\\n\n=\n (n + 1)\\lg{(n + 1)} \\\\\n\n 2n\\lg{n^2} \\quad (\\text{when } n \\geq 2) \\\\\n\n=\n 4n\\lg{n} \\\\\n\n=\nO(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nSo $\\lg{(n!)} = \\Theta(n\\lg{n})$.\n\n\n8.1-3\n\n\nIn the proof of theorem 8.1 we know that given m permutations, we have $m \\leq l$, where l is the number of reachable leaves, and since a binary tree of height h has no more than $2^h$ leaves, we have $m \\leq l \\leq 2^h$, so $h \\geq \\lg{m}$.\n\n\nWhen $m = \\frac{n!}{2}$, $\\lg{m} = \\lg{\\frac{n!}{2}} = \\lg{(n!)} - 1 = \\Omega(n\\lg{n})$.\n\n\nWhen $m = \\frac{n!}{n}$, $\\lg{m} = \\lg{\\frac{n!}{n}} = \\lg{(n!)} - \\lg{n} = \\Omega(n\\lg{n})$.\n\n\nWhen $m = \\frac{n!}{2^n}$, $\\lg{m} = \\lg{\\frac{n!}{2^n}} = \\lg{(n!)} - n = \\Omega(n\\lg{n})$.\n\n\nThus none of the above m is linear.\n\n\n8.1-4\n\n\nIn each subsequence, we need at least $\\Omega(k\\lg{k})$ comparisions, thus for $\\frac{n}{k}$ subsequences, it needs $\\frac{n}{k}\\Omega(k\\lg{k}) = \\Omega(n\\lg{k})$ comparisions.", 
            "title": "8.1 Lower bounds for sorting"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-lower-bounds-for-sorting", 
            "text": "", 
            "title": "8.1 Lower bounds for sorting"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-1", 
            "text": "It's $\\Theta(n)$, when the array is already sorted.", 
            "title": "8.1-1"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-2", 
            "text": "By A.11 we have $\\int_0^n \\lg{k} dk \\leq \\sum_{k = 1}^n \\lg{k} \\leq \\int_1^{n + 1} \\lg{k} dk$.  And $\\int \\lg{k} dk = k\\lg{k} - \\frac{1}{\\ln2}k + c = k\\lg{k} - k\\lg{e} + c$, but k could not be 0, so we cannot calculate the left part. Notice that when k = 1, $\\lg{k} = 0$, so $\\sum_{k = 1}^n \\lg{k} = \\sum_{k = 2}^n \\lg{k}$ for $n \\geq 2$. Thus we have $\\int_1^n \\lg{k} dk \\leq \\sum_{k = 2}^n \\lg{k} \\leq \\int_2^{n + 1} \\lg{k} dk$.  For the left part:  $$\n\\begin{eqnarray}\n\\int_1^n \\lg{k} dk  =  n\\lg{n} - n\\lg{e} + c - (1\\lg1 - 1\\lg{e} + c) \\\\ =  n(\\lg{n} - \\lg{e}) + \\lg{e} \\\\ \\geq  n(\\lg{n} - \\frac{1}{2}\\lg{n}) + \\lg{e} \\quad (\\text{when } n \\geq e^2) \\\\ =  \\frac{1}{2}n\\lg{n} + \\lg{e} \\\\  \\frac{1}{2}n\\lg{n} \\\\ =  \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$  For the right part:  $$\n\\begin{eqnarray}\n\\int_2^{n + 1} \\lg{k} dk  =  (n + 1)\\lg{(n + 1)} - (n + 1)\\lg{e} + c - (2\\lg2 - 2\\lg{e} + c) \\\\ =  (n + 1)\\lg{\\frac{n + 1}{e}} + 2(\\lg{e} - 1) \\\\ \\leq  (n + 1)\\lg{\\frac{n + 1}{e}} + (n + 1)\\lg{e} \\quad (\\text{when } n \\geq 1 - \\frac{1}{\\lg{e}}) \\\\ =  (n + 1)\\lg{(n + 1)} \\\\  2n\\lg{n^2} \\quad (\\text{when } n \\geq 2) \\\\ =  4n\\lg{n} \\\\ = O(n\\lg{n})\n\\end{eqnarray}\n$$  So $\\lg{(n!)} = \\Theta(n\\lg{n})$.", 
            "title": "8.1-2"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-3", 
            "text": "In the proof of theorem 8.1 we know that given m permutations, we have $m \\leq l$, where l is the number of reachable leaves, and since a binary tree of height h has no more than $2^h$ leaves, we have $m \\leq l \\leq 2^h$, so $h \\geq \\lg{m}$.  When $m = \\frac{n!}{2}$, $\\lg{m} = \\lg{\\frac{n!}{2}} = \\lg{(n!)} - 1 = \\Omega(n\\lg{n})$.  When $m = \\frac{n!}{n}$, $\\lg{m} = \\lg{\\frac{n!}{n}} = \\lg{(n!)} - \\lg{n} = \\Omega(n\\lg{n})$.  When $m = \\frac{n!}{2^n}$, $\\lg{m} = \\lg{\\frac{n!}{2^n}} = \\lg{(n!)} - n = \\Omega(n\\lg{n})$.  Thus none of the above m is linear.", 
            "title": "8.1-3"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-4", 
            "text": "In each subsequence, we need at least $\\Omega(k\\lg{k})$ comparisions, thus for $\\frac{n}{k}$ subsequences, it needs $\\frac{n}{k}\\Omega(k\\lg{k}) = \\Omega(n\\lg{k})$ comparisions.", 
            "title": "8.1-4"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.2-Counting-sort/", 
            "text": "8.2 Counting sort\n\n\n8.2-1\n\n\n\\documentclass{standalone}\n\n\\usepackage{colortbl}\n\\makeatletter\n\\newcolumntype{W}{!{\\smash{\\vrule\n\\@width 4\\arrayrulewidth\n\\@height\\dimexpr\\ht\\@arstrutbox+2pt\\relax\n\\@depth\\dimexpr\\dp\\@arstrutbox+2pt\\relax}}}\n\\makeatother\n\\definecolor{gray}{rgb}{.5,.5,.5}\n\\definecolor{lightgray}{rgb}{.8,.8,.8}\n\\begin{document}\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|}\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{1}\n\n\\multicolumn{1}{c}{2}\n\n\\multicolumn{1}{c}{3}\n\n\\multicolumn{1}{c}{4}\n\n\\multicolumn{1}{c}{5}\n\n\\multicolumn{1}{c}{6}\n\n\\multicolumn{1}{c}{7}\n\n\\multicolumn{1}{c}{8}\n\n\\multicolumn{1}{c}{9}\n\n\\multicolumn{1}{c}{10}\n\n\\multicolumn{1}{c}{11}\\\\\nB\n\n\\cellcolor{lightgray}0\n\n\\cellcolor{lightgray}0\n\n\\cellcolor{lightgray}1\n\n\\cellcolor{lightgray}1\n\n\\cellcolor{lightgray}2\n\n\\cellcolor{lightgray}2\n\n\\cellcolor{lightgray}3\n\n\\cellcolor{lightgray}3\n\n\\cellcolor{lightgray}4\n\n\\cellcolor{lightgray}6\n\n\\cellcolor{lightgray}6\\\\\n\\multicolumn{1}{c}{}\n\n\\multicolumn{1}{c}{0}\n\n\\multicolumn{1}{c}{1}\n\n\\multicolumn{1}{c}{2}\n\n\\multicolumn{1}{c}{3}\n\n\\multicolumn{1}{c}{4}\n\n\\multicolumn{1}{c}{5}\n\n\\multicolumn{1}{c}{6}\\\\\nC\n\n\\cellcolor{lightgray}0\n\n\\cellcolor{lightgray}2\n\n\\cellcolor{lightgray}4\n\n\\cellcolor{lightgray}6\n\n\\cellcolor{lightgray}8\n\n\\cellcolor{lightgray}9\n\n\\cellcolor{lightgray}9\n\\end{tabular}\n\\end{document}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.2-2\n\n\nSuppose for two indices i \n j, we have A[i] = A[j]. And we scan A from n to 1, so we met A[j] first, and then we put A[j] in B and subtract 1 from C[A[j]], so when we met A[i], it will be put before A[j], so the numbers with the same value appear in the output array in the same order as they do in the input array, \nCOUNTING-SORT\n is stable.\n\n\n8.2-3\n\n\nThe new algorithm works properly because it only affects the output of the elements that have same value, the relative order of distinct elements is not affected.\n\n\nBut the algorithm is not stable, because the numbers with the same value appear in the output array in the opposite order now.\n\n\n8.2-4\n\n\nWe can reuse the code from line 1 to 8 in \nCOUNTING-SORT\n. C[i] contains the number of elements less than or equal to i, so the number of elements fall into the range [a..b] is \nC[b] - C[a - 1]\n. We assume \nC[i] = 0\n when i is negative.\n\n\nPREPROCESS(A, k)\nlet C[0..k] be a new array\nfor i = 0 to k\n    C[i] = 0\nfor j = 1 to A.length\n    C[A[j]] = C[A[j]] + 1\nfor i = 1 to k\n    C[i] = C[i] + C[i - 1]\n\nQUERY(a, b)\nif a - 1 \n 0\n    return C[b]\nreturn C[b] - C[a - 1]\n\n\n\n\nAs discussed in the book, the running time to preprocess is $\\Theta(n + k)$.", 
            "title": "8.2 Counting sort"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.2-Counting-sort/#82-counting-sort", 
            "text": "", 
            "title": "8.2 Counting sort"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.2-Counting-sort/#82-1", 
            "text": "\\documentclass{standalone}\n\n\\usepackage{colortbl}\n\\makeatletter\n\\newcolumntype{W}{!{\\smash{\\vrule\n\\@width 4\\arrayrulewidth\n\\@height\\dimexpr\\ht\\@arstrutbox+2pt\\relax\n\\@depth\\dimexpr\\dp\\@arstrutbox+2pt\\relax}}}\n\\makeatother\n\\definecolor{gray}{rgb}{.5,.5,.5}\n\\definecolor{lightgray}{rgb}{.8,.8,.8}\n\\begin{document}\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|}\n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{1} \n\\multicolumn{1}{c}{2} \n\\multicolumn{1}{c}{3} \n\\multicolumn{1}{c}{4} \n\\multicolumn{1}{c}{5} \n\\multicolumn{1}{c}{6} \n\\multicolumn{1}{c}{7} \n\\multicolumn{1}{c}{8} \n\\multicolumn{1}{c}{9} \n\\multicolumn{1}{c}{10} \n\\multicolumn{1}{c}{11}\\\\\nB \n\\cellcolor{lightgray}0 \n\\cellcolor{lightgray}0 \n\\cellcolor{lightgray}1 \n\\cellcolor{lightgray}1 \n\\cellcolor{lightgray}2 \n\\cellcolor{lightgray}2 \n\\cellcolor{lightgray}3 \n\\cellcolor{lightgray}3 \n\\cellcolor{lightgray}4 \n\\cellcolor{lightgray}6 \n\\cellcolor{lightgray}6\\\\\n\\multicolumn{1}{c}{} \n\\multicolumn{1}{c}{0} \n\\multicolumn{1}{c}{1} \n\\multicolumn{1}{c}{2} \n\\multicolumn{1}{c}{3} \n\\multicolumn{1}{c}{4} \n\\multicolumn{1}{c}{5} \n\\multicolumn{1}{c}{6}\\\\\nC \n\\cellcolor{lightgray}0 \n\\cellcolor{lightgray}2 \n\\cellcolor{lightgray}4 \n\\cellcolor{lightgray}6 \n\\cellcolor{lightgray}8 \n\\cellcolor{lightgray}9 \n\\cellcolor{lightgray}9\n\\end{tabular}\n\\end{document}", 
            "title": "8.2-1"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.2-Counting-sort/#82-2", 
            "text": "Suppose for two indices i   j, we have A[i] = A[j]. And we scan A from n to 1, so we met A[j] first, and then we put A[j] in B and subtract 1 from C[A[j]], so when we met A[i], it will be put before A[j], so the numbers with the same value appear in the output array in the same order as they do in the input array,  COUNTING-SORT  is stable.", 
            "title": "8.2-2"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.2-Counting-sort/#82-3", 
            "text": "The new algorithm works properly because it only affects the output of the elements that have same value, the relative order of distinct elements is not affected.  But the algorithm is not stable, because the numbers with the same value appear in the output array in the opposite order now.", 
            "title": "8.2-3"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.2-Counting-sort/#82-4", 
            "text": "We can reuse the code from line 1 to 8 in  COUNTING-SORT . C[i] contains the number of elements less than or equal to i, so the number of elements fall into the range [a..b] is  C[b] - C[a - 1] . We assume  C[i] = 0  when i is negative.  PREPROCESS(A, k)\nlet C[0..k] be a new array\nfor i = 0 to k\n    C[i] = 0\nfor j = 1 to A.length\n    C[A[j]] = C[A[j]] + 1\nfor i = 1 to k\n    C[i] = C[i] + C[i - 1]\n\nQUERY(a, b)\nif a - 1   0\n    return C[b]\nreturn C[b] - C[a - 1]  As discussed in the book, the running time to preprocess is $\\Theta(n + k)$.", 
            "title": "8.2-4"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.3-Radix-sort/", 
            "text": "8.3 Radix sort\n\n\n8.3-1\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\n\n\n\n\nCOW\n\n\nSE\nA\n\n\nT\nA\nB\n\n\nB\nAR\n\n\n\n\n\n\nDOG\n\n\nTE\nA\n\n\nB\nA\nR\n\n\nB\nIG\n\n\n\n\n\n\nSEA\n\n\nMO\nB\n\n\nE\nA\nR\n\n\nB\nOX\n\n\n\n\n\n\nRUG\n\n\nTA\nB\n\n\nT\nA\nR\n\n\nC\nOW\n\n\n\n\n\n\nROW\n\n\nDO\nG\n\n\nS\nE\nA\n\n\nD\nIG\n\n\n\n\n\n\nMOB\n\n\nRU\nG\n\n\nT\nE\nA\n\n\nD\nOG\n\n\n\n\n\n\nBOX\n\n\nDI\nG\n\n\nD\nI\nG\n\n\nEAR\n\n\n\n\n\n\nTAB\n\n\nBI\nG\n\n\nB\nI\nG\n\n\nF\nOX\n\n\n\n\n\n\nBAR\n\n\nBA\nR\n\n\nM\nO\nB\n\n\nM\nOB\n\n\n\n\n\n\nEAR\n\n\nEA\nR\n\n\nD\nO\nG\n\n\nN\nOW\n\n\n\n\n\n\nTAR\n\n\nTA\nR\n\n\nC\nO\nW\n\n\nR\nOW\n\n\n\n\n\n\nDIG\n\n\nCO\nW\n\n\nR\nO\nW\n\n\nR\nUG\n\n\n\n\n\n\nBIG\n\n\nRO\nW\n\n\nN\nO\nW\n\n\nS\nEA\n\n\n\n\n\n\nTEA\n\n\nNO\nW\n\n\nB\nO\nX\n\n\nT\nAB\n\n\n\n\n\n\nNOW\n\n\nBO\nX\n\n\nF\nO\nX\n\n\nT\nAR\n\n\n\n\n\n\nFOX\n\n\nFO\nX\n\n\nR\nU\nG\n\n\nT\nEA\n\n\n\n\n\n\n\n\n8.3-2\n\n\nInsertion sort and merge sort are stable, heapsort and quicksort are not stable.\n\n\nWe can create an additional array to record the index of each element in the original array. When comparing two elements, if they are not equal, we compare them in the usualy way, if they are equal, we compare their indices.\n\n\nThe scheme doesn't affect the running time since the new comparision method still requires O(1), but it needs additional O(n) space.\n\n\n8.3-3\n\n\nWe use the following loop invariant:\n\n\nAt the start of each iteration of the for loop of 1-2 lines, the n-digit numbers are sorted by digit 1 to i - 1.\n\n\nInitialization\n: Prior to the first iteration of the loop, i = 1, digit 1 to i - 1 contains 0 digit, thus we can say the n-digit numbers are sorted by digit 1 to 0.\n\n\nMaintenance\n: After the ith iteration, the numbers are sorted in the ith digit, because we used a stable sorting algorithm, so if two numbers has same value on digit i, it won't change their relative order, and digit 1 to digit i - 1 is already sorted, so digit 1 to digit i is sorted.\n\n\nTermination\n: At termination, i = d + 1, numbers are sorted by digit 1 to digit d, so the array is sorted.\n\n\nWe need the assumption in the maintenance step.\n\n\n8.3-4\n\n\nWe can treat the numbers are in base n, each number contains 3 digits, thus the max number is $(n - 1)n^2 + (n - 1)n + (n - 1) = n^3 - 1$, so d = 3. There are n possible values in each digit, so k = n, so we can use the radix sort to sort the numbers. The running time is $d\\Theta(n + k) = 3\\Theta(n + n) = \\Theta(n)$, which is also O(n).\n\n\n8.3-5", 
            "title": "8.3 Radix sort"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.3-Radix-sort/#83-radix-sort", 
            "text": "", 
            "title": "8.3 Radix sort"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.3-Radix-sort/#83-1", 
            "text": "1  2  3  4      COW  SE A  T A B  B AR    DOG  TE A  B A R  B IG    SEA  MO B  E A R  B OX    RUG  TA B  T A R  C OW    ROW  DO G  S E A  D IG    MOB  RU G  T E A  D OG    BOX  DI G  D I G  EAR    TAB  BI G  B I G  F OX    BAR  BA R  M O B  M OB    EAR  EA R  D O G  N OW    TAR  TA R  C O W  R OW    DIG  CO W  R O W  R UG    BIG  RO W  N O W  S EA    TEA  NO W  B O X  T AB    NOW  BO X  F O X  T AR    FOX  FO X  R U G  T EA", 
            "title": "8.3-1"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.3-Radix-sort/#83-2", 
            "text": "Insertion sort and merge sort are stable, heapsort and quicksort are not stable.  We can create an additional array to record the index of each element in the original array. When comparing two elements, if they are not equal, we compare them in the usualy way, if they are equal, we compare their indices.  The scheme doesn't affect the running time since the new comparision method still requires O(1), but it needs additional O(n) space.", 
            "title": "8.3-2"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.3-Radix-sort/#83-3", 
            "text": "We use the following loop invariant:  At the start of each iteration of the for loop of 1-2 lines, the n-digit numbers are sorted by digit 1 to i - 1.  Initialization : Prior to the first iteration of the loop, i = 1, digit 1 to i - 1 contains 0 digit, thus we can say the n-digit numbers are sorted by digit 1 to 0.  Maintenance : After the ith iteration, the numbers are sorted in the ith digit, because we used a stable sorting algorithm, so if two numbers has same value on digit i, it won't change their relative order, and digit 1 to digit i - 1 is already sorted, so digit 1 to digit i is sorted.  Termination : At termination, i = d + 1, numbers are sorted by digit 1 to digit d, so the array is sorted.  We need the assumption in the maintenance step.", 
            "title": "8.3-3"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.3-Radix-sort/#83-4", 
            "text": "We can treat the numbers are in base n, each number contains 3 digits, thus the max number is $(n - 1)n^2 + (n - 1)n + (n - 1) = n^3 - 1$, so d = 3. There are n possible values in each digit, so k = n, so we can use the radix sort to sort the numbers. The running time is $d\\Theta(n + k) = 3\\Theta(n + n) = \\Theta(n)$, which is also O(n).", 
            "title": "8.3-4"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.3-Radix-sort/#83-5", 
            "text": "", 
            "title": "8.3-5"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.4-Bucket-sort/", 
            "text": "8.4 Bucket sort\n\n\n8.4-1\n\n\n\n\n8.4-2\n\n\nThe worst-case can happen when it takes too long to sort list \nB[i]\n. Because the worst-case running time of insertion sort is $\\Theta(n^2)$, if for a given \nB[i]\n, which happens to contain all elements, then the insertion sort takes $\\Theta(n^2)$, thus the worst-case running time becomes $\\Theta(n^2)$.\n\n\nWe can use another sort algorithm to sort \nB[i]\n, for example, the mergesort algorithm.\n\n\n8.4-3\n\n\nWe have:\n\n\n$$\n\\begin{eqnarray}\nX \n=\n I\\lbrace\\text{the number of heads in two flips of a fair coin}\\rbrace \\\\\n\n=\n \\begin{cases}\n      0 \\\\\n      1 \\\\\n      2 \\\\\n    \\end{cases}\n\\end{eqnarray}\n$$\n\n\nAnd $Pr\\lbrace \\text{the number of heads in two flips is } 0\\rbrace = \\frac{1}{4}$, $Pr\\lbrace \\text{the number of heads in two flips is } 1\\rbrace = \\frac{1}{2}$, $Pr\\lbrace\\text{the number of heads in two flips is } 2\\rbrace = \\frac{1}{4}$.\n\n\nSo $E[X] = 0 * \\frac{1}{4} + 1 * \\frac{1}{2} + 2 * \\frac{1}{4} = 1$. Thus $E[X^2] = 0^2 * \\frac{1}{4} + 1^2 * \\frac{1}{2} + 2^2 * \\frac{1}{4} = \\frac{3}{2}$, $(E[X])^2 = 1$.\n\n\n8.4-4\n\n\nSince the points are uniformly distributed, the probability of finding a point in any region of the circle is proportional to the area of that region. We can divide the unit circle into n parts, each part has area $\\frac{\\pi}{n}$.\n\n\nTo make it simple, we can create n - 1 concentric circles inside the unit circle. Let $a_1$ be the first circle, so the unit circle is $a_n$. The area of $a_1$ is $\\frac{\\pi}{n}$, so we have $r_{a_1} = \\frac{1}{\\sqrt{n}}$. Notice that $\\pi{r_{a_k}^2} - \\pi{r_{a_{k - 1}}^2} = \\frac{\\pi}{n}$, so:\n\n\n$$\n\\begin{eqnarray}\nr_{a_2}^2 - r_{a_1}^2 \n=\n \\frac{1}{n} \\\\\nr_{a_3}^2 - r_{a_2}^2 \n=\n \\frac{1}{n} \\\\\n\\ldots \\\\\nr_{a_k}^2 - r_{a_{k - 1}}^2 \n=\n \\frac{1}{n}\n\\end{eqnarray}\n$$\n\n\nSo we add the equations together and have $r_{a_k}^2 - r_{a_1}^2 = \\frac{k - 1}{n}$, thus $r_{a_k} = \\sqrt{\\frac{k}{n}}, 1 \\leq k \\leq n$.\n\n\nNow we can create the buckets $[0, \\sqrt{\\frac{1}{n}}), [\\sqrt{\\frac{1}{n}}, \\sqrt{\\frac{2}{n}}), \\ldots, [\\sqrt{\\frac{n - 1}{n}}, 1]$. In order to sort the n points by their distance $d_i = \\sqrt{x_i^2 + y_i^2}$, let array A contains the n points ($A[i] = d_i$), then create an empty array B, and insert \nA[i]\n into $B[\\lfloor nA[i] \\rfloor]$, at last, we sort \nB[i]\n and concatenate the lists \nB[i]\n.\n\n\n8.4-5\n\n\nIt's similar like bucket sort. We have $P(x) = \\text{Pr}\\lbrace X \\leq x \\rbrace$, so $P(1) = \\text{Pr}\\lbrace X \\leq 1 \\rbrace, P(2) = \\text{Pr}\\lbrace X \\leq 2 \\rbrace, \\ldots, P(n) = \\text{Pr}\\lbrace X \\leq n \\rbrace$. Thus $P(n) - P(n - 1) = \\text{Pr}\\lbrace n - 1 \n X \\leq n \\rbrace$. So we can create the buckets $[0, P(1)), [P(1), P(2)), \\ldots, [P(n - 1), P(n)]$.\n\n\nIn order to apply bucket sort, we let $A[i] = P(X_i)$, thus, we can use the bucket sort. At last, we have a list of sorted $P(X_i)$, which is also a list of sorted $X_i$.", 
            "title": "8.4 Bucket sort"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-bucket-sort", 
            "text": "", 
            "title": "8.4 Bucket sort"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-1", 
            "text": "", 
            "title": "8.4-1"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-2", 
            "text": "The worst-case can happen when it takes too long to sort list  B[i] . Because the worst-case running time of insertion sort is $\\Theta(n^2)$, if for a given  B[i] , which happens to contain all elements, then the insertion sort takes $\\Theta(n^2)$, thus the worst-case running time becomes $\\Theta(n^2)$.  We can use another sort algorithm to sort  B[i] , for example, the mergesort algorithm.", 
            "title": "8.4-2"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-3", 
            "text": "We have:  $$\n\\begin{eqnarray}\nX  =  I\\lbrace\\text{the number of heads in two flips of a fair coin}\\rbrace \\\\ =  \\begin{cases}\n      0 \\\\\n      1 \\\\\n      2 \\\\\n    \\end{cases}\n\\end{eqnarray}\n$$  And $Pr\\lbrace \\text{the number of heads in two flips is } 0\\rbrace = \\frac{1}{4}$, $Pr\\lbrace \\text{the number of heads in two flips is } 1\\rbrace = \\frac{1}{2}$, $Pr\\lbrace\\text{the number of heads in two flips is } 2\\rbrace = \\frac{1}{4}$.  So $E[X] = 0 * \\frac{1}{4} + 1 * \\frac{1}{2} + 2 * \\frac{1}{4} = 1$. Thus $E[X^2] = 0^2 * \\frac{1}{4} + 1^2 * \\frac{1}{2} + 2^2 * \\frac{1}{4} = \\frac{3}{2}$, $(E[X])^2 = 1$.", 
            "title": "8.4-3"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-4", 
            "text": "Since the points are uniformly distributed, the probability of finding a point in any region of the circle is proportional to the area of that region. We can divide the unit circle into n parts, each part has area $\\frac{\\pi}{n}$.  To make it simple, we can create n - 1 concentric circles inside the unit circle. Let $a_1$ be the first circle, so the unit circle is $a_n$. The area of $a_1$ is $\\frac{\\pi}{n}$, so we have $r_{a_1} = \\frac{1}{\\sqrt{n}}$. Notice that $\\pi{r_{a_k}^2} - \\pi{r_{a_{k - 1}}^2} = \\frac{\\pi}{n}$, so:  $$\n\\begin{eqnarray}\nr_{a_2}^2 - r_{a_1}^2  =  \\frac{1}{n} \\\\\nr_{a_3}^2 - r_{a_2}^2  =  \\frac{1}{n} \\\\\n\\ldots \\\\\nr_{a_k}^2 - r_{a_{k - 1}}^2  =  \\frac{1}{n}\n\\end{eqnarray}\n$$  So we add the equations together and have $r_{a_k}^2 - r_{a_1}^2 = \\frac{k - 1}{n}$, thus $r_{a_k} = \\sqrt{\\frac{k}{n}}, 1 \\leq k \\leq n$.  Now we can create the buckets $[0, \\sqrt{\\frac{1}{n}}), [\\sqrt{\\frac{1}{n}}, \\sqrt{\\frac{2}{n}}), \\ldots, [\\sqrt{\\frac{n - 1}{n}}, 1]$. In order to sort the n points by their distance $d_i = \\sqrt{x_i^2 + y_i^2}$, let array A contains the n points ($A[i] = d_i$), then create an empty array B, and insert  A[i]  into $B[\\lfloor nA[i] \\rfloor]$, at last, we sort  B[i]  and concatenate the lists  B[i] .", 
            "title": "8.4-4"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-5", 
            "text": "It's similar like bucket sort. We have $P(x) = \\text{Pr}\\lbrace X \\leq x \\rbrace$, so $P(1) = \\text{Pr}\\lbrace X \\leq 1 \\rbrace, P(2) = \\text{Pr}\\lbrace X \\leq 2 \\rbrace, \\ldots, P(n) = \\text{Pr}\\lbrace X \\leq n \\rbrace$. Thus $P(n) - P(n - 1) = \\text{Pr}\\lbrace n - 1   X \\leq n \\rbrace$. So we can create the buckets $[0, P(1)), [P(1), P(2)), \\ldots, [P(n - 1), P(n)]$.  In order to apply bucket sort, we let $A[i] = P(X_i)$, thus, we can use the bucket sort. At last, we have a list of sorted $P(X_i)$, which is also a list of sorted $X_i$.", 
            "title": "8.4-5"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/Problems/", 
            "text": "8-1\n\n\na\n\n\nThere are n! permutations of n inputs, and each permutation is equally likely, thus the probability of each permutation is $\\frac{1}{n}$. So there are n! leaves are labeled $\\frac{1}{n}$, if there are more than n! leaves, the rest are labeled 0.\n\n\nb\n\n\nThe external path length of a tree is the total length of all paths, from the root to the leaves. We have D(T) - D(LT) = leaves of LT, and D(T) - D(RT) = leaves of RT. So D(T) = D(LT) + D(RT) + leaves of LT + leaves of RT = D(LT) + D(RT) + k.\n\n\nc\n\n\nFrom the previous question, we have D(T) = D(LT) + D(RT) + k, and there are k - 1 permutations of LT and RT, so $d(k) = \\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace$.\n\n\nd\n\n\nWe have:\n\n\n$$\n\\begin{eqnarray}\nf'(i) \n=\n (i\\lg{i} + (k - i)\\lg{(k - i)})' \\\\\n\n=\n \\lg{i} + i\\frac{1}{i\\ln2} + (-1)\\lg{(k - i)} + (k - i)\\frac{1}{(k - i)\\ln2}(-1) \\\\\n\n=\n \\lg{i} + \\frac{1}{\\ln2} - \\lg{(k - i)} - \\frac{1}{\\ln2} \\\\\n\n=\n \\lg{\\frac{i}{k - i}}\n\\end{eqnarray}\n$$\n\n\nLet $\\lg{\\frac{i}{k - i}} \\geq 0$, we get $i \\geq \\frac{k}{2}$, let $\\lg{\\frac{i}{k - i}} \n 0$, we have $i \n \\frac{k}{2}$, so f(i) is monotonically decreasing at $[1, \\frac{k}{2})$ and monotonically increasing at $(\\frac{k}{2}, k - 1]$, and $f'(\\frac{k}{2}) = 0$, so f(i) is minimized at $i = \\frac{k}{2}$.\n\n\nSo $f(\\frac{k}{2}) = k(\\lg{k} - 1)$.\n\n\nWe assume that $d(k) = \\Omega(k\\lg{k})$, substituting it to $\\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace$ yielding:\n\n\n$$\n\\begin{eqnarray}\n\\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace \n\\geq\n \\min_{1 \\leq i \\leq k - 1}\\lbrace ci\\lg{i} + c(k - i)\\lg{(k - i)} + k \\rbrace \\\\\n\n=\n \\min_{1 \\leq i \\leq k - 1}\\lbrace c(i\\lg{i} + (k - i)\\lg{(k - i)}) + k \\rbrace \\\\\n\n\\geq\n c(\\frac{k}{2}\\lg{\\frac{k}{2}} + \\frac{k}{2}\\lg{\\frac{k}{2}}) + k \\\\\n\n=\n ck\\lg{k} + (1 - c)k \\\\\n\n\\geq\n ck\\lg{k} \\\\\n\n=\n \\Omega(k\\lg{k})\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $c \\leq 1$.\n\n\nSo $d(k) = \\Omega(k\\lg{k})$\n\n\ne\n\n\nWe know that $T_A$ has n! leaves, so k = n!, thus, $D(T_A) = d(n!) = \\Omega(n!\\lg{(n!)})$.\n\n\nSince each permutation has probability $\\frac{1}{n!}$, the average-case time to sort n elements is $\\frac{D(T_A)}{n!} = \\frac{\\Omega(n!\\lg{(n!)})}{n!} = \\Omega(\\lg{(n!)}) = \\Omega(n\\lg{n})$.\n\n\nf\n\n\nSuppose the randomized decision tree B has k permutations, so there exists a tree in the k permutations that has minimum comparisions, we can pick that tree as the deterministic decision tree A.\n\n\n8-2\n\n\na\n\n\nCounting sort.\n\n\nb\n\n\nSORT-IN-PLACE(A)\n\ni = 1\nj = A.length\n\nwhile i \n j\n    while A[i] == 0\n        i = i + 1\n\n    while A[j] == 1\n        j = j - 1\n\n    if i \n j\n        exchange A[i] with A[j]\n\n\n\n\nc\n\n\nInsertion sort.\n\n\nd\n\n\nThe first algorithm satisfies, and that's the algorithm used in the book.\n\n\nThe second algorithm is not stable, and the third algorithm doesn't run in O(n) time.\n\n\ne\n\n\nCOUNTING-SORT-IN-PLACE(A, k)\n\nlet C[0..k] be a new array\nfor i = 0 to k\n    C[i] = 0\nfor j = 1 to A.length\n    C[A[j]] = C[A[j]] + 1\nfor i = 1 to k\n    C[i] = C[i] + C[i - 1]\ni = 1\nwhile i \n= A.length\n    element = A[i]\n    position = C[element]\n    if i \n= position\n        i = i + 1\n    else if element != A[position]\n        exchange A[i] with A[position]\n        C[element] = C[element] - 1\n    else\n        C[element] = C[element] - 1\n\n\n\n\nIt's not stable.\n\n\n8-3\n\n\na\n\n\nSORTING-VARIABLE-LENGTH-INTEGERS(A)\n\nn = A.length\nlet B[1..n] be a new array\nfor i = 1 to n\n    let B[i] be a new array\nfor i = 1 to n\n    insert A[i] to B[j], where j is the length of A[i]\nfor i = 1 to n:\n    sort B[i] by RADIX-SORT\nconcatenate the lists B[1], B[2], ..., B[n] together in order\n\n\n\n\nLet $a_i$ be the number of integers which have i digits, so $\\sum_{i = 1}^n{a_ii} = n$. The running time of \nRADIX-SORT\n is $O(dn)$, here we have $d = i, n = a_i$ so the total running time to sort all \nB[i]\n is $\\sum_{i = 1}^nO(ia_i) = O(n)$.\n\n\nThe other for loops also takes O(n), so the running time of the algorithm is still O(n).\n\n\nb\n\n\nThe idea is similar like the previous question, but we don't group the strings by string length, because they should be in alphabetical order. We group them by the first character, then we sort the groups by the first character using \nCOUNTING-SORT\n. Then we do the same procedure recursively in each group, ignoring the first character.\n\n\nThere is an important property that ensures the running time is O(n). If two strings have different first letter, then they are only compared once, we don't need to compare other characters in the two strings. Given a string $a_i$ with length $l_i$, then it will be sorted by \nCOUNTING-SORT\n at most $l_i + 1$ times, the extra 1 time means $a_i$ is sorted as an empty string with other strings, then it will be grouped, and won't be compared any more in the next recursive procedure.\n\n\nSuppose there are m strings, so the running time is $O(\\sum_{i = 1}^m (l_i + 1)) = O(\\sum_{i = 1}^m l_i + m) = O(n + m) = O(n)$.\n\n\nSORTING-VARIABLE-LENGTH-STRINGS(A, start_letter_index)\n\n// To make it simple, we assume there are only 256 characters\nk = 255\nlet B[0..k] be a new array\n// Sort A by character index\nCOUNTING-SORT(A, start_letter_index)\nfor i = 1 to A.length\n    insert A[i] to B by A's character at start_letter_index\nfor i = 0 to k\n    SORTING-VARIABLE-LENGTH-STRINGS(B[i], start_letter_index + 1)\nconcatenate the lists B[0], B[1], ..., B[k] together in order\n\n\n\n\n8-4\n\n\na\n\n\nWATER-JUGS(RED-JUGS, BLUE-JUGS)\n\nlet WATER-JUG-PAIRS be a new array\n\nfor i = 1 to RED-JUGS.length\n    for j = 1 to BLUE-JUGS.length\n        if RED-JUGS[i] and BLUE-JUGS[j] have the same volume\n            put RED-JUGS[i] and BLUE-JUGS[j] into WATER-JUG-PAIRS\n\n\n\n\nb\n\n\nWe are mapping n red jugs to n blue jugs, so there are $n * (n - 1) * \\ldots * 1 = n!$ permutations, in the decision tree, each node has 3 children (\n, \n, =). Consider the decision tree of height h with l reachable leaves. Because each of the n! permutations of the input appears as some leaf, we have $n! \\leq l$, and the tree of height h has no more than $3^h$ leaves, we have $n! \\leq l \\leq 3^h$, so $h \\geq \\log_3{(n!)} = c\\lg{(n!)} = \\Omega(n\\lg{n})$.\n\n\nThus the lower bound for the number of comparisions is $\\Omega(n\\lg{n})$.\n\n\nc\n\n\nThe idea is similar like \nQUICK-SORT\n, first we randomly pick a jug in red jugs as pivot, then we partition the red jugs into two parts, red jugs that have smaller volume and red jugs that have larger volume. And we also partition the blue jugs into two parts. Then we recursively solve the problem with the new jugs.\n\n\nWATER-JUGS(RED-JUGS, BLUE-JUGS)\n\ni = RANDOM(1, RED-JUGS.length)\npivot = RED-JUGS[i]\nlet RED-JUGS-SMALLER-THAN-PIVOT be a new array\nlet RED-JUGS-LARGER-THAN-PIVOT be a new array\nlet BLUE-JUGS-SMALLER-THAN-PIVOT be a new array\nlet BLUE-JUGS-LARGER-THAN-PIVOT be a new array\nfor i = 1 to RED-JUGS.length\n    if RED-JUGS[i] \n pivot\n        insert RED-JUGS[i] into RED-JUGS-SMALLER-THAN-PIVOT\n    else if RED-JUGS[i] \n pivot\n        insert RED-JUGS[i] into RED-JUGS-LARGER-THAN-PIVOT\n    if BLUE-JUGS[i] \n pivot\n        insert RED-JUGS[i] into BLUE-JUGS-SMALLER-THAN-PIVOT\n    else if BLUE-JUGS[i] \n pivot\n        insert BLUE-JUGS[i] into BLUE-JUGS-LARGER-THAN-PIVOT\ndisplay(pivot, pivot)\nWATER-JUGS(RED-JUGS-SMALLER-THAN-PIVOT, BLUE-JUGS-SMALLER-THAN-PIVOT)\nWATER-JUGS(RED-JUGS-LARGER-THAN-PIVOT, BLUE-JUGS-LARGER-THAN-PIVOT)\n\n\n\n\nThe running time analysis is similar like \nQUICK-SORT\n, the worst-case number of comparisions is $\\Theta(n^2)$ when the smallest or the largest jug is chosen as the pivot in each procedure, the problem size is splited into n - 1 and 1.\n\n\n8-5\n\n\na\n\n\nIt means the array is sorted.\n\n\nb\n\n\n[1, 2, 3, 4, 5, 6, 7, 8, 10, 9]\n\n\n\n\nc\n\n\nIf for all $i = 1, 2, \\ldots, n - k$, $A[i] \\leq A[i + k]$, then:\n\n\n$$\n\\begin{eqnarray}\n\\sum_{j = i}^{i + k - 1} A[j] \n=\n A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\\n\n\\leq\n A[i + k] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\\n\n=\n \\sum_{j = i + 1}^{i + k} A[j]\n\\end{eqnarray}\n$$\n\n\nSo for all $i = 1, 2, \\ldots, n - k$, $\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\leq \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}$.\n\n\nIf for all $i = 1, 2, \\ldots, n - k$, $\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\leq \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}$, then:\n\n\n$$\n\\begin{eqnarray}\n\\sum_{j = i}^{i + k - 1} A[j] \n=\n A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\\n\n\\leq\n \\sum_{j = i + 1}^{i + k} A[j] \\\\\n\n=\n \\sum_{j = i + 1}^{i + k - 1} A[j] + A[i + k]\n\\end{eqnarray}\n$$\n\n\nSo $A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\leq \\sum_{j = i + 1}^{i + k - 1} A[j] + A[i + k]$, thus $A[i] \\leq A[i + k]$, so for all $i = 1, 2, \\ldots, n - k$, $A[i] \\leq A[i + k]$.\n\n\nd\n\n\nWe can use the property in the previous question to k-sort an n-element array. Thus, we need to split the array into k groups, each group contains $\\frac{n}{k}$ elements, then we sort the k groups separately.\n\n\nK-SORT-ARRAY(A, k)\n\nfor i = 1 to k\n    sort A[i], A[i + k], ...\n\n\n\n\nThe running time to sort one group is $O(\\frac{n}{k}\\lg{\\frac{n}{k}})$, so the running time of the algorithm is $kO(\\frac{n}{k}\\lg{\\frac{n}{k}}) = O(n\\lg{\\frac{n}{k}})$.\n\n\ne\n\n\nA k-sorted array means we have k sorted lists, so we can use the algorithm in exercise 6.5-9 to sort it in $O(n\\lg{k})$ time.\n\n\nf\n\n\nThe running time of k-sorting an n-element array is also $\\Omega(n\\lg{\\frac{n}{k}})$, when k is constant, we have:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\geq\n cn\\lg{\\frac{n}{k}} \\\\\n\n=\n cn\\lg{n} - cn\\lg{k} \\\\\n\n=\n \\frac{1}{2}cn\\lg{n} + cn(\\frac{1}{2}\\lg{n} - \\lg{k}) \\\\\n\n\\geq\n \\frac{1}{2}cn\\lg{n} \\\\\n\n=\n \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $n \\geq k^2$.\n\n\nSo $T(n) = \\Omega(n\\lg{n})$\n\n\n8-6\n\n\na\n\n\nFirst we select n numbers from the 2n numbers as the first sorted list, then the left numbers are belong to the other sorted list.\n\n\nThe number of possible ways is $C_{2n}^n$.\n\n\nb\n\n\nAssume the height of decision tree is h, and there are l reachable leaves, thus we have $C_{2n}^n \\leq l \\leq 2^h$, so:\n\n\n$$\n\\begin{eqnarray}\nh \n\\geq\n \\lg{(C_{2n}^n)} \\\\\n\n=\n \\lg{\\frac{(2n)!}{n!n!}} \\\\\n\n=\n \\lg{\\frac{\\sqrt{2\\pi2n}(\\frac{2n}{e})^{2n}(1 + O(\\frac{1}{2n}))}{(\\sqrt{2\\pi{n}}(\\frac{n}{e})^n(1 + O(\\frac{1}{n})))^2}} \n \\text{(Stirling's approximation)} \\\\\n\n=\n \\lg{\\frac{2^{2n}(1 + O(\\frac{1}{2n}))}{\\sqrt{\\pi{n}}(1 + O(\\frac{1}{n}))^2}} \\\\\n\n=\n \\lg{(2^{2n}(1 + O(\\frac{1}{2n})))} - \\lg{(\\sqrt{\\pi{n}}(1 + O(\\frac{1}{n}))^2)} \\\\\n\n=\n \\lg{2^{2n}} + \\lg{(1 + O(\\frac{1}{2n}))} - (\\lg{\\sqrt{\\pi{n}}} + \\lg{((1 + O(\\frac{1}{n}))^2)}) \\\\\n\n=\n 2n + \\lg{(1 + O(\\frac{1}{2n}))} - \\lg{\\sqrt{\\pi{n}}} - 2\\lg{(1 + O(\\frac{1}{n}))} \\\\\n\n=\n 2n - \\frac{1}{2}\\lg{(\\pi{n})} - \\lg{(1 + O(\\frac{1}{2n}))} \\\\\n\n=\n 2n - o(n)\n\\end{eqnarray}\n$$\n\n\nSo it must perform at least $2n - o(n)$ comparisions.\n\n\nc\n\n\nIf they are from different lists, we have to compare them to know which is larger or smaller.\n\n\nd\n\n\nThere are 2n - 1 consecutive elements in the sorted 2n elements, thus we need 2n - 1 comparisions.\n\n\n8-7\n\n\na\n\n\nWe know that \nA[p]\n is put into a wrong location, and \nA[q]\n is the value that algorithm X moves to the location into which \nA[p]\n should have gone. So \nA[q]\n is also put into a wrong location, but \nA[p]\n is the smallest value in A that algorithm X puts into the wrong location, so \nA[p] \n A[q]\n, thus, \nB[p] = 0\n and \nB[q] = 1\n.\n\n\nb", 
            "title": "Problems"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/Problems/#8-1", 
            "text": "a  There are n! permutations of n inputs, and each permutation is equally likely, thus the probability of each permutation is $\\frac{1}{n}$. So there are n! leaves are labeled $\\frac{1}{n}$, if there are more than n! leaves, the rest are labeled 0.  b  The external path length of a tree is the total length of all paths, from the root to the leaves. We have D(T) - D(LT) = leaves of LT, and D(T) - D(RT) = leaves of RT. So D(T) = D(LT) + D(RT) + leaves of LT + leaves of RT = D(LT) + D(RT) + k.  c  From the previous question, we have D(T) = D(LT) + D(RT) + k, and there are k - 1 permutations of LT and RT, so $d(k) = \\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace$.  d  We have:  $$\n\\begin{eqnarray}\nf'(i)  =  (i\\lg{i} + (k - i)\\lg{(k - i)})' \\\\ =  \\lg{i} + i\\frac{1}{i\\ln2} + (-1)\\lg{(k - i)} + (k - i)\\frac{1}{(k - i)\\ln2}(-1) \\\\ =  \\lg{i} + \\frac{1}{\\ln2} - \\lg{(k - i)} - \\frac{1}{\\ln2} \\\\ =  \\lg{\\frac{i}{k - i}}\n\\end{eqnarray}\n$$  Let $\\lg{\\frac{i}{k - i}} \\geq 0$, we get $i \\geq \\frac{k}{2}$, let $\\lg{\\frac{i}{k - i}}   0$, we have $i   \\frac{k}{2}$, so f(i) is monotonically decreasing at $[1, \\frac{k}{2})$ and monotonically increasing at $(\\frac{k}{2}, k - 1]$, and $f'(\\frac{k}{2}) = 0$, so f(i) is minimized at $i = \\frac{k}{2}$.  So $f(\\frac{k}{2}) = k(\\lg{k} - 1)$.  We assume that $d(k) = \\Omega(k\\lg{k})$, substituting it to $\\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace$ yielding:  $$\n\\begin{eqnarray}\n\\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace  \\geq  \\min_{1 \\leq i \\leq k - 1}\\lbrace ci\\lg{i} + c(k - i)\\lg{(k - i)} + k \\rbrace \\\\ =  \\min_{1 \\leq i \\leq k - 1}\\lbrace c(i\\lg{i} + (k - i)\\lg{(k - i)}) + k \\rbrace \\\\ \\geq  c(\\frac{k}{2}\\lg{\\frac{k}{2}} + \\frac{k}{2}\\lg{\\frac{k}{2}}) + k \\\\ =  ck\\lg{k} + (1 - c)k \\\\ \\geq  ck\\lg{k} \\\\ =  \\Omega(k\\lg{k})\n\\end{eqnarray}\n$$  where the last step holds as long as $c \\leq 1$.  So $d(k) = \\Omega(k\\lg{k})$  e  We know that $T_A$ has n! leaves, so k = n!, thus, $D(T_A) = d(n!) = \\Omega(n!\\lg{(n!)})$.  Since each permutation has probability $\\frac{1}{n!}$, the average-case time to sort n elements is $\\frac{D(T_A)}{n!} = \\frac{\\Omega(n!\\lg{(n!)})}{n!} = \\Omega(\\lg{(n!)}) = \\Omega(n\\lg{n})$.  f  Suppose the randomized decision tree B has k permutations, so there exists a tree in the k permutations that has minimum comparisions, we can pick that tree as the deterministic decision tree A.", 
            "title": "8-1"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/Problems/#8-2", 
            "text": "a  Counting sort.  b  SORT-IN-PLACE(A)\n\ni = 1\nj = A.length\n\nwhile i   j\n    while A[i] == 0\n        i = i + 1\n\n    while A[j] == 1\n        j = j - 1\n\n    if i   j\n        exchange A[i] with A[j]  c  Insertion sort.  d  The first algorithm satisfies, and that's the algorithm used in the book.  The second algorithm is not stable, and the third algorithm doesn't run in O(n) time.  e  COUNTING-SORT-IN-PLACE(A, k)\n\nlet C[0..k] be a new array\nfor i = 0 to k\n    C[i] = 0\nfor j = 1 to A.length\n    C[A[j]] = C[A[j]] + 1\nfor i = 1 to k\n    C[i] = C[i] + C[i - 1]\ni = 1\nwhile i  = A.length\n    element = A[i]\n    position = C[element]\n    if i  = position\n        i = i + 1\n    else if element != A[position]\n        exchange A[i] with A[position]\n        C[element] = C[element] - 1\n    else\n        C[element] = C[element] - 1  It's not stable.", 
            "title": "8-2"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/Problems/#8-3", 
            "text": "a  SORTING-VARIABLE-LENGTH-INTEGERS(A)\n\nn = A.length\nlet B[1..n] be a new array\nfor i = 1 to n\n    let B[i] be a new array\nfor i = 1 to n\n    insert A[i] to B[j], where j is the length of A[i]\nfor i = 1 to n:\n    sort B[i] by RADIX-SORT\nconcatenate the lists B[1], B[2], ..., B[n] together in order  Let $a_i$ be the number of integers which have i digits, so $\\sum_{i = 1}^n{a_ii} = n$. The running time of  RADIX-SORT  is $O(dn)$, here we have $d = i, n = a_i$ so the total running time to sort all  B[i]  is $\\sum_{i = 1}^nO(ia_i) = O(n)$.  The other for loops also takes O(n), so the running time of the algorithm is still O(n).  b  The idea is similar like the previous question, but we don't group the strings by string length, because they should be in alphabetical order. We group them by the first character, then we sort the groups by the first character using  COUNTING-SORT . Then we do the same procedure recursively in each group, ignoring the first character.  There is an important property that ensures the running time is O(n). If two strings have different first letter, then they are only compared once, we don't need to compare other characters in the two strings. Given a string $a_i$ with length $l_i$, then it will be sorted by  COUNTING-SORT  at most $l_i + 1$ times, the extra 1 time means $a_i$ is sorted as an empty string with other strings, then it will be grouped, and won't be compared any more in the next recursive procedure.  Suppose there are m strings, so the running time is $O(\\sum_{i = 1}^m (l_i + 1)) = O(\\sum_{i = 1}^m l_i + m) = O(n + m) = O(n)$.  SORTING-VARIABLE-LENGTH-STRINGS(A, start_letter_index)\n\n// To make it simple, we assume there are only 256 characters\nk = 255\nlet B[0..k] be a new array\n// Sort A by character index\nCOUNTING-SORT(A, start_letter_index)\nfor i = 1 to A.length\n    insert A[i] to B by A's character at start_letter_index\nfor i = 0 to k\n    SORTING-VARIABLE-LENGTH-STRINGS(B[i], start_letter_index + 1)\nconcatenate the lists B[0], B[1], ..., B[k] together in order", 
            "title": "8-3"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/Problems/#8-4", 
            "text": "a  WATER-JUGS(RED-JUGS, BLUE-JUGS)\n\nlet WATER-JUG-PAIRS be a new array\n\nfor i = 1 to RED-JUGS.length\n    for j = 1 to BLUE-JUGS.length\n        if RED-JUGS[i] and BLUE-JUGS[j] have the same volume\n            put RED-JUGS[i] and BLUE-JUGS[j] into WATER-JUG-PAIRS  b  We are mapping n red jugs to n blue jugs, so there are $n * (n - 1) * \\ldots * 1 = n!$ permutations, in the decision tree, each node has 3 children ( ,  , =). Consider the decision tree of height h with l reachable leaves. Because each of the n! permutations of the input appears as some leaf, we have $n! \\leq l$, and the tree of height h has no more than $3^h$ leaves, we have $n! \\leq l \\leq 3^h$, so $h \\geq \\log_3{(n!)} = c\\lg{(n!)} = \\Omega(n\\lg{n})$.  Thus the lower bound for the number of comparisions is $\\Omega(n\\lg{n})$.  c  The idea is similar like  QUICK-SORT , first we randomly pick a jug in red jugs as pivot, then we partition the red jugs into two parts, red jugs that have smaller volume and red jugs that have larger volume. And we also partition the blue jugs into two parts. Then we recursively solve the problem with the new jugs.  WATER-JUGS(RED-JUGS, BLUE-JUGS)\n\ni = RANDOM(1, RED-JUGS.length)\npivot = RED-JUGS[i]\nlet RED-JUGS-SMALLER-THAN-PIVOT be a new array\nlet RED-JUGS-LARGER-THAN-PIVOT be a new array\nlet BLUE-JUGS-SMALLER-THAN-PIVOT be a new array\nlet BLUE-JUGS-LARGER-THAN-PIVOT be a new array\nfor i = 1 to RED-JUGS.length\n    if RED-JUGS[i]   pivot\n        insert RED-JUGS[i] into RED-JUGS-SMALLER-THAN-PIVOT\n    else if RED-JUGS[i]   pivot\n        insert RED-JUGS[i] into RED-JUGS-LARGER-THAN-PIVOT\n    if BLUE-JUGS[i]   pivot\n        insert RED-JUGS[i] into BLUE-JUGS-SMALLER-THAN-PIVOT\n    else if BLUE-JUGS[i]   pivot\n        insert BLUE-JUGS[i] into BLUE-JUGS-LARGER-THAN-PIVOT\ndisplay(pivot, pivot)\nWATER-JUGS(RED-JUGS-SMALLER-THAN-PIVOT, BLUE-JUGS-SMALLER-THAN-PIVOT)\nWATER-JUGS(RED-JUGS-LARGER-THAN-PIVOT, BLUE-JUGS-LARGER-THAN-PIVOT)  The running time analysis is similar like  QUICK-SORT , the worst-case number of comparisions is $\\Theta(n^2)$ when the smallest or the largest jug is chosen as the pivot in each procedure, the problem size is splited into n - 1 and 1.", 
            "title": "8-4"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/Problems/#8-5", 
            "text": "a  It means the array is sorted.  b  [1, 2, 3, 4, 5, 6, 7, 8, 10, 9]  c  If for all $i = 1, 2, \\ldots, n - k$, $A[i] \\leq A[i + k]$, then:  $$\n\\begin{eqnarray}\n\\sum_{j = i}^{i + k - 1} A[j]  =  A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\ \\leq  A[i + k] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\ =  \\sum_{j = i + 1}^{i + k} A[j]\n\\end{eqnarray}\n$$  So for all $i = 1, 2, \\ldots, n - k$, $\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\leq \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}$.  If for all $i = 1, 2, \\ldots, n - k$, $\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\leq \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}$, then:  $$\n\\begin{eqnarray}\n\\sum_{j = i}^{i + k - 1} A[j]  =  A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\ \\leq  \\sum_{j = i + 1}^{i + k} A[j] \\\\ =  \\sum_{j = i + 1}^{i + k - 1} A[j] + A[i + k]\n\\end{eqnarray}\n$$  So $A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\leq \\sum_{j = i + 1}^{i + k - 1} A[j] + A[i + k]$, thus $A[i] \\leq A[i + k]$, so for all $i = 1, 2, \\ldots, n - k$, $A[i] \\leq A[i + k]$.  d  We can use the property in the previous question to k-sort an n-element array. Thus, we need to split the array into k groups, each group contains $\\frac{n}{k}$ elements, then we sort the k groups separately.  K-SORT-ARRAY(A, k)\n\nfor i = 1 to k\n    sort A[i], A[i + k], ...  The running time to sort one group is $O(\\frac{n}{k}\\lg{\\frac{n}{k}})$, so the running time of the algorithm is $kO(\\frac{n}{k}\\lg{\\frac{n}{k}}) = O(n\\lg{\\frac{n}{k}})$.  e  A k-sorted array means we have k sorted lists, so we can use the algorithm in exercise 6.5-9 to sort it in $O(n\\lg{k})$ time.  f  The running time of k-sorting an n-element array is also $\\Omega(n\\lg{\\frac{n}{k}})$, when k is constant, we have:  $$\n\\begin{eqnarray}\nT(n)  \\geq  cn\\lg{\\frac{n}{k}} \\\\ =  cn\\lg{n} - cn\\lg{k} \\\\ =  \\frac{1}{2}cn\\lg{n} + cn(\\frac{1}{2}\\lg{n} - \\lg{k}) \\\\ \\geq  \\frac{1}{2}cn\\lg{n} \\\\ =  \\Omega(n\\lg{n})\n\\end{eqnarray}\n$$  where the last step holds as long as $n \\geq k^2$.  So $T(n) = \\Omega(n\\lg{n})$", 
            "title": "8-5"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/Problems/#8-6", 
            "text": "a  First we select n numbers from the 2n numbers as the first sorted list, then the left numbers are belong to the other sorted list.  The number of possible ways is $C_{2n}^n$.  b  Assume the height of decision tree is h, and there are l reachable leaves, thus we have $C_{2n}^n \\leq l \\leq 2^h$, so:  $$\n\\begin{eqnarray}\nh  \\geq  \\lg{(C_{2n}^n)} \\\\ =  \\lg{\\frac{(2n)!}{n!n!}} \\\\ =  \\lg{\\frac{\\sqrt{2\\pi2n}(\\frac{2n}{e})^{2n}(1 + O(\\frac{1}{2n}))}{(\\sqrt{2\\pi{n}}(\\frac{n}{e})^n(1 + O(\\frac{1}{n})))^2}}   \\text{(Stirling's approximation)} \\\\ =  \\lg{\\frac{2^{2n}(1 + O(\\frac{1}{2n}))}{\\sqrt{\\pi{n}}(1 + O(\\frac{1}{n}))^2}} \\\\ =  \\lg{(2^{2n}(1 + O(\\frac{1}{2n})))} - \\lg{(\\sqrt{\\pi{n}}(1 + O(\\frac{1}{n}))^2)} \\\\ =  \\lg{2^{2n}} + \\lg{(1 + O(\\frac{1}{2n}))} - (\\lg{\\sqrt{\\pi{n}}} + \\lg{((1 + O(\\frac{1}{n}))^2)}) \\\\ =  2n + \\lg{(1 + O(\\frac{1}{2n}))} - \\lg{\\sqrt{\\pi{n}}} - 2\\lg{(1 + O(\\frac{1}{n}))} \\\\ =  2n - \\frac{1}{2}\\lg{(\\pi{n})} - \\lg{(1 + O(\\frac{1}{2n}))} \\\\ =  2n - o(n)\n\\end{eqnarray}\n$$  So it must perform at least $2n - o(n)$ comparisions.  c  If they are from different lists, we have to compare them to know which is larger or smaller.  d  There are 2n - 1 consecutive elements in the sorted 2n elements, thus we need 2n - 1 comparisions.", 
            "title": "8-6"
        }, 
        {
            "location": "/8-Sorting-in-Linear-Time/Problems/#8-7", 
            "text": "a  We know that  A[p]  is put into a wrong location, and  A[q]  is the value that algorithm X moves to the location into which  A[p]  should have gone. So  A[q]  is also put into a wrong location, but  A[p]  is the smallest value in A that algorithm X puts into the wrong location, so  A[p]   A[q] , thus,  B[p] = 0  and  B[q] = 1 .  b", 
            "title": "8-7"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.1-Minimum-and-maximum/", 
            "text": "9.1 Minimum and maximum\n\n\n9.1-1\n\n\nWe know that we need n - 1 comparisions to find the smallest element. Let's define an algorithm to find the smallest element:\n\n\nFIND-THE-SMALLEST(A, low, high)\n\nmid = (low + high) / 2\nleft-min = FIND-THE-SMALLEST(A, low, mid)\nright-min = FIND-THE-SMALLEST(A, mid + 1, high)\nreturn min(left-min, right-min)\n\n\n\n\nThe root node of the recursive tree has two children, one of them is the smallest element, let's assume the left child is the smallest element and let k denote the right child, then in order to find the second smallest element, we can traverse through the left branch, we let the smaller value of left and right children be the value of each node.\n\n\nWhen we are traversing, we compare the current second smallest element with the child with larger value (because the smaller one is the smallest element), if it's smaller than the second smallest element, we update the second smaller element.\n\n\nBecause we only compare the smaller element with k in each node, so we don't need to traverse all branches in left branch. Since the height of recursive tree is $\\lceil \\lg{n} \\rceil$, thus we need $\\lceil \\lg{n} \\rceil - 1$ comparisions. So we need $n - 1 + \\lceil \\lg{n} \\rceil - 1 = n + \\lceil \\lg{n} \\rceil - 2$ comparisions.\n\n\n9.1-2\n\n\nThe analysis is similar like the analysis in the book. When n is even, we need $\\frac{3n}{2} - 2$ comparisions, which is also $\\lceil \\frac{3n}{2} \\rceil - 2$. When n is odd, we need $3\\lfloor \\frac{n}{2} \\rfloor$ comparisions. And:\n\n\n$$\n\\begin{eqnarray}\n3\\lfloor \\frac{n}{2} \\rfloor \n=\n 3\\lceil \\frac{n - 2 + 1}{2} \\rceil \\\\\n\n=\n 3\\lceil \\frac{n - 1}{2} \\rceil \\\\\n\n=\n \\lceil \\frac{3(n - 1)}{2} \\rceil \n (n \\text{ is odd}) \\\\\n\n=\n \\lceil \\frac{3n}{2} - \\frac{3}{2} \\rceil \\\\\n\n=\n \\lceil (\\frac{3n}{2} + \\frac{1}{2}) - (\\frac{3}{2} + \\frac{1}{2}) \\rceil \\\\\n\n=\n \\lceil \\frac{3n}{2} + \\frac{1}{2} \\rceil - \\lceil \\frac{3}{2} + \\frac{1}{2} \\rceil \\\\\n\n=\n \\lceil \\frac{3n}{2} \\rceil - 2\n\\end{eqnarray}\n$$\n\n\nSo when n is odd, it also needs $\\lceil \\frac{3n}{2} \\rceil - 2$ comparisions.", 
            "title": "9.1 Minimum and maximum"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.1-Minimum-and-maximum/#91-minimum-and-maximum", 
            "text": "", 
            "title": "9.1 Minimum and maximum"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.1-Minimum-and-maximum/#91-1", 
            "text": "We know that we need n - 1 comparisions to find the smallest element. Let's define an algorithm to find the smallest element:  FIND-THE-SMALLEST(A, low, high)\n\nmid = (low + high) / 2\nleft-min = FIND-THE-SMALLEST(A, low, mid)\nright-min = FIND-THE-SMALLEST(A, mid + 1, high)\nreturn min(left-min, right-min)  The root node of the recursive tree has two children, one of them is the smallest element, let's assume the left child is the smallest element and let k denote the right child, then in order to find the second smallest element, we can traverse through the left branch, we let the smaller value of left and right children be the value of each node.  When we are traversing, we compare the current second smallest element with the child with larger value (because the smaller one is the smallest element), if it's smaller than the second smallest element, we update the second smaller element.  Because we only compare the smaller element with k in each node, so we don't need to traverse all branches in left branch. Since the height of recursive tree is $\\lceil \\lg{n} \\rceil$, thus we need $\\lceil \\lg{n} \\rceil - 1$ comparisions. So we need $n - 1 + \\lceil \\lg{n} \\rceil - 1 = n + \\lceil \\lg{n} \\rceil - 2$ comparisions.", 
            "title": "9.1-1"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.1-Minimum-and-maximum/#91-2", 
            "text": "The analysis is similar like the analysis in the book. When n is even, we need $\\frac{3n}{2} - 2$ comparisions, which is also $\\lceil \\frac{3n}{2} \\rceil - 2$. When n is odd, we need $3\\lfloor \\frac{n}{2} \\rfloor$ comparisions. And:  $$\n\\begin{eqnarray}\n3\\lfloor \\frac{n}{2} \\rfloor  =  3\\lceil \\frac{n - 2 + 1}{2} \\rceil \\\\ =  3\\lceil \\frac{n - 1}{2} \\rceil \\\\ =  \\lceil \\frac{3(n - 1)}{2} \\rceil   (n \\text{ is odd}) \\\\ =  \\lceil \\frac{3n}{2} - \\frac{3}{2} \\rceil \\\\ =  \\lceil (\\frac{3n}{2} + \\frac{1}{2}) - (\\frac{3}{2} + \\frac{1}{2}) \\rceil \\\\ =  \\lceil \\frac{3n}{2} + \\frac{1}{2} \\rceil - \\lceil \\frac{3}{2} + \\frac{1}{2} \\rceil \\\\ =  \\lceil \\frac{3n}{2} \\rceil - 2\n\\end{eqnarray}\n$$  So when n is odd, it also needs $\\lceil \\frac{3n}{2} \\rceil - 2$ comparisions.", 
            "title": "9.1-2"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/", 
            "text": "9.2 Selection in expected linear time\n\n\n9.2-1\n\n\nIf an array has 0 length, we have \nq - p = 0\n or \nr - q = 0\n. If \nq - p = 0\n, we have \nk = 1\n and line 8 will be executed, but we have \ni \n k\n, so \ni \n 1\n, which is not possible. If \nr - q = 0\n, we have \ni \n k\n, but \nk = q - p + 1 = r - p + 1\n, so \nk\n is the length of array, thus \ni\n could not be larger than \nk\n.\n\n\nSo the algorithm never makes a recursive call to a 0-length array.\n\n\n9.2-2\n\n\nThe value of $X_k$ doesn't affect the subproblem T(max(k - 1, n - k)). So they are independent.\n\n\n9.2-3\n\n\nRANDOMIZED-SELECT(A, p, r, i)\n\nwhile p \n= r\n    if p == r\n        return A[p]\n    q = RANDOMIZED-PARTITION(A, p, r)\n    k = q - p + 1\n    if i = k\n        return A[q]\n    elseif i \n k\n        r = q - 1\n    else\n        p = q + 1\n        i = i - k\n\n\n\n\n9.2-4\n\n\nThe worst-case happens when the partition procedure always select the largest element as pivot.\n\n\npivot = 9, subarray = { 3, 2, 1, 0, 7, 5, 4, 8, 6}\npivot = 8, subarray = { 3, 2, 1, 0, 7, 5, 4, 6 }\npivot = 7, subarray = { 3, 2, 1, 0, 6, 5, 4 }\npivot = 6, subarray = { 3, 2, 1, 0, 4, 5 }\npivot = 5, subarray = { 3, 2, 1, 0, 4}\npivot = 4, subarray = { 3, 2, 1, 0 }\npivot = 3, subarray = { 0, 2, 1 }\npivot = 2, subarray = { 0, 1 }\npivot = 1, subarray = { 0 }\nminimum = 0", 
            "title": "9.2 Selection in expected linear time"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-selection-in-expected-linear-time", 
            "text": "", 
            "title": "9.2 Selection in expected linear time"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-1", 
            "text": "If an array has 0 length, we have  q - p = 0  or  r - q = 0 . If  q - p = 0 , we have  k = 1  and line 8 will be executed, but we have  i   k , so  i   1 , which is not possible. If  r - q = 0 , we have  i   k , but  k = q - p + 1 = r - p + 1 , so  k  is the length of array, thus  i  could not be larger than  k .  So the algorithm never makes a recursive call to a 0-length array.", 
            "title": "9.2-1"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-2", 
            "text": "The value of $X_k$ doesn't affect the subproblem T(max(k - 1, n - k)). So they are independent.", 
            "title": "9.2-2"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-3", 
            "text": "RANDOMIZED-SELECT(A, p, r, i)\n\nwhile p  = r\n    if p == r\n        return A[p]\n    q = RANDOMIZED-PARTITION(A, p, r)\n    k = q - p + 1\n    if i = k\n        return A[q]\n    elseif i   k\n        r = q - 1\n    else\n        p = q + 1\n        i = i - k", 
            "title": "9.2-3"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-4", 
            "text": "The worst-case happens when the partition procedure always select the largest element as pivot.  pivot = 9, subarray = { 3, 2, 1, 0, 7, 5, 4, 8, 6}\npivot = 8, subarray = { 3, 2, 1, 0, 7, 5, 4, 6 }\npivot = 7, subarray = { 3, 2, 1, 0, 6, 5, 4 }\npivot = 6, subarray = { 3, 2, 1, 0, 4, 5 }\npivot = 5, subarray = { 3, 2, 1, 0, 4}\npivot = 4, subarray = { 3, 2, 1, 0 }\npivot = 3, subarray = { 0, 2, 1 }\npivot = 2, subarray = { 0, 1 }\npivot = 1, subarray = { 0 }\nminimum = 0", 
            "title": "9.2-4"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/", 
            "text": "9.3 Selection in worst-case linear time\n\n\n9.3-1\n\n\nLet's assume the input elements are divided into groups of k. Similar like the analysis in the book, at least half of the $\\lceil \\frac{n}{k} \\rceil$ groups contribute at least $\\lceil \\frac{k}{2} \\rceil$ elements that are greater than x, except for the one group that has fewer than k elements if k does not divide n exactly, and the one group containing x itself. Discounting these two groups, it follows that the number of elements greater than x is at least $\\lceil \\frac{k}{2} \\rceil(\\lceil \\frac{1}{2} \\lceil \\frac{n}{k} \\rceil \\rceil - 2) \\geq \\frac{k}{2}(\\frac{1}{2}\\frac{n}{k} - 2) = \\frac{n}{4} - k$.\n\n\nSimilarly, at least $\\frac{n}{4} - k$ elements are less than x. Thus, in the worst case, step 5 calls SELECT recursively on at most $\\frac{3n}{4} + k$ elements.\n\n\nSo when n is greater than some constant, we have $T(n) \\leq T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + O(n)$. We assume T(n) runs in linear time, substituting it into the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n\\leq\n T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + O(n) \\\\\n\n\\leq\n T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + an \\\\\n\n\\leq\n c\\frac{n}{k} + c(\\frac{3n}{4} + k) + an \\\\\n\n=\n cn + (\\frac{c}{k}n + an - \\frac{c}{4}n + ck) \\\\\n\n\\leq\n cn\n\\end{eqnarray}\n$$\n\n\nwhere the last step holds as long as $\\frac{c}{k}n + an - \\frac{c}{4}n + ck \\leq 0$. So we need to find some k such that there exists constants c and a such that $\\frac{c}{k}n + an - \\frac{c}{4}n + ck \\leq 0$.\n\n\nWe have $\\frac{c}{k}n + an - \\frac{c}{4}n + ck = c(\\frac{n}{k} - \\frac{n}{4} + k) + an \\leq 0$. Because both c and a are positive, so it could only be $\\frac{n}{k} - \\frac{n}{4} + k \\leq 0$. Let $f(k) = \\frac{n}{k} - \\frac{n}{4} + k$, so $f(4) = 4 \n 0, f(5) = -\\frac{n}{20} + 5 \\leq 0 \\text{ when } n \\geq 100$. So we can always find a $n_0$ such that $f(k) \\leq 0$ when $k \\geq 5$.\n\n\nThus the algorithm work in linear time if the input elements are divided into groups of 7, but doesn't run in linear time if they are divided into groups of 3.\n\n\n9.3-2\n\n\nWe already know that there are at least $\\frac{3n}{10} - 6$ elements are less (greater) than x, because $\\lceil \\frac{n}{4} \\rceil \n \\frac{n}{4} + 1$, let $\\frac{3n}{10} - 6 \\geq \\frac{n}{4} + 1$, we get $n \\geq 140$.\n\n\n9.3-3\n\n\nIn the previous question, we already know that if $n \\geq 140$, then at least $\\lceil \\frac{n}{4} \\rceil$ elements are less (greater) than x. So the partition procedure splits the problem size to $\\frac{n}{4}$ and $\\frac{3n}{4}$ in quicksort. Thus, $T(n) = T(\\frac{n}{4}) + T(\\frac{3n}{4}) + \\Theta(n)$. And the solution is $T(n) = \\Theta(n\\lg{n})$, which is solved in exercise 4.4-9.\n\n\n9.3-4\n\n\n9.3-5\n\n\nThe idea is similar.\n\n\nSELECT(A, p, r, i)\n\nif p == r\n    return A[p]\nmedian = BLACK-BOX-MEDIAN(A, p, r)\nq = PARTITION(A, p, r, median)\nk = q - p + 1\nif i == k\n    return A[q]\nelseif i \n k\n    return SELECT(A, p, q - 1, i)\nelse\n    return SELECT(A, q + 1, r, i - k)\n\n\n\n\nFirst we find the median of the array, and use that median as pivot to partition the array. Then we recursively call the procedure to get the ith element. Now let's analysis the running time. The \nBLAK-BOX-MEDIAN\n takes O(n), and partition method also takes O(n), then it at least reduces the problem size to $\\frac{n}{2}$, thus we have $T(n) = T(\\frac{n}{2}) + O(n)$.\n\n\nWe guess T(n) = O(n), substituting it to the recurrence yielding:\n\n\n$$\n\\begin{eqnarray}\nT(n) \n=\n T(\\frac{n}{2}) + O(n) \\\\\n\n\\leq\n c\\frac{n}{2} + dn \\\\\n\n=\n (\\frac{c}{2} + d)n \\\\\n\n=\n O(n)\n\\end{eqnarray}\n$$\n\n\nSo the algorithm solves the problem in linear time.\n\n\n9.3-6\n\n\nThe k quantiles of an n-element set are $A[\\lceil \\frac{n}{k} \\rceil], A[2\\lceil \\frac{n}{k} \\rceil], \\ldots, A[(k - 1)\\lceil \\frac{n}{k} \\rceil]$. And the idea is simple, first we divide the elements into two parts, one part contains $\\lceil \\frac{k}{2} \\rceil$ quantiles, and the other part contains $ k - \\lceil \\frac{k}{2} \\rceil$ quantiles. Then we recursively call the procedure on the two parts. In order to divide the elements into two parts, we use the \nSELECT\n procedure to get the $\\lceil \\frac{k}{2} \\rceil\\lceil \\frac{n}{k} \\rceil$th smallest element, which is the last element of the $\\lceil \\frac{k}{2} \\rceil$ quantiles.\n\n\nKTH-QUANTILES(A, p, r, k)\n\nif k == 1\n    return\nelse:\n    i = MATH.CEIL(k / 2) * SIZE-OF-ELEMENTS-IN-EACH-QUANTILE\n    index, quantile = SELECT(A, p, r, i)\n    OUTPUT quantile\n    KTH-QUANTILES(A, p, index, MATH.CEIL(k / 2))\n    KTH-QUANTILES(A, index + 1, r, k - MATH.CEIL(k / 2))\n\n\n\n\nNow let's analysis the running time. We have $T(n, k) = 2T(\\frac{n}{2}, \\frac{k}{2}) + O(n)$. If we draw a recursion tree, we can see that each level of the tree costs O(n) and the depth of tree is $\\lg{k}$, thus the running time is $O(n\\lg{k})$.\n\n\n9.3-7\n\n\nFirst, we find the median of the set, it costs O(n), then we create another array that contains the absolute distance between the median and each element. Then we use the \nSELECT\n procedure to select the kth smallest element p in the new array, at last, we compare each element in S with median, if the distance between element and median is not greater than p, then the element one of the k closest elements. Each step costs O(n), so the algorithm runs in O(n).\n\n\nK-CLOSEST(A, k)\n\ni = MATH.CEIL(A.length / 2)\nmedian = SELECT(A, i)\nlet B[1..n] be a new array\nfor j = 1 to n\n    B[j] = MATH.ABS(A[j] - median)\nkth = SELECT(B, k)\nlet C be a new array\nfor j = 1 to n\n    if MATH.ABS(A[j] - median) \n kth\n        add A[j] to C\nfor j = 1 to n\n    MATH.ABS(A[j] - median) == kth\n        add A[j] to C\n    if C.length \n= k\n        break\nreturn C\n\n\n\n\nBecause the elements in \nA\n are all distinct, so each element in \nB\n have at most 1 duplicate. The reason to scan \nA\n twice to get \nC\n is that \nA\n is not sorted, if we combine the last two loops together, it is possible we cannot get the correct k closest elements to the median.\n\n\n9.3-8\n\n\nWe can first get the medians of x and y, if the median of x is smaller than the median of y, then the median of all is in x's right part and y's left part, if the median of x is larger than the median of y, then the median of all is in x's left part and y's right part. So we can recursively solves the problem.\n\n\nMEDIAN-OF-TWO-SORTED-ARRAYS(X, px, rx, Y, py, ry)\n\nif rx - px == 1\n    if X[px] \n= Y[py]\n        return MATH.MIN(X[rx], Y[py])\n    else:\n        return MATH.MIN(X[px], Y[ry])\nmedian-index-of-x = MATH.FLOOR((px + rx) / 2)\nmedian-index-of-y = MATH.FLOOR((py + ry) / 2)\nif X[index] \n Y[median-index-of-y]\n    return MEDIAN-OF-TWO-SORTED-ARRAYS(X, index, rx, Y, py, median-index-of-y)\nelif X[index] \n Y[median-index-of-y]\n    return MEDIAN-OF-TWO-SORTED-ARRAYS(X, px, index, Y, median-index-of-y, ry)\nelse:\n    return X[index]\n\n\n\n\nEach recursive procedure reduces the problem size to $\\frac{n}{2}$, so we have $T(n) = T(\\frac{n}{2}) + \\Theta(1)$. Let's solve the recurrence by the master method. Here we have a = 1, b = 2, $f(n) = \\Theta(1)$. so $n^{\\log_b{a}} = 1$, thus $f(n) = \\Theta(n^{\\log_b{a}})$, so $T(n) = \\Theta(n^{\\log_b{a}}\\lg{n}) = \\Theta(\\lg{n})$.\n\n\n9.3-9\n\n\nSuppose the n wells have y-coordinates $y_1, y_2, \\ldots, y_n$, and we need to find $y_0$ such that the value $\\sum_{i = 1}^{n}|y_i - y_0|$ is minimum. This is the 1-dimensional case of \nGeometric median\n, and the median minimize the sum. \nExplanation\n.", 
            "title": "9.3 Selection in worst-case linear time"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-selection-in-worst-case-linear-time", 
            "text": "", 
            "title": "9.3 Selection in worst-case linear time"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-1", 
            "text": "Let's assume the input elements are divided into groups of k. Similar like the analysis in the book, at least half of the $\\lceil \\frac{n}{k} \\rceil$ groups contribute at least $\\lceil \\frac{k}{2} \\rceil$ elements that are greater than x, except for the one group that has fewer than k elements if k does not divide n exactly, and the one group containing x itself. Discounting these two groups, it follows that the number of elements greater than x is at least $\\lceil \\frac{k}{2} \\rceil(\\lceil \\frac{1}{2} \\lceil \\frac{n}{k} \\rceil \\rceil - 2) \\geq \\frac{k}{2}(\\frac{1}{2}\\frac{n}{k} - 2) = \\frac{n}{4} - k$.  Similarly, at least $\\frac{n}{4} - k$ elements are less than x. Thus, in the worst case, step 5 calls SELECT recursively on at most $\\frac{3n}{4} + k$ elements.  So when n is greater than some constant, we have $T(n) \\leq T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + O(n)$. We assume T(n) runs in linear time, substituting it into the recurrence yields:  $$\n\\begin{eqnarray}\nT(n)  \\leq  T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + O(n) \\\\ \\leq  T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + an \\\\ \\leq  c\\frac{n}{k} + c(\\frac{3n}{4} + k) + an \\\\ =  cn + (\\frac{c}{k}n + an - \\frac{c}{4}n + ck) \\\\ \\leq  cn\n\\end{eqnarray}\n$$  where the last step holds as long as $\\frac{c}{k}n + an - \\frac{c}{4}n + ck \\leq 0$. So we need to find some k such that there exists constants c and a such that $\\frac{c}{k}n + an - \\frac{c}{4}n + ck \\leq 0$.  We have $\\frac{c}{k}n + an - \\frac{c}{4}n + ck = c(\\frac{n}{k} - \\frac{n}{4} + k) + an \\leq 0$. Because both c and a are positive, so it could only be $\\frac{n}{k} - \\frac{n}{4} + k \\leq 0$. Let $f(k) = \\frac{n}{k} - \\frac{n}{4} + k$, so $f(4) = 4   0, f(5) = -\\frac{n}{20} + 5 \\leq 0 \\text{ when } n \\geq 100$. So we can always find a $n_0$ such that $f(k) \\leq 0$ when $k \\geq 5$.  Thus the algorithm work in linear time if the input elements are divided into groups of 7, but doesn't run in linear time if they are divided into groups of 3.", 
            "title": "9.3-1"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-2", 
            "text": "We already know that there are at least $\\frac{3n}{10} - 6$ elements are less (greater) than x, because $\\lceil \\frac{n}{4} \\rceil   \\frac{n}{4} + 1$, let $\\frac{3n}{10} - 6 \\geq \\frac{n}{4} + 1$, we get $n \\geq 140$.", 
            "title": "9.3-2"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-3", 
            "text": "In the previous question, we already know that if $n \\geq 140$, then at least $\\lceil \\frac{n}{4} \\rceil$ elements are less (greater) than x. So the partition procedure splits the problem size to $\\frac{n}{4}$ and $\\frac{3n}{4}$ in quicksort. Thus, $T(n) = T(\\frac{n}{4}) + T(\\frac{3n}{4}) + \\Theta(n)$. And the solution is $T(n) = \\Theta(n\\lg{n})$, which is solved in exercise 4.4-9.", 
            "title": "9.3-3"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-4", 
            "text": "", 
            "title": "9.3-4"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-5", 
            "text": "The idea is similar.  SELECT(A, p, r, i)\n\nif p == r\n    return A[p]\nmedian = BLACK-BOX-MEDIAN(A, p, r)\nq = PARTITION(A, p, r, median)\nk = q - p + 1\nif i == k\n    return A[q]\nelseif i   k\n    return SELECT(A, p, q - 1, i)\nelse\n    return SELECT(A, q + 1, r, i - k)  First we find the median of the array, and use that median as pivot to partition the array. Then we recursively call the procedure to get the ith element. Now let's analysis the running time. The  BLAK-BOX-MEDIAN  takes O(n), and partition method also takes O(n), then it at least reduces the problem size to $\\frac{n}{2}$, thus we have $T(n) = T(\\frac{n}{2}) + O(n)$.  We guess T(n) = O(n), substituting it to the recurrence yielding:  $$\n\\begin{eqnarray}\nT(n)  =  T(\\frac{n}{2}) + O(n) \\\\ \\leq  c\\frac{n}{2} + dn \\\\ =  (\\frac{c}{2} + d)n \\\\ =  O(n)\n\\end{eqnarray}\n$$  So the algorithm solves the problem in linear time.", 
            "title": "9.3-5"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-6", 
            "text": "The k quantiles of an n-element set are $A[\\lceil \\frac{n}{k} \\rceil], A[2\\lceil \\frac{n}{k} \\rceil], \\ldots, A[(k - 1)\\lceil \\frac{n}{k} \\rceil]$. And the idea is simple, first we divide the elements into two parts, one part contains $\\lceil \\frac{k}{2} \\rceil$ quantiles, and the other part contains $ k - \\lceil \\frac{k}{2} \\rceil$ quantiles. Then we recursively call the procedure on the two parts. In order to divide the elements into two parts, we use the  SELECT  procedure to get the $\\lceil \\frac{k}{2} \\rceil\\lceil \\frac{n}{k} \\rceil$th smallest element, which is the last element of the $\\lceil \\frac{k}{2} \\rceil$ quantiles.  KTH-QUANTILES(A, p, r, k)\n\nif k == 1\n    return\nelse:\n    i = MATH.CEIL(k / 2) * SIZE-OF-ELEMENTS-IN-EACH-QUANTILE\n    index, quantile = SELECT(A, p, r, i)\n    OUTPUT quantile\n    KTH-QUANTILES(A, p, index, MATH.CEIL(k / 2))\n    KTH-QUANTILES(A, index + 1, r, k - MATH.CEIL(k / 2))  Now let's analysis the running time. We have $T(n, k) = 2T(\\frac{n}{2}, \\frac{k}{2}) + O(n)$. If we draw a recursion tree, we can see that each level of the tree costs O(n) and the depth of tree is $\\lg{k}$, thus the running time is $O(n\\lg{k})$.", 
            "title": "9.3-6"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-7", 
            "text": "First, we find the median of the set, it costs O(n), then we create another array that contains the absolute distance between the median and each element. Then we use the  SELECT  procedure to select the kth smallest element p in the new array, at last, we compare each element in S with median, if the distance between element and median is not greater than p, then the element one of the k closest elements. Each step costs O(n), so the algorithm runs in O(n).  K-CLOSEST(A, k)\n\ni = MATH.CEIL(A.length / 2)\nmedian = SELECT(A, i)\nlet B[1..n] be a new array\nfor j = 1 to n\n    B[j] = MATH.ABS(A[j] - median)\nkth = SELECT(B, k)\nlet C be a new array\nfor j = 1 to n\n    if MATH.ABS(A[j] - median)   kth\n        add A[j] to C\nfor j = 1 to n\n    MATH.ABS(A[j] - median) == kth\n        add A[j] to C\n    if C.length  = k\n        break\nreturn C  Because the elements in  A  are all distinct, so each element in  B  have at most 1 duplicate. The reason to scan  A  twice to get  C  is that  A  is not sorted, if we combine the last two loops together, it is possible we cannot get the correct k closest elements to the median.", 
            "title": "9.3-7"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-8", 
            "text": "We can first get the medians of x and y, if the median of x is smaller than the median of y, then the median of all is in x's right part and y's left part, if the median of x is larger than the median of y, then the median of all is in x's left part and y's right part. So we can recursively solves the problem.  MEDIAN-OF-TWO-SORTED-ARRAYS(X, px, rx, Y, py, ry)\n\nif rx - px == 1\n    if X[px]  = Y[py]\n        return MATH.MIN(X[rx], Y[py])\n    else:\n        return MATH.MIN(X[px], Y[ry])\nmedian-index-of-x = MATH.FLOOR((px + rx) / 2)\nmedian-index-of-y = MATH.FLOOR((py + ry) / 2)\nif X[index]   Y[median-index-of-y]\n    return MEDIAN-OF-TWO-SORTED-ARRAYS(X, index, rx, Y, py, median-index-of-y)\nelif X[index]   Y[median-index-of-y]\n    return MEDIAN-OF-TWO-SORTED-ARRAYS(X, px, index, Y, median-index-of-y, ry)\nelse:\n    return X[index]  Each recursive procedure reduces the problem size to $\\frac{n}{2}$, so we have $T(n) = T(\\frac{n}{2}) + \\Theta(1)$. Let's solve the recurrence by the master method. Here we have a = 1, b = 2, $f(n) = \\Theta(1)$. so $n^{\\log_b{a}} = 1$, thus $f(n) = \\Theta(n^{\\log_b{a}})$, so $T(n) = \\Theta(n^{\\log_b{a}}\\lg{n}) = \\Theta(\\lg{n})$.", 
            "title": "9.3-8"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-9", 
            "text": "Suppose the n wells have y-coordinates $y_1, y_2, \\ldots, y_n$, and we need to find $y_0$ such that the value $\\sum_{i = 1}^{n}|y_i - y_0|$ is minimum. This is the 1-dimensional case of  Geometric median , and the median minimize the sum.  Explanation .", 
            "title": "9.3-9"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/Problems/", 
            "text": "Problems\n\n\n9-1\n\n\na\n\n\nSorting requires $O(n\\lg{n})$, plus $O(i)$ to list the i numbers, the total running time is $O(n\\lg{n}) + O(i)$.\n\n\nb\n\n\nIt requires $O(n)$ to build a max-priority queue, the \nEXTRACT-MAX\n costs $O(\\lg{n})$, thus the total running time is $O(n) + iO(\\lg{n})$.\n\n\nc\n\n\nFirst we need to find the \nn - i + 1\n smallest element, this requires O(n). And the array is partitioned around the \nn - i + 1\n smallest element. So we need to sort the \ni - 1\n numbers, it costs $O((i - 1)\\lg{(i - 1)})$, so the total running time is $O(n) + O((i - 1)\\lg{(i - 1)})$.\n\n\n9-2\n\n\na\n\n\nThe median of $x_1, x_2, \\ldots, x_n$ is $x_{\\lceil \\frac{n}{2} \\rceil}$. So $\\sum_{x_i \n x_k} w_i = \\frac{1}{n}(\\lceil \\frac{n}{2} \\rceil - 1) \n \\frac{1}{n}(\\frac{n}{2} + 1 - 1) = \\frac{1}{2}$. $\\sum_{x_i \n x_k} w_i = \\frac{1}{n}(n - \\lceil \\frac{n}{2} \\rceil) \\leq \\frac{1}{n}(n - \\frac{n}{2}) = \\frac{1}{2}$.\n\n\nb\n\n\nFirst we sort the elements, and then iterate the sorted elements and sum the corresponding weight, if the sum of weights is bigger than 0.5, then the current element is the weighted median.\n\n\nWEIGHTED-MEDIAN(A)\n\nSORT(A)\nweight-sum = 0\nfor i = 1 to n\n    weight-sum += get weight of A[i]\n    if weight-sum \n= 0.5:\n        return A[i]\n\n\n\n\nWe need $O(n\\lg{n})$ to sort all elements, and need O(n) to find the weighted median, the total running time is thus $O(n\\lg{n}) + O(n) = O(n\\lg{n})$.\n\n\nc\n\n\nFirst we get the median of all x, and sum the weights of elements whose value is less than median, and sum the weights of elements whose value is larger than median. If the sum of weights in left part is smaller than 0.5 and the sum of weights in right part is also not larger than 0.5, then the median is weighted median. Otherwise, we do the same procedure on the on the left part if its sum of weights is larger than 0.5, or the right part.\n\n\nWEIGHTED-MEDIAN(A)\n\nif n = 1\n    return A[1]\nif n = 2\n    if A[1].weight \n= A[2].weight\n        return A[1]\n    else\n        return A[2]\nelse\n    median = SELECT(A, MATH.CEIL(A / 2))\n    left-weights = 0\n    right-weights = 0\n    for i = 1 to MATH.CEIL(A / 2) - 1\n        left-weights = left-weights + A[i].weight\n    for i = MATH.CEIL(A / 2) + 1 to n\n        right-weights = right-weights + A[i].weight\n    if left-weights \n 0.5 and right-weights \n= 0.5\n        return median\n    elseif left-weights \n= 0.5\n        median.weight = median.weight + right-weights\n        A' = { x \n= median }\n        return WEIGHTED-MEDIAN(A')\n    elseif right-weights \n 0.5\n        median.weight = median.weight + left-weights\n        A' = { x \n= median }\n        return WEIGHTED-MEDIAN(A')\n\n\n\n\nThe recurrence of the algorithm is $T(n) = T(\\frac{n}{2}) + \\Theta(n)$. Let's solve it by the master method. Here we have a = 1, b = 2, $f(n) = \\Theta(n)$. So $\\log_b{a} = \\log_2{1} = 0$, so $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$ for $\\epsilon = 0.5$. And $af(\\frac{n}{b}) = f(\\frac{n}{2}) \\leq cf(n)$ for $c = \\frac{1}{2}$ and all sufficiently large n, thus $T(n) = \\Theta(f(n)) = \\Theta(n)$.\n\n\nd\n\n\nLet p denotes the weighted median, so $f(p) = \\sum_{i = 1}^n w_id(p - p_i) = \\sum_{i = 1}^n w_i|p - p_i|$. Since we need to prove the weighted median is the best solution, so for any other point x other than p we should have $f(x) \\geq f(p)$, or $f(x) - f(p) \\geq 0$.\n\n\nSo $f(x) - f(p) = \\sum_{i = 1}^n w_i|x - p_i| - \\sum_{i = 1}^n w_i|p - p_i| = \\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|)$.\n\n\nFirst let's check the situation when x \n p.\n\n\nWhen $p_i \\leq x \n p$, $|x - p_i| - |p - p_i| = x - p_i - (p - p_i) = x - p$, when $x \n p_i \n p$, $|x - p_i| - |p - p_i| = (p_i - x) - (p - p_i) \n 0 - (p - x) = x - p$. When $x \n p \\leq p_i$, $|x - p_i| - |p - p_i| = p_i - x - (p_i - p) = p - x$.\n\n\nThus:\n\n\n$$\n\\begin{eqnarray}\n\\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|) \n (x - p)\\sum_{p_i \n p}w_i + (p - x)\\sum_{p_i \\geq p}w_i \\\\\n\n=\n (p - x)(\\sum_{p_i \\geq p}w_i - \\sum_{p_i \n p}w_i)\n\\end{eqnarray}\n$$\n\n\nBecause p is weighted median, so $\\sum_{p_i \\geq p}w_i \n \\frac{1}{2}$, $\\sum_{p_i \n p}w_i \n \\frac{1}{2}$, so $(p - x)(\\sum_{p_i \\geq p}w_i - \\sum_{p_i \n p}w_i) \n 0$.\n\n\nNow let's check the situation when x \n p.\n\n\nWhen $p_i \\leq p \n x$, $|x - p_i| - |p - p_i| = x - p_i - (p - p_i) = x - p$, when $p \n p_i \n x$, $|x - p_i| - |p - p_i| = x - p_i - (p_i - p) \n 0 - (x - p) = p - x$, when $p \n x \\leq p_i$, $|x - p_i| - |p - p_i| = p_i - x - (p_i - p) = p - x$.\n\n\nThus:\n\n\n$$\n\\begin{eqnarray}\n\\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|) \n (p - x)\\sum_{p_i \n p}w_i + (x - p)\\sum_{p_i \\leq p}w_i \\\\\n\n=\n (x - p)(\\sum_{p_i \\leq p}w_i - \\sum_{p_i \n p}w_i)\n\\end{eqnarray}\n$$\n\n\nBecause $\\sum_{p_i \\leq p}w_i \n \\frac{1}{2}$ and $\\sum_{p_i \n p}w_i \n \\frac{1}{2}$, so $(x - p)(\\sum_{p_i \\leq p}w_i - \\sum_{p_i \n p}w_i) \n 0$.\n\n\nSo for any point x other than p we have $f(x) \n f(p)$, so the weighted median is the best solution.\n\n\ne\n\n\nWe need to find a point p(x, y) such that $\\sum_{i = 1}^n w_i(|x - x_i| + |y - y_i|)$ is minimum. Because $\\sum_{i = 1}^n w_i(|x - x_i| + |y - y_i|) = \\sum_{i = 1}^n w_i|x - x_i| + \\sum_{i = 1}^n w_i|y - y_i|$, the problem is actually 2 1-dimensional problems. Thus let x be the weighted median of all x coordinate values, and let y be the weighted median of all y coordinate values, so p(x, y) is the best solution.\n\n\n9-3\n\n\na\n\n\nIf $i \\geq \\frac{n}{2}$, then we use the \nSELECT\n algorithm, otherwise we group every two elements into pairs $(a_j, b_j)$ and make sure $a_j \\leq b_j$, if n is odd, we also let the last element be a pair, this step needs $\\lfloor \\frac{n}{2} \\rfloor$. So now we have $\\lceil \\frac{n}{2} \\rceil$ pairs. Then we recursively call the algorithm on $a_j$, so we can get the ith smallest element of all $a_j$, this step requires $U_i(\\lceil \\frac{n}{2} \\rceil)$. Notice that the partition method partition all $a_j$ into two parts, so the ith smallest element of all elements could only be among $a_1\\ldots{a_i}$ and $b_1\\ldots{b_i}$. Then we run the \nSELECT\n algorithm on the 2i elements to find the ith smallest element.\n\n\nSMALL-ORDER-STATISTICS(A, i)\nif i \n= n / 2\n    return SELECT(A, i)\nlet pairs be a new array\nfor i = 1 to n with step = 2\n    if i + 1 \n= n\n        if A[i] \n= A[i + 1]\n            insert [A[i], A[i + 1]] into pairs\n        else\n            insert [A[i + 1], A[i]] into pairs\n    else\n        insert [A[i]] into pairs\nSMALL-ORDER-STATISTICS(pairs, i) // Run algorithm on all aj\nreturn SELECT(pairs, i) // Run algorithm on a1 to ai plus b1 to bi\n\n\n\n\nb\n\n\nLet's solve by the substitution method. We start by assuming that $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i}))$ holds for all positive m \n n, in particular for $m = \\lceil \\frac{n}{2} \\rceil$, yielding $U_i(\\lceil \\frac{n}{2} \\rceil) = \\lceil \\frac{n}{2} \\rceil + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}))$. Substituting into the recurrence yields:\n\n\n$$\n\\begin{eqnarray}\nU_i(n) \n=\n \\lfloor \\frac{n}{2} \\rfloor + \\lceil \\frac{n}{2} \\rceil + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i})) + T(2i) \\\\\n\n=\n n + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i})) + O(T(2i)) \\\\\n\n=\n n + O(T(2i)(\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}) + 1)) \\\\\n\n=\n n + O(T(2i)(\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}) + \\lg2)) \\\\\n\n=\n n + O(T(2i)\\lg(\\frac{n}{i}))\n\\end{eqnarray}\n$$\n\n\nc\n\n\nIf i is a constant, and because $T(n) = O(n)$, so T(2i) = O(2i), so T(2i) is also a constant, and $O(\\lg{\\frac{n}{i}}) = O(\\lg{n})$. Thus $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i})) = n + O(\\lg{n})$.\n\n\nd\n\n\nIt's so obvious, we just replace i with $\\frac{n}{k}$ and yields $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i})) = n + O(T(2\\frac{n}{k})\\lg(\\frac{n}{\\frac{n}{k}})) = n + O(T(\\frac{2n}{k})\\lg{k})$.\n\n\n9-4\n\n\na\n\n\n$z_i$ and $z_j$ are compared if and only if the first element to be chosen as a pivot from $Z_{ijk}$ is either $z_i$ or $z_j$. And the range of $Z_{ijk}$ depends on k. So:\n\n\n$$\n\\begin{eqnarray}\nE[X_{ijk}] \n=\n\n    \\begin{cases}\n      \\frac{2}{j - k + 1} \n \\text{if } k \\leq i \n j \\\\\n      \\frac{2}{j - i + 1} \n \\text{if } i \n k \\leq j \\\\\n      \\frac{2}{k - i + 1} \n \\text{if } i \n j \n k\n    \\end{cases}\n\\end{eqnarray}\n$$\n\n\nb\n\n\n$$\n\\begin{eqnarray}\nE[X_k] \n=\n \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n E[X_{ijk}] \\\\\n\n=\n \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} E[X_{ijk}] + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} E[X_{ijk}] + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} E[X_{ijk}] \\\\\n\n=\n \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{2}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{2}{j - i + 1} + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} \\frac{2}{k - i + 1} \\\\\n\n=\n 2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} \\frac{1}{k - i + 1}) \\\\\n\n=\n 2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\\\\n\n=\n 2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} - \\sum_{j = k}^n \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\\\\n\n=\n 2(\\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} - \\sum_{j = k}^n \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1})\n\\end{eqnarray}\n$$\n\n\nc\n\n\nWe have $\\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} \\leq n$ (\nsource\n), and $\\sum_{k + 1}^n \\frac{j - k - 1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1} \n \\sum_{k + 1}^n 1 + \\sum_{i = 1}^{k - 2} 1 = n - k + k - 2 = n - 2 \n n$, so $E[X_k] \\leq 2(n + n) = 4n$.\n\n\nd\n\n\nSince $E[X_k] \\leq 4n$, thus $T(n) = O(n)$.", 
            "title": "Problems"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/Problems/#problems", 
            "text": "", 
            "title": "Problems"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/Problems/#9-1", 
            "text": "a  Sorting requires $O(n\\lg{n})$, plus $O(i)$ to list the i numbers, the total running time is $O(n\\lg{n}) + O(i)$.  b  It requires $O(n)$ to build a max-priority queue, the  EXTRACT-MAX  costs $O(\\lg{n})$, thus the total running time is $O(n) + iO(\\lg{n})$.  c  First we need to find the  n - i + 1  smallest element, this requires O(n). And the array is partitioned around the  n - i + 1  smallest element. So we need to sort the  i - 1  numbers, it costs $O((i - 1)\\lg{(i - 1)})$, so the total running time is $O(n) + O((i - 1)\\lg{(i - 1)})$.", 
            "title": "9-1"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/Problems/#9-2", 
            "text": "a  The median of $x_1, x_2, \\ldots, x_n$ is $x_{\\lceil \\frac{n}{2} \\rceil}$. So $\\sum_{x_i   x_k} w_i = \\frac{1}{n}(\\lceil \\frac{n}{2} \\rceil - 1)   \\frac{1}{n}(\\frac{n}{2} + 1 - 1) = \\frac{1}{2}$. $\\sum_{x_i   x_k} w_i = \\frac{1}{n}(n - \\lceil \\frac{n}{2} \\rceil) \\leq \\frac{1}{n}(n - \\frac{n}{2}) = \\frac{1}{2}$.  b  First we sort the elements, and then iterate the sorted elements and sum the corresponding weight, if the sum of weights is bigger than 0.5, then the current element is the weighted median.  WEIGHTED-MEDIAN(A)\n\nSORT(A)\nweight-sum = 0\nfor i = 1 to n\n    weight-sum += get weight of A[i]\n    if weight-sum  = 0.5:\n        return A[i]  We need $O(n\\lg{n})$ to sort all elements, and need O(n) to find the weighted median, the total running time is thus $O(n\\lg{n}) + O(n) = O(n\\lg{n})$.  c  First we get the median of all x, and sum the weights of elements whose value is less than median, and sum the weights of elements whose value is larger than median. If the sum of weights in left part is smaller than 0.5 and the sum of weights in right part is also not larger than 0.5, then the median is weighted median. Otherwise, we do the same procedure on the on the left part if its sum of weights is larger than 0.5, or the right part.  WEIGHTED-MEDIAN(A)\n\nif n = 1\n    return A[1]\nif n = 2\n    if A[1].weight  = A[2].weight\n        return A[1]\n    else\n        return A[2]\nelse\n    median = SELECT(A, MATH.CEIL(A / 2))\n    left-weights = 0\n    right-weights = 0\n    for i = 1 to MATH.CEIL(A / 2) - 1\n        left-weights = left-weights + A[i].weight\n    for i = MATH.CEIL(A / 2) + 1 to n\n        right-weights = right-weights + A[i].weight\n    if left-weights   0.5 and right-weights  = 0.5\n        return median\n    elseif left-weights  = 0.5\n        median.weight = median.weight + right-weights\n        A' = { x  = median }\n        return WEIGHTED-MEDIAN(A')\n    elseif right-weights   0.5\n        median.weight = median.weight + left-weights\n        A' = { x  = median }\n        return WEIGHTED-MEDIAN(A')  The recurrence of the algorithm is $T(n) = T(\\frac{n}{2}) + \\Theta(n)$. Let's solve it by the master method. Here we have a = 1, b = 2, $f(n) = \\Theta(n)$. So $\\log_b{a} = \\log_2{1} = 0$, so $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$ for $\\epsilon = 0.5$. And $af(\\frac{n}{b}) = f(\\frac{n}{2}) \\leq cf(n)$ for $c = \\frac{1}{2}$ and all sufficiently large n, thus $T(n) = \\Theta(f(n)) = \\Theta(n)$.  d  Let p denotes the weighted median, so $f(p) = \\sum_{i = 1}^n w_id(p - p_i) = \\sum_{i = 1}^n w_i|p - p_i|$. Since we need to prove the weighted median is the best solution, so for any other point x other than p we should have $f(x) \\geq f(p)$, or $f(x) - f(p) \\geq 0$.  So $f(x) - f(p) = \\sum_{i = 1}^n w_i|x - p_i| - \\sum_{i = 1}^n w_i|p - p_i| = \\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|)$.  First let's check the situation when x   p.  When $p_i \\leq x   p$, $|x - p_i| - |p - p_i| = x - p_i - (p - p_i) = x - p$, when $x   p_i   p$, $|x - p_i| - |p - p_i| = (p_i - x) - (p - p_i)   0 - (p - x) = x - p$. When $x   p \\leq p_i$, $|x - p_i| - |p - p_i| = p_i - x - (p_i - p) = p - x$.  Thus:  $$\n\\begin{eqnarray}\n\\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|)   (x - p)\\sum_{p_i   p}w_i + (p - x)\\sum_{p_i \\geq p}w_i \\\\ =  (p - x)(\\sum_{p_i \\geq p}w_i - \\sum_{p_i   p}w_i)\n\\end{eqnarray}\n$$  Because p is weighted median, so $\\sum_{p_i \\geq p}w_i   \\frac{1}{2}$, $\\sum_{p_i   p}w_i   \\frac{1}{2}$, so $(p - x)(\\sum_{p_i \\geq p}w_i - \\sum_{p_i   p}w_i)   0$.  Now let's check the situation when x   p.  When $p_i \\leq p   x$, $|x - p_i| - |p - p_i| = x - p_i - (p - p_i) = x - p$, when $p   p_i   x$, $|x - p_i| - |p - p_i| = x - p_i - (p_i - p)   0 - (x - p) = p - x$, when $p   x \\leq p_i$, $|x - p_i| - |p - p_i| = p_i - x - (p_i - p) = p - x$.  Thus:  $$\n\\begin{eqnarray}\n\\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|)   (p - x)\\sum_{p_i   p}w_i + (x - p)\\sum_{p_i \\leq p}w_i \\\\ =  (x - p)(\\sum_{p_i \\leq p}w_i - \\sum_{p_i   p}w_i)\n\\end{eqnarray}\n$$  Because $\\sum_{p_i \\leq p}w_i   \\frac{1}{2}$ and $\\sum_{p_i   p}w_i   \\frac{1}{2}$, so $(x - p)(\\sum_{p_i \\leq p}w_i - \\sum_{p_i   p}w_i)   0$.  So for any point x other than p we have $f(x)   f(p)$, so the weighted median is the best solution.  e  We need to find a point p(x, y) such that $\\sum_{i = 1}^n w_i(|x - x_i| + |y - y_i|)$ is minimum. Because $\\sum_{i = 1}^n w_i(|x - x_i| + |y - y_i|) = \\sum_{i = 1}^n w_i|x - x_i| + \\sum_{i = 1}^n w_i|y - y_i|$, the problem is actually 2 1-dimensional problems. Thus let x be the weighted median of all x coordinate values, and let y be the weighted median of all y coordinate values, so p(x, y) is the best solution.", 
            "title": "9-2"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/Problems/#9-3", 
            "text": "a  If $i \\geq \\frac{n}{2}$, then we use the  SELECT  algorithm, otherwise we group every two elements into pairs $(a_j, b_j)$ and make sure $a_j \\leq b_j$, if n is odd, we also let the last element be a pair, this step needs $\\lfloor \\frac{n}{2} \\rfloor$. So now we have $\\lceil \\frac{n}{2} \\rceil$ pairs. Then we recursively call the algorithm on $a_j$, so we can get the ith smallest element of all $a_j$, this step requires $U_i(\\lceil \\frac{n}{2} \\rceil)$. Notice that the partition method partition all $a_j$ into two parts, so the ith smallest element of all elements could only be among $a_1\\ldots{a_i}$ and $b_1\\ldots{b_i}$. Then we run the  SELECT  algorithm on the 2i elements to find the ith smallest element.  SMALL-ORDER-STATISTICS(A, i)\nif i  = n / 2\n    return SELECT(A, i)\nlet pairs be a new array\nfor i = 1 to n with step = 2\n    if i + 1  = n\n        if A[i]  = A[i + 1]\n            insert [A[i], A[i + 1]] into pairs\n        else\n            insert [A[i + 1], A[i]] into pairs\n    else\n        insert [A[i]] into pairs\nSMALL-ORDER-STATISTICS(pairs, i) // Run algorithm on all aj\nreturn SELECT(pairs, i) // Run algorithm on a1 to ai plus b1 to bi  b  Let's solve by the substitution method. We start by assuming that $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i}))$ holds for all positive m   n, in particular for $m = \\lceil \\frac{n}{2} \\rceil$, yielding $U_i(\\lceil \\frac{n}{2} \\rceil) = \\lceil \\frac{n}{2} \\rceil + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}))$. Substituting into the recurrence yields:  $$\n\\begin{eqnarray}\nU_i(n)  =  \\lfloor \\frac{n}{2} \\rfloor + \\lceil \\frac{n}{2} \\rceil + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i})) + T(2i) \\\\ =  n + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i})) + O(T(2i)) \\\\ =  n + O(T(2i)(\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}) + 1)) \\\\ =  n + O(T(2i)(\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}) + \\lg2)) \\\\ =  n + O(T(2i)\\lg(\\frac{n}{i}))\n\\end{eqnarray}\n$$  c  If i is a constant, and because $T(n) = O(n)$, so T(2i) = O(2i), so T(2i) is also a constant, and $O(\\lg{\\frac{n}{i}}) = O(\\lg{n})$. Thus $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i})) = n + O(\\lg{n})$.  d  It's so obvious, we just replace i with $\\frac{n}{k}$ and yields $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i})) = n + O(T(2\\frac{n}{k})\\lg(\\frac{n}{\\frac{n}{k}})) = n + O(T(\\frac{2n}{k})\\lg{k})$.", 
            "title": "9-3"
        }, 
        {
            "location": "/9-Medians-and-Order-Statistics/Problems/#9-4", 
            "text": "a  $z_i$ and $z_j$ are compared if and only if the first element to be chosen as a pivot from $Z_{ijk}$ is either $z_i$ or $z_j$. And the range of $Z_{ijk}$ depends on k. So:  $$\n\\begin{eqnarray}\nE[X_{ijk}]  = \n    \\begin{cases}\n      \\frac{2}{j - k + 1}   \\text{if } k \\leq i   j \\\\\n      \\frac{2}{j - i + 1}   \\text{if } i   k \\leq j \\\\\n      \\frac{2}{k - i + 1}   \\text{if } i   j   k\n    \\end{cases}\n\\end{eqnarray}\n$$  b  $$\n\\begin{eqnarray}\nE[X_k]  =  \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n E[X_{ijk}] \\\\ =  \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} E[X_{ijk}] + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} E[X_{ijk}] + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} E[X_{ijk}] \\\\ =  \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{2}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{2}{j - i + 1} + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} \\frac{2}{k - i + 1} \\\\ =  2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} \\frac{1}{k - i + 1}) \\\\ =  2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\\\ =  2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} - \\sum_{j = k}^n \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\\\ =  2(\\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} - \\sum_{j = k}^n \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1})\n\\end{eqnarray}\n$$  c  We have $\\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} \\leq n$ ( source ), and $\\sum_{k + 1}^n \\frac{j - k - 1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}   \\sum_{k + 1}^n 1 + \\sum_{i = 1}^{k - 2} 1 = n - k + k - 2 = n - 2   n$, so $E[X_k] \\leq 2(n + n) = 4n$.  d  Since $E[X_k] \\leq 4n$, thus $T(n) = O(n)$.", 
            "title": "9-4"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.1-Stacks-and-queues/", 
            "text": "10.1 Stacks and queues\n\n\n10.1-1\n\n\n\n\n10.1-2\n\n\nThe first stack \nS1\n starts at the first index, when pushing element into \nS1\n, we let \nS1.top += 1\n, and the second stack \nS2\n starts at the last index, when pushing element into \nS2\n, we let \nS2.top -= 1\n, when \nS1.top \n S2.top\n, it overflows.\n\n\n10.1-3\n\n\n\n\n10.1-4\n\n\nInitially, we let \nQ.head = 0\n and \nQ.tail = 1\n.\n\n\nENQUEUE(Q, x)\n\nif Q.head == Q.tail\n    error \noverflow\n\nQ[Q.tail] = x\nif Q.tail == Q.length\n    Q.tail = 1\nelse\n    Q.tail = Q.tail + 1\nif Q.head == 0\n    Q.head = 1\n\n\n\n\nDEQUEUE(Q)\n\nif Q.head == 0\n    error \nunderflow\n\nx = Q[Q.head]\nif Q.head = Q.length\n    Q.head = 1\nelse Q.head = Q.head + 1\nif Q.head == Q.tail\n    Q.head = 0\n    Q.tail = 1\nreturn x\n\n\n\n\n10.1-5\n\n\nIS-EMPTY(DQ)\n\nreturn DQ.left == 0 and DQ.right == DQ.size\n\n\n\n\nIS-FULL(DQ)\n\nreturn DQ.right - DQ.left == 1\n\n\n\n\nAPPEND(DQ, x)\n\nif IS-FULL(DQ)\n    error \noverflow\n\nDQ.right = DQ.right - 1\nif DQ.right == 0\n    DQ.right = DQ.size\nDQ[DQ.right] = x\n\n\n\n\nPOP(DQ)\n\nif IS-EMPTY(DQ)\n    error \nunderflow\n\nif DQ.right == DQ.size + 1\n    DQ.right = 1\nx = DQ[DQ.right]\nif DQ.left == DQ.right\n    DQ.left = 0\n    DQ.right = DQ.size + 1\nelse\n    DQ.right = (DQ.right + 1) % DQ.size\nreturn x\n\n\n\n\nAPPEND-LEFT(DQ, x)\n\nif IS-FULL(DQ)\n    error \noverflow\n\nDQ.left = DQ.left + 1\nif DQ.left == DQ.size + 1\n    DQ.left = 1\nDQ[DQ.left] = x\n\n\n\n\nSHIFT(DQ)\n\nif IS-EMPTY(DQ)\n    error \nunderflow\n\nif DQ.left == 0\n    DQ.left = DQ.size\nx = DQ[DQ.left]\nif DQ.left == DQ.right\n    DQ.left = 0\n    DQ.right = DQ.size + 1\nelse\n    DQ.left = DQ.left - 1\n    if DQ.left == 0\n        DQ.left = DQ.size\nreturn x\n\n\n\n\n10.1-6\n\n\nENQUEUE(Q, x)\n\nif stack-a.count + stack-b.count == n\n    error \noverflow\n\nstack-a.push(x)\n\n\n\n\nDEQUEUE(Q)\n\nif stack-a.count == 0 and stack-b.count == 0\n    error \nunderflow\n\nif stack-b.count == 0\n    while stack-a.count != 0\n        stack-b.push(stack-a.pop())\nreturn stack-b.pop()\n\n\n\n\nWe create two stacks \nstack-a\n and \nstack-b\n, the \nENQUEUE\n operation push \nx\n to \nstack-a\n, the \nDEQUEUE\n operation checks if \nstack-b\n is empty first, if it's empty, then pop every elements from \nstack-a\n, and push them to \nstack-b\n. Then call pop on \nstack-b\n.\n\n\nThe running time of \nENQUEUE\n is O(1), but the running time of \nDEQUEUE\n is O(n).\n\n\n10.1-7\n\n\nPUSH(S, x)\nif queue.count == n\n    error \noverflow\n\nqueue.enqueue(x)\n\n\n\n\nPOP(S)\nif queue.count == 0\n    error \nunderflow\n\nwhile not queue.count == 0\n    x = queue.dequeue()\n\n    if not queue.count == 0\n        queue-auxiliary.enqueue(x)\nexchange queue with queue-auxiliary\nreturn x\n\n\n\n\nWe create two queues \nqueue\n and \nqueue-auxiliary\n, the \nPUSH\n operation insert \nx\n to \nqueue\n, and the \nPOP\n operation moves all elements except the last one into \nqueue-auxiliary\n, and exchange \nqueue\n with \nqueue-auxiliary\n, then return \nx\n.\n\n\nThe running time of \nPUSH\n is O(1), but the running time of \nPOP\n is O(n).", 
            "title": "10.1 Stacks and queues"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-stacks-and-queues", 
            "text": "", 
            "title": "10.1 Stacks and queues"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-1", 
            "text": "", 
            "title": "10.1-1"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-2", 
            "text": "The first stack  S1  starts at the first index, when pushing element into  S1 , we let  S1.top += 1 , and the second stack  S2  starts at the last index, when pushing element into  S2 , we let  S2.top -= 1 , when  S1.top   S2.top , it overflows.", 
            "title": "10.1-2"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-3", 
            "text": "", 
            "title": "10.1-3"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-4", 
            "text": "Initially, we let  Q.head = 0  and  Q.tail = 1 .  ENQUEUE(Q, x)\n\nif Q.head == Q.tail\n    error  overflow \nQ[Q.tail] = x\nif Q.tail == Q.length\n    Q.tail = 1\nelse\n    Q.tail = Q.tail + 1\nif Q.head == 0\n    Q.head = 1  DEQUEUE(Q)\n\nif Q.head == 0\n    error  underflow \nx = Q[Q.head]\nif Q.head = Q.length\n    Q.head = 1\nelse Q.head = Q.head + 1\nif Q.head == Q.tail\n    Q.head = 0\n    Q.tail = 1\nreturn x", 
            "title": "10.1-4"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-5", 
            "text": "IS-EMPTY(DQ)\n\nreturn DQ.left == 0 and DQ.right == DQ.size  IS-FULL(DQ)\n\nreturn DQ.right - DQ.left == 1  APPEND(DQ, x)\n\nif IS-FULL(DQ)\n    error  overflow \nDQ.right = DQ.right - 1\nif DQ.right == 0\n    DQ.right = DQ.size\nDQ[DQ.right] = x  POP(DQ)\n\nif IS-EMPTY(DQ)\n    error  underflow \nif DQ.right == DQ.size + 1\n    DQ.right = 1\nx = DQ[DQ.right]\nif DQ.left == DQ.right\n    DQ.left = 0\n    DQ.right = DQ.size + 1\nelse\n    DQ.right = (DQ.right + 1) % DQ.size\nreturn x  APPEND-LEFT(DQ, x)\n\nif IS-FULL(DQ)\n    error  overflow \nDQ.left = DQ.left + 1\nif DQ.left == DQ.size + 1\n    DQ.left = 1\nDQ[DQ.left] = x  SHIFT(DQ)\n\nif IS-EMPTY(DQ)\n    error  underflow \nif DQ.left == 0\n    DQ.left = DQ.size\nx = DQ[DQ.left]\nif DQ.left == DQ.right\n    DQ.left = 0\n    DQ.right = DQ.size + 1\nelse\n    DQ.left = DQ.left - 1\n    if DQ.left == 0\n        DQ.left = DQ.size\nreturn x", 
            "title": "10.1-5"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-6", 
            "text": "ENQUEUE(Q, x)\n\nif stack-a.count + stack-b.count == n\n    error  overflow \nstack-a.push(x)  DEQUEUE(Q)\n\nif stack-a.count == 0 and stack-b.count == 0\n    error  underflow \nif stack-b.count == 0\n    while stack-a.count != 0\n        stack-b.push(stack-a.pop())\nreturn stack-b.pop()  We create two stacks  stack-a  and  stack-b , the  ENQUEUE  operation push  x  to  stack-a , the  DEQUEUE  operation checks if  stack-b  is empty first, if it's empty, then pop every elements from  stack-a , and push them to  stack-b . Then call pop on  stack-b .  The running time of  ENQUEUE  is O(1), but the running time of  DEQUEUE  is O(n).", 
            "title": "10.1-6"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-7", 
            "text": "PUSH(S, x)\nif queue.count == n\n    error  overflow \nqueue.enqueue(x)  POP(S)\nif queue.count == 0\n    error  underflow \nwhile not queue.count == 0\n    x = queue.dequeue()\n\n    if not queue.count == 0\n        queue-auxiliary.enqueue(x)\nexchange queue with queue-auxiliary\nreturn x  We create two queues  queue  and  queue-auxiliary , the  PUSH  operation insert  x  to  queue , and the  POP  operation moves all elements except the last one into  queue-auxiliary , and exchange  queue  with  queue-auxiliary , then return  x .  The running time of  PUSH  is O(1), but the running time of  POP  is O(n).", 
            "title": "10.1-7"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.2-Linked-lists/", 
            "text": "10.2 Linked lists\n\n\n10.2-1\n\n\nWe can implement \nINSERT\n in O(1) time, but cannot implement \nDELETE\n in O(1) time, it's O(n).\n\n\n10.2-2\n\n\nPUSH(x)\n\nL.insert(x)\n\n\n\n\nPOP()\n\nif L.head == NIL\n    error \nunderflow\n\nx = L.head\nL.head = L.head.next\nreturn x\n\n\n\n\n10.2-3\n\n\nENQUEUE(x)\n\nnew = SinglyLinkedNode(x)\nif L.head == NIL\n    L.head = new\n    L.tail = new\nelse\n    L.tail.next = new\n    L.tail = new\n\n\n\n\nDEQUEUE()\n\nif L.head == NIL\n    error \nunderflow\n\nx = L.head\nL.head = L.head.next\nreturn x\n\n\n\n\n10.2-4\n\n\nLIST-SEARCH'(L, k)\n\nx = L.nil.next\nL.nil.key = k\nwhile x.key != k\n    x = x.next\nif x != L.nil\n    return x\nelse\n    return NIL\n\n\n\n\n10.2-5", 
            "title": "10.2 Linked lists"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.2-Linked-lists/#102-linked-lists", 
            "text": "", 
            "title": "10.2 Linked lists"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.2-Linked-lists/#102-1", 
            "text": "We can implement  INSERT  in O(1) time, but cannot implement  DELETE  in O(1) time, it's O(n).", 
            "title": "10.2-1"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.2-Linked-lists/#102-2", 
            "text": "PUSH(x)\n\nL.insert(x)  POP()\n\nif L.head == NIL\n    error  underflow \nx = L.head\nL.head = L.head.next\nreturn x", 
            "title": "10.2-2"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.2-Linked-lists/#102-3", 
            "text": "ENQUEUE(x)\n\nnew = SinglyLinkedNode(x)\nif L.head == NIL\n    L.head = new\n    L.tail = new\nelse\n    L.tail.next = new\n    L.tail = new  DEQUEUE()\n\nif L.head == NIL\n    error  underflow \nx = L.head\nL.head = L.head.next\nreturn x", 
            "title": "10.2-3"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.2-Linked-lists/#102-4", 
            "text": "LIST-SEARCH'(L, k)\n\nx = L.nil.next\nL.nil.key = k\nwhile x.key != k\n    x = x.next\nif x != L.nil\n    return x\nelse\n    return NIL", 
            "title": "10.2-4"
        }, 
        {
            "location": "/10-Elementary-Data-Structures/10.2-Linked-lists/#102-5", 
            "text": "", 
            "title": "10.2-5"
        }
    ]
}