{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to Algorithms Solutions to Introduction to Algorithms, third edition.","title":"Introduction"},{"location":"#introduction-to-algorithms","text":"Solutions to Introduction to Algorithms, third edition.","title":"Introduction to Algorithms"},{"location":"02-Getting-Started/2.1-Insertion-sort/","text":"2.1 Insertion sort 2.1-1 2.1-2 INSERTION-SORT (A) (Decreasing order) for j = 2 to A.length key = A[j] // Insert A[j] into the sorted sequence A[1..j - 1] i = j - 1 while i > 0 and A[i] < key A[i + 1] = A[i] i = i - 1 A[i + 1] = key 2.1-3 Pseudocode: LINEAR-SEARCH (A) for i = 1 to A.length if A[i] == v return i return NIL Loop invariant: At the start of each iteration of the for loop, v doesn't exist in the subarray A[1..i - 1] . And let's see how the loop invariant fulfills the three necessary properties. Initialization : in the first loop iteration, i = 1, the subarray A[1..i - 1] is an empty array, therefore, v doesn't exist int A[1..i - 1]. So it shows that the loop invariant holds prior to the first iteration of the loop. Maintenance : at the start of each iteration of the for loop, we suppose it is true that v doesn't exist in the subarray A[1..i - 1]. Then we check if A[i] == v, if it is true, then we are done, we found v. If it is not true, then it means v doesn't exist in the subarray A[1..i], which holds the loop invariant before the i + 1 iteration. Termination : when the for loop is terminated, i might be some number between 1 and A.length, or i equals to A.length + 1. If i equals to A.length + 1, we have that the subarray A[1..A.length] doesn't contain v, and since the subarray A[1..A.length] is the entire array, we know v doesn't exist in the entir array and it returns NIL, the algorithm is correct. And if i is some number between 1 and A.length, then we know v is found during the for loop, the algorithm is also correct. 2.1-4 The problem description: Input : a sequence of n numbers $A = \\langle a_1, a_2, \\ldots, a_n \\rangle, a_i \\in \\langle0, 1\\rangle$, and a sequence of n numbers $B = \\langle b_1, b_2, \\ldots, b_n \\rangle, b_i \\in \\langle0, 1\\rangle$. A and B represent the binay form of two integers. Output : a sequence of n + 1 numbers C, the binary integer represented by C is the sum of the binary integers represented by A and B. Pseudocode: BINARY-SUM(A, B) carry = 0 C = [0] * (A.length + 1) for i = A.length to 1 C[i] = (A[i] + B[i] + carry) % 2 carry = (A[i] + B[i] + carry) / 2 C[1] = carry return C","title":"2.1 Insertion sort"},{"location":"02-Getting-Started/2.1-Insertion-sort/#21-insertion-sort","text":"","title":"2.1 Insertion sort"},{"location":"02-Getting-Started/2.1-Insertion-sort/#21-1","text":"","title":"2.1-1"},{"location":"02-Getting-Started/2.1-Insertion-sort/#21-2","text":"INSERTION-SORT (A) (Decreasing order) for j = 2 to A.length key = A[j] // Insert A[j] into the sorted sequence A[1..j - 1] i = j - 1 while i > 0 and A[i] < key A[i + 1] = A[i] i = i - 1 A[i + 1] = key","title":"2.1-2"},{"location":"02-Getting-Started/2.1-Insertion-sort/#21-3","text":"Pseudocode: LINEAR-SEARCH (A) for i = 1 to A.length if A[i] == v return i return NIL Loop invariant: At the start of each iteration of the for loop, v doesn't exist in the subarray A[1..i - 1] . And let's see how the loop invariant fulfills the three necessary properties. Initialization : in the first loop iteration, i = 1, the subarray A[1..i - 1] is an empty array, therefore, v doesn't exist int A[1..i - 1]. So it shows that the loop invariant holds prior to the first iteration of the loop. Maintenance : at the start of each iteration of the for loop, we suppose it is true that v doesn't exist in the subarray A[1..i - 1]. Then we check if A[i] == v, if it is true, then we are done, we found v. If it is not true, then it means v doesn't exist in the subarray A[1..i], which holds the loop invariant before the i + 1 iteration. Termination : when the for loop is terminated, i might be some number between 1 and A.length, or i equals to A.length + 1. If i equals to A.length + 1, we have that the subarray A[1..A.length] doesn't contain v, and since the subarray A[1..A.length] is the entire array, we know v doesn't exist in the entir array and it returns NIL, the algorithm is correct. And if i is some number between 1 and A.length, then we know v is found during the for loop, the algorithm is also correct.","title":"2.1-3"},{"location":"02-Getting-Started/2.1-Insertion-sort/#21-4","text":"The problem description: Input : a sequence of n numbers $A = \\langle a_1, a_2, \\ldots, a_n \\rangle, a_i \\in \\langle0, 1\\rangle$, and a sequence of n numbers $B = \\langle b_1, b_2, \\ldots, b_n \\rangle, b_i \\in \\langle0, 1\\rangle$. A and B represent the binay form of two integers. Output : a sequence of n + 1 numbers C, the binary integer represented by C is the sum of the binary integers represented by A and B. Pseudocode: BINARY-SUM(A, B) carry = 0 C = [0] * (A.length + 1) for i = A.length to 1 C[i] = (A[i] + B[i] + carry) % 2 carry = (A[i] + B[i] + carry) / 2 C[1] = carry return C","title":"2.1-4"},{"location":"02-Getting-Started/2.2-Analyzing-algorithms/","text":"2.2 Analyzing algorithms 2.2-1 $\\Theta(n^3)$. 2.2-2 Pseudocode: SELECTION-SORT (A) 1 for i = 1 to A.length - 1 2 min_index = i 3 4 for j = i + 1 to A.length 5 if A[min_index] > A[j] 6 min_index = j 7 8 if i != min_index: 9 temp = A[i] 10 A[i] = A[min_index] 11 A[min_index] = temp Loop invariant: At the start of each iteration of the outer for loop, the subarray A[1..i - 1] consists of elements that are in sorted order . Why does it need to run for only the first n - 1 elements, rather than for all n elements? When i equals to A.length - 1, only the last two elements may not be in sorted order, and we need to find the A.length - 1 smallest element, so we compare A[A.length - 1] and A[A.length]. We put the smaller element at the index of A.length - 1, and put the larger element at the index of A.length. So after this iteration, the last element of the array is already the largest number. So it's not necessary to run for the nth element. Give the best-case and worst-case running times of selection sort in $\\Theta$ notation. Let's first see the cost of each statement and the number of times each statement is executed. We let $t_i$ denote the number of times the operation in line 6 is executed for that value of i, and let $p_i$ denote the number of times the operation in line 9/10/11 is executed for that value of i. Line number Cost Times 1 $c_1$ $n$ 2 $c_2$ $n - 1$ 4 $c_4$ $\\sum_{i = 1}^{n - 1} (n - i + 1) = \\frac{n(n + 1)}{2} - 1$ 5 $c_5$ $\\sum_{i = 1}^{n - 1} (n - i) = \\frac{n(n - 1)}{2}$ 6 $c_6$ $\\sum_{i = 1}^{n - 1} t_i$ 8 $c_8$ $n - 1$ 9 $c_9$ $\\sum_{i = 1}^{n - 1} p_i$ 10 $c_{10}$ $\\sum_{i = 1}^{n - 1} p_i$ 11 $c_{11}$ $\\sum_{i = 1}^{n - 1} p_i$ So the running time of selection sort is: $$T(n) = c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_6\\sum_{i = 1}^{n - 1} t_i + c_8(n - 1) + \\\\\\ (c_9 + c_{10} + c_{11})\\sum_{i = 1}^{n - 1} p_i$$ Now let's see the best-case. The best-case occurs if the array is already sorted. Thus $t_i$ and $p_i$ are both 0, and the best-case running time is: $$ \\begin{eqnarray} T(n) &=& c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_8(n - 1) \\\\\\ &=& (\\frac{c_4}{2} + \\frac{c_5}{2})n^2 + (c_1 + c_2 + \\frac{c_4}{2} - \\frac{c_5}{2} + c_8)n - (c_2 + c_4 + c_8) \\\\\\ &=& \\Theta(n^2) \\end{eqnarray} $$ And the worst-case occurs if the array is in reverse sorted order. Thus in the iteration of j, line 6 is executed from i + 1 to A.length - (i - 1) (because after the first iteration of i, the largest element is at the index of A.length, after the second iteration of i, the second largest element is at the index of A.length - 1, so the last i - 1 elements are bigger than the previous elements), so $t_i = n - i - (i - 1) = n - 2i + 1$. So the times of line 6 is $c_6\\sum_{i = 1}^{n - 1} (n - 2i + 1)$, notice that $n - 2i + 1$ will be 0, when it's 0, it's not necessary to sum it, to make it simple, let's assume it stops when $i = \\frac{n}{2}$. So: $$ \\begin{eqnarray} \\sum_{i = 1}^{n - 1} (n - 2i + 1) &=& (n - 1) + (n - 3) + \\ldots + 1 \\\\\\ &=& n * \\frac{n}{2} - (1 + 3 + 5 + \\ldots + (2 * \\frac{n}{2} - 1)) \\\\\\ &=& \\frac{n^2}{2} - \\frac{\\frac{n}{2}(1 + n - 1)}{2} \\\\\\ &=& \\frac{n^2}{4} \\end{eqnarray} $$ Now let's see $p_i$, we already assume that line 6 will not be executed when $i \\geq \\frac{n}{2}$, which also means line 9 to line 11 will not be executed. So $\\sum_{i = 1}^{n - 1} p_i$ should be $\\frac{n}{2}$. So the running time of worst-case is: $$ \\begin{eqnarray} T(n) &=& c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_6\\frac{n^2}{4} + c_8(n - 1) + \\frac{c_9 + c_{10} + c_{11}}{2}n \\\\\\ &=& (\\frac{c_4}{2} + \\frac{c_5}{2} + \\frac{c_6}{4})n^2 + (c_1 + c_2 + \\frac{c_4}{2} - \\frac{c_5}{2} + c_8 + \\frac{c_9 + c_{10} + c_{11}}{2})n - (c_2 + c_4 + c_8) \\\\\\ &=& \\Theta(n^2) \\end{eqnarray} $$ 2.2-3 How many elements of the input sequence need to be checked on the average? Since the element being searched for is equally likely to be any element in the array, so the probability of finding the target value at index i is $1 / n$, and it will check i elements. So the average checks is: $$\\frac{1}{n}(1 + 2 + 3 + \\ldots + n - 1 + n) = \\frac{1}{n}\\frac{n(n + 1)}{2}=\\frac{n + 1}{2}$$ And the worst case will check n elements. The running time of average case and worst case are both $\\Theta(n)$. 2.2-4 We can check if the input is a best-case first, if it is true, then we can return a predefined solution for that best-case. For example, in a sorting algorithm, if the input is sorted, then we can return the array directly.","title":"2.2 Analyzing algorithms"},{"location":"02-Getting-Started/2.2-Analyzing-algorithms/#22-analyzing-algorithms","text":"","title":"2.2 Analyzing algorithms"},{"location":"02-Getting-Started/2.2-Analyzing-algorithms/#22-1","text":"$\\Theta(n^3)$.","title":"2.2-1"},{"location":"02-Getting-Started/2.2-Analyzing-algorithms/#22-2","text":"Pseudocode: SELECTION-SORT (A) 1 for i = 1 to A.length - 1 2 min_index = i 3 4 for j = i + 1 to A.length 5 if A[min_index] > A[j] 6 min_index = j 7 8 if i != min_index: 9 temp = A[i] 10 A[i] = A[min_index] 11 A[min_index] = temp Loop invariant: At the start of each iteration of the outer for loop, the subarray A[1..i - 1] consists of elements that are in sorted order . Why does it need to run for only the first n - 1 elements, rather than for all n elements? When i equals to A.length - 1, only the last two elements may not be in sorted order, and we need to find the A.length - 1 smallest element, so we compare A[A.length - 1] and A[A.length]. We put the smaller element at the index of A.length - 1, and put the larger element at the index of A.length. So after this iteration, the last element of the array is already the largest number. So it's not necessary to run for the nth element. Give the best-case and worst-case running times of selection sort in $\\Theta$ notation. Let's first see the cost of each statement and the number of times each statement is executed. We let $t_i$ denote the number of times the operation in line 6 is executed for that value of i, and let $p_i$ denote the number of times the operation in line 9/10/11 is executed for that value of i. Line number Cost Times 1 $c_1$ $n$ 2 $c_2$ $n - 1$ 4 $c_4$ $\\sum_{i = 1}^{n - 1} (n - i + 1) = \\frac{n(n + 1)}{2} - 1$ 5 $c_5$ $\\sum_{i = 1}^{n - 1} (n - i) = \\frac{n(n - 1)}{2}$ 6 $c_6$ $\\sum_{i = 1}^{n - 1} t_i$ 8 $c_8$ $n - 1$ 9 $c_9$ $\\sum_{i = 1}^{n - 1} p_i$ 10 $c_{10}$ $\\sum_{i = 1}^{n - 1} p_i$ 11 $c_{11}$ $\\sum_{i = 1}^{n - 1} p_i$ So the running time of selection sort is: $$T(n) = c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_6\\sum_{i = 1}^{n - 1} t_i + c_8(n - 1) + \\\\\\ (c_9 + c_{10} + c_{11})\\sum_{i = 1}^{n - 1} p_i$$ Now let's see the best-case. The best-case occurs if the array is already sorted. Thus $t_i$ and $p_i$ are both 0, and the best-case running time is: $$ \\begin{eqnarray} T(n) &=& c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_8(n - 1) \\\\\\ &=& (\\frac{c_4}{2} + \\frac{c_5}{2})n^2 + (c_1 + c_2 + \\frac{c_4}{2} - \\frac{c_5}{2} + c_8)n - (c_2 + c_4 + c_8) \\\\\\ &=& \\Theta(n^2) \\end{eqnarray} $$ And the worst-case occurs if the array is in reverse sorted order. Thus in the iteration of j, line 6 is executed from i + 1 to A.length - (i - 1) (because after the first iteration of i, the largest element is at the index of A.length, after the second iteration of i, the second largest element is at the index of A.length - 1, so the last i - 1 elements are bigger than the previous elements), so $t_i = n - i - (i - 1) = n - 2i + 1$. So the times of line 6 is $c_6\\sum_{i = 1}^{n - 1} (n - 2i + 1)$, notice that $n - 2i + 1$ will be 0, when it's 0, it's not necessary to sum it, to make it simple, let's assume it stops when $i = \\frac{n}{2}$. So: $$ \\begin{eqnarray} \\sum_{i = 1}^{n - 1} (n - 2i + 1) &=& (n - 1) + (n - 3) + \\ldots + 1 \\\\\\ &=& n * \\frac{n}{2} - (1 + 3 + 5 + \\ldots + (2 * \\frac{n}{2} - 1)) \\\\\\ &=& \\frac{n^2}{2} - \\frac{\\frac{n}{2}(1 + n - 1)}{2} \\\\\\ &=& \\frac{n^2}{4} \\end{eqnarray} $$ Now let's see $p_i$, we already assume that line 6 will not be executed when $i \\geq \\frac{n}{2}$, which also means line 9 to line 11 will not be executed. So $\\sum_{i = 1}^{n - 1} p_i$ should be $\\frac{n}{2}$. So the running time of worst-case is: $$ \\begin{eqnarray} T(n) &=& c_1n + c_2(n - 1) + c_4(\\frac{n(n + 1)}{2} - 1) + c_5\\frac{n(n - 1)}{2} + c_6\\frac{n^2}{4} + c_8(n - 1) + \\frac{c_9 + c_{10} + c_{11}}{2}n \\\\\\ &=& (\\frac{c_4}{2} + \\frac{c_5}{2} + \\frac{c_6}{4})n^2 + (c_1 + c_2 + \\frac{c_4}{2} - \\frac{c_5}{2} + c_8 + \\frac{c_9 + c_{10} + c_{11}}{2})n - (c_2 + c_4 + c_8) \\\\\\ &=& \\Theta(n^2) \\end{eqnarray} $$","title":"2.2-2"},{"location":"02-Getting-Started/2.2-Analyzing-algorithms/#22-3","text":"How many elements of the input sequence need to be checked on the average? Since the element being searched for is equally likely to be any element in the array, so the probability of finding the target value at index i is $1 / n$, and it will check i elements. So the average checks is: $$\\frac{1}{n}(1 + 2 + 3 + \\ldots + n - 1 + n) = \\frac{1}{n}\\frac{n(n + 1)}{2}=\\frac{n + 1}{2}$$ And the worst case will check n elements. The running time of average case and worst case are both $\\Theta(n)$.","title":"2.2-3"},{"location":"02-Getting-Started/2.2-Analyzing-algorithms/#22-4","text":"We can check if the input is a best-case first, if it is true, then we can return a predefined solution for that best-case. For example, in a sorting algorithm, if the input is sorted, then we can return the array directly.","title":"2.2-4"},{"location":"02-Getting-Started/2.3-Designing-algorithms/","text":"2.3 Designing algorithms 2.3-1 2.3-2 MERGE(A, p, q, r) n1 = q - p + 1 n2 = r - q let L[1..n1] and R[1..n2] be new arrays for i = 1 to n1 L[i] = A[p + i - 1] for j = 1 to n2 R[j] = A[q + j] i = 1 j = 1 for k = p to r if i > n1: A[k] = R[j] j = j + 1 else if j > n2: A[k] = L[i] i = i + 1 else if L[i] <= R[j]: A[k] = L[i] i = i + 1 else A[k] = R[j] j = j + 1 2.3-3 Because n is an exact power of 2, so the sequence of n is $2, 4, 8, 16, \\ldots, 2^k$. First, let's see the base case, the base case is $n = 2$. And we have $T(n) = 2 = 2\\lg{2}$. So the statement holds true for $n = 2$. Second, let's see the inductive step. We assume $T(n) = n\\lg{n}$ is true when $n = 2^k, k > 1$, and we want to prove it also holds true for the $n + 1$ element, which is $2^{k + 1}$. When $n = 2^{k + 1}$, we have: $$ \\begin{eqnarray} T(2^{k + 1}) &=& 2T(\\frac{2^{k + 1}}{2}) + 2^{k + 1} \\\\\\ &=& 2T(2^k) + 2^{k + 1} \\\\\\ &=& 2 * 2^k\\lg{2^k} + 2^{k + 1} \\\\\\ &=& 2^{k + 1}\\lg{2^k} + 2^{k + 1} \\\\\\ &=& 2^{k + 1}(\\lg{2^k} + 1) \\\\\\ &=& 2^{k + 1}(\\lg{2^k} + \\lg{2}) \\\\\\ &=& 2^{k + 1}\\lg{2^{k + 1}} \\end{eqnarray} $$ So $T(n) = n\\lg{n}$ is true for all $n = 2^k, k \\geq 1$. 2.3-4 The pseudocode would be like this: INSERT(A, end) key = A[end] i = end - 1 while i > 0 and A[i] > key A[i + 1] = A[i] i = i - 1 A[i + 1] = key RECURSIVE-INSERTION-SORT(A, end) if end > 1: RECURSIVE-INSERTION-SORT(A, end - 1) INSERT(A, end) And here is the recurrence for the running time (let $I(n)$ denotes the running time of inserting $A[n]$ to $A[1..n - 1]$): $$ T(n) = \\begin{cases} \\Theta(1) & if\\ n = 1, \\\\\\ T(n - 1) + I(n) & if\\ n \\geq 2 \\end{cases} $$ 2.3-5 Pseudocode: ITERATIVE-BINARY-SEARCH(A, v) low = 1 high = n while low <= high middle = (low + high) / 2 if A[middle] < v low = middle + 1 else if A[middle] > v high = middle - 1 else return middle return NIL RECURSIVE-BINARY-SEARCH(A, v, low, high) if low <= high middle = (low + high) / 2 if A[middle] < v return RECURSIVE-BINARY-SEARCH(A, v, middle + 1, high) else if A[middle] > v return RECURSIVE-BINARY-SEARCH(A, v, low, middle - 1) else return middle return NIL From the pseudocode we can see either iterative or recursive binary search will halve the size of problem in each step when the middle element doesn't match the target value. So we have: $$ T(n) = \\begin{cases} C & if\\ n = 1, \\\\\\ T(n / 2) + C & if\\ n \\geq 2 \\end{cases} $$ And we can rewrite $T(n)$ like: $$ \\begin{eqnarray} T(n) &=& T(n / 2) + C \\\\\\ &=& T(n / 2^2) + C + C \\\\\\ &=& T(n / 2^3) + C + C + C \\\\\\ &=& ... \\\\\\ &=& T(1) + C\\lg{n} \\\\\\ &=& C(\\lg{n} + 1) \\\\\\ &=& \\Theta(\\lg{n}) \\end{eqnarray} $$ So the worst-case running time of binary search is $\\Theta(\\lg{n})$. 2.3-6 No, we cannot. The running time of finding the position to insert the new element is $\\Theta(\\lg{n})$, but we still need $\\Theta(n)$ time to insert that element into the array. So the worst-case running time is still $\\Theta(n^2)$. 2.3-7 TWO-SUM(S, x) MERGE-SORT(S) for i = 1 to S.length index = BINARY-SEARCH(S, x - S[i]) if index != NIL and index != i return true return false First we sort S by merge sort, and the running time is $\\Theta(n\\lg{n})$. Then we iterate S, for each element in S, if this element is one of the two elements whose sum is exactly x, then we know x minus this element should be also in S. We can use binary search to search it. If the search result is not NIL and not equal to i, then we return true. The running time of binary search is $\\Theta(\\lg{n})$, and in the worst-case, we will run it for $n$ times, and the running time is $\\Theta(n\\lg{n})$. So the worst-case running time of this algorithm is $\\Theta(n\\lg{n}) + \\Theta(n\\lg{n})$, which is still $\\Theta(n\\lg{n})$.","title":"2.3 Designing algorithms"},{"location":"02-Getting-Started/2.3-Designing-algorithms/#23-designing-algorithms","text":"","title":"2.3 Designing algorithms"},{"location":"02-Getting-Started/2.3-Designing-algorithms/#23-1","text":"","title":"2.3-1"},{"location":"02-Getting-Started/2.3-Designing-algorithms/#23-2","text":"MERGE(A, p, q, r) n1 = q - p + 1 n2 = r - q let L[1..n1] and R[1..n2] be new arrays for i = 1 to n1 L[i] = A[p + i - 1] for j = 1 to n2 R[j] = A[q + j] i = 1 j = 1 for k = p to r if i > n1: A[k] = R[j] j = j + 1 else if j > n2: A[k] = L[i] i = i + 1 else if L[i] <= R[j]: A[k] = L[i] i = i + 1 else A[k] = R[j] j = j + 1","title":"2.3-2"},{"location":"02-Getting-Started/2.3-Designing-algorithms/#23-3","text":"Because n is an exact power of 2, so the sequence of n is $2, 4, 8, 16, \\ldots, 2^k$. First, let's see the base case, the base case is $n = 2$. And we have $T(n) = 2 = 2\\lg{2}$. So the statement holds true for $n = 2$. Second, let's see the inductive step. We assume $T(n) = n\\lg{n}$ is true when $n = 2^k, k > 1$, and we want to prove it also holds true for the $n + 1$ element, which is $2^{k + 1}$. When $n = 2^{k + 1}$, we have: $$ \\begin{eqnarray} T(2^{k + 1}) &=& 2T(\\frac{2^{k + 1}}{2}) + 2^{k + 1} \\\\\\ &=& 2T(2^k) + 2^{k + 1} \\\\\\ &=& 2 * 2^k\\lg{2^k} + 2^{k + 1} \\\\\\ &=& 2^{k + 1}\\lg{2^k} + 2^{k + 1} \\\\\\ &=& 2^{k + 1}(\\lg{2^k} + 1) \\\\\\ &=& 2^{k + 1}(\\lg{2^k} + \\lg{2}) \\\\\\ &=& 2^{k + 1}\\lg{2^{k + 1}} \\end{eqnarray} $$ So $T(n) = n\\lg{n}$ is true for all $n = 2^k, k \\geq 1$.","title":"2.3-3"},{"location":"02-Getting-Started/2.3-Designing-algorithms/#23-4","text":"The pseudocode would be like this: INSERT(A, end) key = A[end] i = end - 1 while i > 0 and A[i] > key A[i + 1] = A[i] i = i - 1 A[i + 1] = key RECURSIVE-INSERTION-SORT(A, end) if end > 1: RECURSIVE-INSERTION-SORT(A, end - 1) INSERT(A, end) And here is the recurrence for the running time (let $I(n)$ denotes the running time of inserting $A[n]$ to $A[1..n - 1]$): $$ T(n) = \\begin{cases} \\Theta(1) & if\\ n = 1, \\\\\\ T(n - 1) + I(n) & if\\ n \\geq 2 \\end{cases} $$","title":"2.3-4"},{"location":"02-Getting-Started/2.3-Designing-algorithms/#23-5","text":"Pseudocode: ITERATIVE-BINARY-SEARCH(A, v) low = 1 high = n while low <= high middle = (low + high) / 2 if A[middle] < v low = middle + 1 else if A[middle] > v high = middle - 1 else return middle return NIL RECURSIVE-BINARY-SEARCH(A, v, low, high) if low <= high middle = (low + high) / 2 if A[middle] < v return RECURSIVE-BINARY-SEARCH(A, v, middle + 1, high) else if A[middle] > v return RECURSIVE-BINARY-SEARCH(A, v, low, middle - 1) else return middle return NIL From the pseudocode we can see either iterative or recursive binary search will halve the size of problem in each step when the middle element doesn't match the target value. So we have: $$ T(n) = \\begin{cases} C & if\\ n = 1, \\\\\\ T(n / 2) + C & if\\ n \\geq 2 \\end{cases} $$ And we can rewrite $T(n)$ like: $$ \\begin{eqnarray} T(n) &=& T(n / 2) + C \\\\\\ &=& T(n / 2^2) + C + C \\\\\\ &=& T(n / 2^3) + C + C + C \\\\\\ &=& ... \\\\\\ &=& T(1) + C\\lg{n} \\\\\\ &=& C(\\lg{n} + 1) \\\\\\ &=& \\Theta(\\lg{n}) \\end{eqnarray} $$ So the worst-case running time of binary search is $\\Theta(\\lg{n})$.","title":"2.3-5"},{"location":"02-Getting-Started/2.3-Designing-algorithms/#23-6","text":"No, we cannot. The running time of finding the position to insert the new element is $\\Theta(\\lg{n})$, but we still need $\\Theta(n)$ time to insert that element into the array. So the worst-case running time is still $\\Theta(n^2)$.","title":"2.3-6"},{"location":"02-Getting-Started/2.3-Designing-algorithms/#23-7","text":"TWO-SUM(S, x) MERGE-SORT(S) for i = 1 to S.length index = BINARY-SEARCH(S, x - S[i]) if index != NIL and index != i return true return false First we sort S by merge sort, and the running time is $\\Theta(n\\lg{n})$. Then we iterate S, for each element in S, if this element is one of the two elements whose sum is exactly x, then we know x minus this element should be also in S. We can use binary search to search it. If the search result is not NIL and not equal to i, then we return true. The running time of binary search is $\\Theta(\\lg{n})$, and in the worst-case, we will run it for $n$ times, and the running time is $\\Theta(n\\lg{n})$. So the worst-case running time of this algorithm is $\\Theta(n\\lg{n}) + \\Theta(n\\lg{n})$, which is still $\\Theta(n\\lg{n})$.","title":"2.3-7"},{"location":"02-Getting-Started/Problems/","text":"Problems 2-1 a We know that the worst-case running time of sorting $n$ elements with insertion sort is $\\Theta(n^2)$. So the running time of sorting $k$ elements is $\\Theta(k^2)$. And there are $\\frac{n}{k}$ sublists of length $k$, so the whole running time is $\\frac{n}{k} * \\Theta(k^2) = \\Theta(nk)$. b We can go back to look at figure 2.5 in the book. In original merge sort, there are $n$ elements at the bottom in recursion tree. In each step, current array will be divided into two subarrays, until the array only contains one element. But in the modified algorithm, this operation stops when current array contains less than $k$ elements. Thus, it stops after $\\lg{\\frac{n}{k}}$ recursions. So the height of recursion tree is $\\lg{\\frac{n}{k}} + 1$. Because each level contributes a cost of $\\Theta(n)$ merge operation, the total merge running time is $\\Theta(n)(\\lg{\\frac{n}{k}} + 1) = \\Theta(n\\lg{\\frac{n}{k}})$. c Given the modified algorithm runs in $\\Theta(nk + n\\lg{\\frac{n}{k}})$ worst-case time, we know $n\\lg{\\frac{n}{k}} \\leq n\\lg{n}$, so this part will not influence the upper bound running time. Thus $nk$ is responsible for the worst-case running time of the modified algorithm. And we can see if $k$ is larger than $\\lg{n}$, the modified algorithm will not have the same running time as the standard merge sort algorithm. So the largest value of $k$ is $\\lg{n}$. d The optimum value of $k$ is system dependent. A optimum value on one machine may not be optimum on another machine. 2-2 a We need to prove the array A' consists of the elements originally in A. b Loop invariant: At the start of each iteration of the for loop of lines 2-4, A[j] is the smallest in the subarray A[j..n] . Initialization : at the start of the iteration, j is n, and the subarray A[j..n] only contains one element, which is of course the smallest. Maintenance : if it is true before an iteration of the loop, we have A[j] is the smallest one in the subarray A[j..n]. Then we compare A[j - 1] and A[j], if A[j - 1] is smaller, we exchange A[j] with A[j - 1], thus A[j - 1] is the smallest one in the subarray A[j - 1..n]. So it remains true before the next iteration. Termination : it terminates when j equals to i. And when it terminates, we know A[i] is the smallest element in the subarray A[i..n]. So the loop invariant is correct. c Loop invariant: At the start of each iteration of the for loop of lines 1-4, the subarray A[1..i - 1] is in sorted order, and any element in the subarray A[1..i - 1] is not larger than any element in the subarray A[i..n] . Initialization : at the start of the iteration, i is 1, the subarray A[1..i - 1] is empty, the loop invariant is true. Maintenance : if it is true before an iteration of the loop, we have the subarray A[1..i - 1] is in sorted order. After the for loop in lines 2-4, A[i] is the smallest in the subarray A[i..n]. Because the elements in the subarray A[1..i - 1] is not larget than the elements in the subarray A[i..n], we know the subarray A[1..i] is still sorted. It remains true before the next iteration. Termination : it terminates when i is n. Substituting n for i in the wording of loop invariant, we have that the subarray A[1..n - 1] is in sorted order, and the elements in the subarray A[1..n - 1] is not larger than the elements in the subarray A[n..n], so the subarray A[1..n] is in sorted order. And the subarray A[1..n] is the entire array, we conclude that the entire array is sorted. Hence, the algorithm is correct. d The worst-case running time of bubblesort is approximately $n + (n - 1) + \\ldots + 3 = \\Theta(n^2)$. The worst-case running time of insertion sort is also $\\Theta(n^2)$, but the best-case running time of insertion sort is $\\Theta(n)$. For bubblesort, the best-case running time is still $\\Theta(n^2)$, because a best-case input could not reduce the cost of the for loop of lines 2-4. 2-3 a It's $\\Theta(n)$. b The pseudocode of the naive polynomial-evaluation algorithm: POLYNOMIAL-EVALUATION(A, x) y = 0 for i = 0 to n a = A[i] x_product = 1 for j = 1 to i x_product = x_product * x y = y + a * x_product The running time of this algorithm is $\\Theta(n^2)$. It's slower than the Horner's rule. c Initialization : at the start of the iteration of the for loop, i is n, and $y = \\sum_{k = 0}^{-1} a_{k + n + 1}{x^k}$, this is not a valid summation, so y is still 0. Thus the loop invariant is correct. Maintenance : at the start of the ith iteration, we have: $$y = \\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1}{x^k}$$ And after the ith iteration, we have: $$ \\begin{eqnarray} y &=& a_i + xy = a_i + x\\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1}{x^k} \\\\\\ &=& a_i + x(a_{i + 1}x^0 + a_{i + 2}x^1 + \\ldots + a_nx^{n - i - 1}) \\\\\\ &=& a_i + a_{i + 1}x^1 + a_{i + 2}x^2 + \\ldots + a_nx^{n - i} \\\\\\ &=& a_ix^0 + a_{i + 1}x^1 + a_{i + 2}x^2 + \\ldots + a_nx^{n - i} \\\\\\ &=& \\sum_{k = 0}^{n - i} a_{k + i}{x^k} \\\\\\ &=& \\sum_{k = 0}^{n - ((i - 1) + 1)} a_{k + (i - 1) + 1}{x^k} \\end{eqnarray} $$ So the loop invariant is still true after the end of ith iteration. Termination : when the for loop terminates, i is -1, and we replace i with -1 in the summation: $$y = \\sum_{k = 0}^{n - (-1 + 1)} a_{k + -1 + 1}{x^k} = \\sum_{k = 0}^{n} a_{k}{x^k}$$ So the loop invariant is correct. d At the end of the iteration of the for loop of lines 2-3, we have $y = \\sum_{k = 0}^{n} a_{k}{x^k}$, so it's correct. 2-4 a $\\langle 0, 4\\rangle, \\langle 1, 4\\rangle, \\langle 2, 3\\rangle, \\langle 2, 4\\rangle, \\langle 6, 1\\rangle$. b The array $\\lbrace n, \\ldots, 2, 1\\rbrace$ has the most inversions. It has: $$(n - 1) + (n - 2) + \\ldots + 1 = \\frac{n(n - 1)}{2}$$ c Let's go back to the pseudocode of insertion sort. We know the running time of insertion sort is: $$T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\sum_{j = 2}^{n} t_j + c_6\\sum_{j = 2}^{n} (t_j - 1) + c_7\\sum_{j = 2}^{n} (t_j - 1) + c_8(n - 1)$$ where $t_j$ is the number of times the while loop test in line 5 is executed for the jth outer for loop. We can see lines 6 and 7 will be executed when there is an inversion, let's denote $p_j$ the number of inversions in the subarray A[1..j]. So we have $p_j = t_j - 1$. When j is increasing, the number of inversions in the whole array A[1..n] is decreasing, but it doesn't create new inversions in eath iteration, since the relative order in subarray A[1..j] and A[j + 1..n] are not changed. So if we denote $m$ the number of inversions in the whole array A, we have $\\sum_{j = 2}^{n} p_j = \\sum_{j = 2}^{n} (t_j - 1) = m$. So: $$T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5(m + n - 1) + c_6m + c_7m + c_8(n - 1)$$ d NUMBER-OF-INVERSIONS(A) n = A.length return MERGE-SORT(A, 1, n) MERGE-SORT(A, p, r) inversions = 0 if p < r q = [(p + q) / 2] inversions += MERGE-SORT(A, p, q) inversions += MERGE-SORT(A, q + 1, r) inversions += MERGE(A, p, q, r) return inversions MERGE(A, p, q, r) n1 = q - p + 1 n2 = r - q let L[1..n1] and R[1..n2] be new arrays for i = 1 to n1 L[i] = A[p + i - 1] for j = 1 to n2 R[j] = A[q + j] i = 1 j = 1 inversions = 0 for k = p to r if i > n1: A[k] = R[j] j = j + 1 else if j > n2: A[k] = L[i] i = i + 1 else if L[i] <= R[j]: A[k] = L[i] i = i + 1 else A[k] = R[j] j = j + 1 inversions += n1 - i + 1","title":"Problems"},{"location":"02-Getting-Started/Problems/#problems","text":"","title":"Problems"},{"location":"02-Getting-Started/Problems/#2-1","text":"","title":"2-1"},{"location":"02-Getting-Started/Problems/#a","text":"We know that the worst-case running time of sorting $n$ elements with insertion sort is $\\Theta(n^2)$. So the running time of sorting $k$ elements is $\\Theta(k^2)$. And there are $\\frac{n}{k}$ sublists of length $k$, so the whole running time is $\\frac{n}{k} * \\Theta(k^2) = \\Theta(nk)$.","title":"a"},{"location":"02-Getting-Started/Problems/#b","text":"We can go back to look at figure 2.5 in the book. In original merge sort, there are $n$ elements at the bottom in recursion tree. In each step, current array will be divided into two subarrays, until the array only contains one element. But in the modified algorithm, this operation stops when current array contains less than $k$ elements. Thus, it stops after $\\lg{\\frac{n}{k}}$ recursions. So the height of recursion tree is $\\lg{\\frac{n}{k}} + 1$. Because each level contributes a cost of $\\Theta(n)$ merge operation, the total merge running time is $\\Theta(n)(\\lg{\\frac{n}{k}} + 1) = \\Theta(n\\lg{\\frac{n}{k}})$.","title":"b"},{"location":"02-Getting-Started/Problems/#c","text":"Given the modified algorithm runs in $\\Theta(nk + n\\lg{\\frac{n}{k}})$ worst-case time, we know $n\\lg{\\frac{n}{k}} \\leq n\\lg{n}$, so this part will not influence the upper bound running time. Thus $nk$ is responsible for the worst-case running time of the modified algorithm. And we can see if $k$ is larger than $\\lg{n}$, the modified algorithm will not have the same running time as the standard merge sort algorithm. So the largest value of $k$ is $\\lg{n}$.","title":"c"},{"location":"02-Getting-Started/Problems/#d","text":"The optimum value of $k$ is system dependent. A optimum value on one machine may not be optimum on another machine.","title":"d"},{"location":"02-Getting-Started/Problems/#2-2","text":"","title":"2-2"},{"location":"02-Getting-Started/Problems/#a_1","text":"We need to prove the array A' consists of the elements originally in A.","title":"a"},{"location":"02-Getting-Started/Problems/#b_1","text":"Loop invariant: At the start of each iteration of the for loop of lines 2-4, A[j] is the smallest in the subarray A[j..n] . Initialization : at the start of the iteration, j is n, and the subarray A[j..n] only contains one element, which is of course the smallest. Maintenance : if it is true before an iteration of the loop, we have A[j] is the smallest one in the subarray A[j..n]. Then we compare A[j - 1] and A[j], if A[j - 1] is smaller, we exchange A[j] with A[j - 1], thus A[j - 1] is the smallest one in the subarray A[j - 1..n]. So it remains true before the next iteration. Termination : it terminates when j equals to i. And when it terminates, we know A[i] is the smallest element in the subarray A[i..n]. So the loop invariant is correct.","title":"b"},{"location":"02-Getting-Started/Problems/#c_1","text":"Loop invariant: At the start of each iteration of the for loop of lines 1-4, the subarray A[1..i - 1] is in sorted order, and any element in the subarray A[1..i - 1] is not larger than any element in the subarray A[i..n] . Initialization : at the start of the iteration, i is 1, the subarray A[1..i - 1] is empty, the loop invariant is true. Maintenance : if it is true before an iteration of the loop, we have the subarray A[1..i - 1] is in sorted order. After the for loop in lines 2-4, A[i] is the smallest in the subarray A[i..n]. Because the elements in the subarray A[1..i - 1] is not larget than the elements in the subarray A[i..n], we know the subarray A[1..i] is still sorted. It remains true before the next iteration. Termination : it terminates when i is n. Substituting n for i in the wording of loop invariant, we have that the subarray A[1..n - 1] is in sorted order, and the elements in the subarray A[1..n - 1] is not larger than the elements in the subarray A[n..n], so the subarray A[1..n] is in sorted order. And the subarray A[1..n] is the entire array, we conclude that the entire array is sorted. Hence, the algorithm is correct.","title":"c"},{"location":"02-Getting-Started/Problems/#d_1","text":"The worst-case running time of bubblesort is approximately $n + (n - 1) + \\ldots + 3 = \\Theta(n^2)$. The worst-case running time of insertion sort is also $\\Theta(n^2)$, but the best-case running time of insertion sort is $\\Theta(n)$. For bubblesort, the best-case running time is still $\\Theta(n^2)$, because a best-case input could not reduce the cost of the for loop of lines 2-4.","title":"d"},{"location":"02-Getting-Started/Problems/#2-3","text":"","title":"2-3"},{"location":"02-Getting-Started/Problems/#a_2","text":"It's $\\Theta(n)$.","title":"a"},{"location":"02-Getting-Started/Problems/#b_2","text":"The pseudocode of the naive polynomial-evaluation algorithm: POLYNOMIAL-EVALUATION(A, x) y = 0 for i = 0 to n a = A[i] x_product = 1 for j = 1 to i x_product = x_product * x y = y + a * x_product The running time of this algorithm is $\\Theta(n^2)$. It's slower than the Horner's rule.","title":"b"},{"location":"02-Getting-Started/Problems/#c_2","text":"Initialization : at the start of the iteration of the for loop, i is n, and $y = \\sum_{k = 0}^{-1} a_{k + n + 1}{x^k}$, this is not a valid summation, so y is still 0. Thus the loop invariant is correct. Maintenance : at the start of the ith iteration, we have: $$y = \\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1}{x^k}$$ And after the ith iteration, we have: $$ \\begin{eqnarray} y &=& a_i + xy = a_i + x\\sum_{k = 0}^{n - (i + 1)} a_{k + i + 1}{x^k} \\\\\\ &=& a_i + x(a_{i + 1}x^0 + a_{i + 2}x^1 + \\ldots + a_nx^{n - i - 1}) \\\\\\ &=& a_i + a_{i + 1}x^1 + a_{i + 2}x^2 + \\ldots + a_nx^{n - i} \\\\\\ &=& a_ix^0 + a_{i + 1}x^1 + a_{i + 2}x^2 + \\ldots + a_nx^{n - i} \\\\\\ &=& \\sum_{k = 0}^{n - i} a_{k + i}{x^k} \\\\\\ &=& \\sum_{k = 0}^{n - ((i - 1) + 1)} a_{k + (i - 1) + 1}{x^k} \\end{eqnarray} $$ So the loop invariant is still true after the end of ith iteration. Termination : when the for loop terminates, i is -1, and we replace i with -1 in the summation: $$y = \\sum_{k = 0}^{n - (-1 + 1)} a_{k + -1 + 1}{x^k} = \\sum_{k = 0}^{n} a_{k}{x^k}$$ So the loop invariant is correct.","title":"c"},{"location":"02-Getting-Started/Problems/#d_2","text":"At the end of the iteration of the for loop of lines 2-3, we have $y = \\sum_{k = 0}^{n} a_{k}{x^k}$, so it's correct.","title":"d"},{"location":"02-Getting-Started/Problems/#2-4","text":"","title":"2-4"},{"location":"02-Getting-Started/Problems/#a_3","text":"$\\langle 0, 4\\rangle, \\langle 1, 4\\rangle, \\langle 2, 3\\rangle, \\langle 2, 4\\rangle, \\langle 6, 1\\rangle$.","title":"a"},{"location":"02-Getting-Started/Problems/#b_3","text":"The array $\\lbrace n, \\ldots, 2, 1\\rbrace$ has the most inversions. It has: $$(n - 1) + (n - 2) + \\ldots + 1 = \\frac{n(n - 1)}{2}$$","title":"b"},{"location":"02-Getting-Started/Problems/#c_3","text":"Let's go back to the pseudocode of insertion sort. We know the running time of insertion sort is: $$T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\sum_{j = 2}^{n} t_j + c_6\\sum_{j = 2}^{n} (t_j - 1) + c_7\\sum_{j = 2}^{n} (t_j - 1) + c_8(n - 1)$$ where $t_j$ is the number of times the while loop test in line 5 is executed for the jth outer for loop. We can see lines 6 and 7 will be executed when there is an inversion, let's denote $p_j$ the number of inversions in the subarray A[1..j]. So we have $p_j = t_j - 1$. When j is increasing, the number of inversions in the whole array A[1..n] is decreasing, but it doesn't create new inversions in eath iteration, since the relative order in subarray A[1..j] and A[j + 1..n] are not changed. So if we denote $m$ the number of inversions in the whole array A, we have $\\sum_{j = 2}^{n} p_j = \\sum_{j = 2}^{n} (t_j - 1) = m$. So: $$T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5(m + n - 1) + c_6m + c_7m + c_8(n - 1)$$","title":"c"},{"location":"02-Getting-Started/Problems/#d_3","text":"NUMBER-OF-INVERSIONS(A) n = A.length return MERGE-SORT(A, 1, n) MERGE-SORT(A, p, r) inversions = 0 if p < r q = [(p + q) / 2] inversions += MERGE-SORT(A, p, q) inversions += MERGE-SORT(A, q + 1, r) inversions += MERGE(A, p, q, r) return inversions MERGE(A, p, q, r) n1 = q - p + 1 n2 = r - q let L[1..n1] and R[1..n2] be new arrays for i = 1 to n1 L[i] = A[p + i - 1] for j = 1 to n2 R[j] = A[q + j] i = 1 j = 1 inversions = 0 for k = p to r if i > n1: A[k] = R[j] j = j + 1 else if j > n2: A[k] = L[i] i = i + 1 else if L[i] <= R[j]: A[k] = L[i] i = i + 1 else A[k] = R[j] j = j + 1 inversions += n1 - i + 1","title":"d"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/","text":"3.1 Asymptotic notation 3.1-1 According to the basic definition of $\\Theta$-notation, if $max(f(n), g(n)) = \\Theta(f(n) + g(n))$, then there exist positive constants $c_1$, $c_2$ and $n_0$ such that: $$0 \\leq c_1(f(n) + g(n)) \\leq max(f(n), g(n)) \\leq c_2(f(n) + g(n)),\\ for \\ all \\ n \\geq n_0$$ Because f(n) and g(n) are nongenative, so it's obvious that $max(f(n), g(n)) \\leq f(n) + g(n)$, so we can let $c_2$ be 1. And we also know that $max(f(n), g(n)) \\geq f(n)$ and $max(f(n), g(n)) \\geq g(n)$, so $2max(f(n), g(n)) \\geq f(n) + g(n)$, thus $max(f(n), g(n)) \\geq \\frac{f(n) + g(n)}{2}$. So we can let $c_1$ be $\\frac{1}{2}$. Because $0 \\leq \\frac{f(n) + g(n)}{2} \\leq max(f(n), g(n)) \\leq f(n) + g(n)$ is true for all n, so we can let $n_0$ be 1. So we've found positive constants $c_1$, $c_2$ and $n_0$, thus $max(f(n), g(n)) = \\Theta(f(n) + g(n))$. 3.1-2 According to Newton's generalised binomial theorem , we have: $$(n + a)^b = \\sum_{k = 0}^{\\infty} \\binom{b}{k}n^{b - k}a^k = n^b + bn^{b - 1}a + \\frac{b(b - 1)}{2!}n^{b - 2}a^2 + \\dots$$ Since $n^b$ grows faster than others, so $(n + a)^b = \\Theta(n^b)$. 3.1-3 The O-notation denotes a upper bound, so we can not say \"at least\". 3.1-4 $2^{n + 1} = O(2^n)$, because there exist positive constants c = 2 and $n_0 = 1$ such that $0 \\leq 2^{n + 1} \\leq 2 * 2^n\\ for\\ all\\ n \\geq n_0$. $2^{2n} \\neq O(2^n)$. Suppose $2^{2n} = O(2^n)$, then there exist positive constants c and $n_0$ such that $0 \\leq 2^{2n} \\leq c2^n\\ for\\ all\\ n \\geq n_0$, which means $0 \\leq 2^n \\leq c\\ for\\ all\\ n \\geq n_0$. No matter how big c is, $2^n$ will be bigger than c after a specific $n_1$, so you cannot find a $n_0$ like that. 3.1-5 If $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq f(n) \\leq c_1g(n)\\ for\\ all\\ n \\geq n_1$, and there exist positive constants $c_2$ and $n_2$ such that $0 \\leq c_2g(n) \\leq f(n)\\ for\\ all\\ n \\geq n_2$. We can choose $n_0 = max(n_1, n_2)$, and combine the two inequations we have $0 \\leq c_2g(n) \\leq f(n) \\leq c_1g(n)\\ for\\ all\\ n \\geq n_0$, which is the definition for $f(n) = \\Theta(g(n))$. So we proved if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$, then $f(n) = \\Theta(g(n))$. Now lets' suppose $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$ is not true (either one of them is wrong or both are wrong), but $f(n) = \\Theta(g(n))$ is ture. And let's prove this hypothesis is not correct. If we have $f(n) = \\Theta(g(n))$, we know there exist positive constants c and $n_0$ such that $0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n)\\ for\\ all\\ n \\geq n_0$. So we can separate it into two inequations: $$0 \\leq f(n) \\leq c_2g(n)\\ for\\ all\\ n \\geq n_0$$ $$0 \\leq c_1g(n) \\leq f(n)\\ for\\ all\\ n \\geq n_0$$ which are the definitions of $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$. So we proved the hypothesis is wrong, thus we proved the \"only if\" part. 3.1-6 It's similiar to 3.1-5, the worst-case and best-case running time denote the upper bound and lower bound, which are $O(g(n))$ and $\\Omega(g(n))$. So the proof is similar. 3.1-7 Let's let $f(n) = o(g(n))$ and $h(n) = \\omega(g(n))$. According to the definition, for any positive constants $c_1$, $c_2$, there exist positive constants $n_1$, $n_2$ such that: $$0 \\leq f(n) < c_1g(n)\\ for\\ all\\ n \\geq n_1$$ $$0 \\leq c_2g(n) < h(n)\\ for\\ all\\ n \\geq n_2$$ Suppose $o(g(n)) \\cap \\omega(g(n))$ is not empty, so at least we have one f(n) and one h(n) such that f(n) = h(n), because $c_1$ and $c_2$ are any positive constants, so we let $c_1 = c_2 = c$, and let $n_0 = max(n_1, n_2)$, so we have: $$0 \\leq f(n) < cg(n)\\ for\\ all\\ n \\geq n_0$$ $$0 \\leq cg(n) < f(n)\\ for\\ all\\ n \\geq n_0$$ which also means: $$0 \\leq cg(n) < f(n)\\ < cg(n)\\ for\\ all\\ n \\geq n_0$$ And we know the above inequation is impossible. So the hypothesis is wrong, thus $o(g(n)) \\cap \\omega(g(n))$ is empty. 3.1-8 $$\\begin{aligned} \\Omega(g(n, m)) = & \\lbrace f(n, m): \\text {there exist positive constants } c, n_0, \\text {and } m_0 \\text{ such that } \\\\ & 0 \\leq cg(n, m) \\leq f(n, m) \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0 \\rbrace \\end{aligned}$$ $$ \\begin{aligned} \\Theta(g(n, m)) = & \\lbrace f(n, m): \\text {there exist positive constants } c_1, c_2, n_0, \\text {and } m_0 \\text{ such that } \\\\ & 0 \\leq c_1g(n, m) \\leq f(n, m) \\leq c_2g(n, m) \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0 \\rbrace \\end{aligned} $$","title":"3.1 Asymptotic notation"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/#31-asymptotic-notation","text":"","title":"3.1 Asymptotic notation"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/#31-1","text":"According to the basic definition of $\\Theta$-notation, if $max(f(n), g(n)) = \\Theta(f(n) + g(n))$, then there exist positive constants $c_1$, $c_2$ and $n_0$ such that: $$0 \\leq c_1(f(n) + g(n)) \\leq max(f(n), g(n)) \\leq c_2(f(n) + g(n)),\\ for \\ all \\ n \\geq n_0$$ Because f(n) and g(n) are nongenative, so it's obvious that $max(f(n), g(n)) \\leq f(n) + g(n)$, so we can let $c_2$ be 1. And we also know that $max(f(n), g(n)) \\geq f(n)$ and $max(f(n), g(n)) \\geq g(n)$, so $2max(f(n), g(n)) \\geq f(n) + g(n)$, thus $max(f(n), g(n)) \\geq \\frac{f(n) + g(n)}{2}$. So we can let $c_1$ be $\\frac{1}{2}$. Because $0 \\leq \\frac{f(n) + g(n)}{2} \\leq max(f(n), g(n)) \\leq f(n) + g(n)$ is true for all n, so we can let $n_0$ be 1. So we've found positive constants $c_1$, $c_2$ and $n_0$, thus $max(f(n), g(n)) = \\Theta(f(n) + g(n))$.","title":"3.1-1"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/#31-2","text":"According to Newton's generalised binomial theorem , we have: $$(n + a)^b = \\sum_{k = 0}^{\\infty} \\binom{b}{k}n^{b - k}a^k = n^b + bn^{b - 1}a + \\frac{b(b - 1)}{2!}n^{b - 2}a^2 + \\dots$$ Since $n^b$ grows faster than others, so $(n + a)^b = \\Theta(n^b)$.","title":"3.1-2"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/#31-3","text":"The O-notation denotes a upper bound, so we can not say \"at least\".","title":"3.1-3"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/#31-4","text":"$2^{n + 1} = O(2^n)$, because there exist positive constants c = 2 and $n_0 = 1$ such that $0 \\leq 2^{n + 1} \\leq 2 * 2^n\\ for\\ all\\ n \\geq n_0$. $2^{2n} \\neq O(2^n)$. Suppose $2^{2n} = O(2^n)$, then there exist positive constants c and $n_0$ such that $0 \\leq 2^{2n} \\leq c2^n\\ for\\ all\\ n \\geq n_0$, which means $0 \\leq 2^n \\leq c\\ for\\ all\\ n \\geq n_0$. No matter how big c is, $2^n$ will be bigger than c after a specific $n_1$, so you cannot find a $n_0$ like that.","title":"3.1-4"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/#31-5","text":"If $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq f(n) \\leq c_1g(n)\\ for\\ all\\ n \\geq n_1$, and there exist positive constants $c_2$ and $n_2$ such that $0 \\leq c_2g(n) \\leq f(n)\\ for\\ all\\ n \\geq n_2$. We can choose $n_0 = max(n_1, n_2)$, and combine the two inequations we have $0 \\leq c_2g(n) \\leq f(n) \\leq c_1g(n)\\ for\\ all\\ n \\geq n_0$, which is the definition for $f(n) = \\Theta(g(n))$. So we proved if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$, then $f(n) = \\Theta(g(n))$. Now lets' suppose $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$ is not true (either one of them is wrong or both are wrong), but $f(n) = \\Theta(g(n))$ is ture. And let's prove this hypothesis is not correct. If we have $f(n) = \\Theta(g(n))$, we know there exist positive constants c and $n_0$ such that $0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n)\\ for\\ all\\ n \\geq n_0$. So we can separate it into two inequations: $$0 \\leq f(n) \\leq c_2g(n)\\ for\\ all\\ n \\geq n_0$$ $$0 \\leq c_1g(n) \\leq f(n)\\ for\\ all\\ n \\geq n_0$$ which are the definitions of $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$. So we proved the hypothesis is wrong, thus we proved the \"only if\" part.","title":"3.1-5"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/#31-6","text":"It's similiar to 3.1-5, the worst-case and best-case running time denote the upper bound and lower bound, which are $O(g(n))$ and $\\Omega(g(n))$. So the proof is similar.","title":"3.1-6"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/#31-7","text":"Let's let $f(n) = o(g(n))$ and $h(n) = \\omega(g(n))$. According to the definition, for any positive constants $c_1$, $c_2$, there exist positive constants $n_1$, $n_2$ such that: $$0 \\leq f(n) < c_1g(n)\\ for\\ all\\ n \\geq n_1$$ $$0 \\leq c_2g(n) < h(n)\\ for\\ all\\ n \\geq n_2$$ Suppose $o(g(n)) \\cap \\omega(g(n))$ is not empty, so at least we have one f(n) and one h(n) such that f(n) = h(n), because $c_1$ and $c_2$ are any positive constants, so we let $c_1 = c_2 = c$, and let $n_0 = max(n_1, n_2)$, so we have: $$0 \\leq f(n) < cg(n)\\ for\\ all\\ n \\geq n_0$$ $$0 \\leq cg(n) < f(n)\\ for\\ all\\ n \\geq n_0$$ which also means: $$0 \\leq cg(n) < f(n)\\ < cg(n)\\ for\\ all\\ n \\geq n_0$$ And we know the above inequation is impossible. So the hypothesis is wrong, thus $o(g(n)) \\cap \\omega(g(n))$ is empty.","title":"3.1-7"},{"location":"03-Growth-of-Functions/3.1-Asymptotic-notation/#31-8","text":"$$\\begin{aligned} \\Omega(g(n, m)) = & \\lbrace f(n, m): \\text {there exist positive constants } c, n_0, \\text {and } m_0 \\text{ such that } \\\\ & 0 \\leq cg(n, m) \\leq f(n, m) \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0 \\rbrace \\end{aligned}$$ $$ \\begin{aligned} \\Theta(g(n, m)) = & \\lbrace f(n, m): \\text {there exist positive constants } c_1, c_2, n_0, \\text {and } m_0 \\text{ such that } \\\\ & 0 \\leq c_1g(n, m) \\leq f(n, m) \\leq c_2g(n, m) \\text{ for all } n \\geq n_0 \\text{ or } m \\geq m_0 \\rbrace \\end{aligned} $$","title":"3.1-8"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/","text":"3.2 Standard notations and common functions 3.2-1 If f(n) and g(n) are monotonically increasing functions, then if $m \\leq n$, we have $f(m) \\leq f(n)$ and $g(m) \\leq g(n)$. So we get $f(m) + g(m) \\leq f(n) + g(n)$ when $m \\leq n$. So the function f(n) + g(n) is monotonically increasing. Let $m_1 = g(m)$ and $n_1 = g(n)$. Because $m_1 \\leq n_1$, so $f(m_1) \\leq f(n_1)$, so f(g(n)) is monotonically increasing. if f(n) and g(n) are nonnegative, because $f(m) \\leq f(n)$ and $g(m) \\leq g(n)$, so we can multiply the inequations and have $f(m) \\cdot g(m) \\leq f(n) \\cdot g(n)$. Thus $f(n) \\cdot g(n)$ is monotonically increasing. 3.2-2 $a^{\\log_bc} = a^{\\frac{\\log_ac}{\\log_ab}} = (a^{\\log_ac})^\\frac{1}{\\log_ab} = c^\\frac{1}{\\log_ab} = c^{\\log_ba}$. 3.2-3 According to Stirling's approximation , we have: $$\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n} \\leq n! \\leq en^{n + \\frac{1}{2}}e^{-n}$$ So: $$\\lg{(\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n})} \\leq \\lg{(n!)} \\leq \\lg{(en^{n + \\frac{1}{2}}e^{-n})}$$ Notice that: $$ \\begin{eqnarray} \\lg{(\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n})} &=& \\lg{(\\sqrt{2\\pi})} + \\lg{n^{n + \\frac{1}{2}}} + \\lg{e^{-n}} \\\\ &=& \\lg{(\\sqrt{2\\pi})} + (n + \\frac{1}{2})\\lg{n} - n\\lg{e} \\\\ &=& n(\\lg{n} - \\lg{e}) + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\ &\\geq& n(\\lg{n} - \\lg{\\sqrt{n}}) + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\text{ (when } n \\geq e^2 \\text{)} \\\\ &=& n\\lg{\\frac{n}{\\sqrt{n}}} + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\ &=& \\frac{1}{2}n\\lg{n} + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\ &\\geq& \\frac{1}{2}n\\lg{n} \\end{eqnarray} $$ And: $$ \\begin{eqnarray} \\lg{(en^{n + \\frac{1}{2}}e^{-n})} &=& \\lg{e} + \\lg{n^{n + \\frac{1}{2}}} + \\lg{e^{-n}} \\\\ &=& \\lg{e} + (n + \\frac{1}{2})\\lg{n} + \\lg{e^{-n}} \\\\ &=& n\\lg{n} + \\lg{e} + \\frac{1}{2}\\lg{n} + \\lg{e^{-n}} \\\\ &=& n\\lg{n} + \\lg{\\frac{e\\sqrt{n}}{e^n}} \\\\ &\\leq& n\\lg{n} + \\lg{1} \\text{ (when } n \\geq 1 \\text{)} \\\\ &=& n\\lg{n} \\end{eqnarray} $$ So now we get: $$\\frac{1}{2}n\\lg{n} \\leq \\lg{(n!)} \\leq n\\lg{n} \\text{ (when } n \\geq e^2 \\text{)}$$ which means: there exist positive constants $c_1 = \\frac{1}{2}$, $c_2 = 1$ and $n_0 = 8$ such that: $$0 \\leq c_1n\\lg{n} \\leq \\lg(n!) \\leq c_2n\\lg{n},\\ for \\ all \\ n \\geq n_0$$ So $\\lg{(n!)} = \\Theta(n\\lg{n})$. When $n \\geq 4$, we have: $$ \\begin{eqnarray} n! &=& n * (n - 1) * \\ldots * 4 * 3 * 2 * 1 \\\\ &=& n * (n - 1) * \\ldots * 2 * 2 * 3 * 2 * 1 \\\\ &>& 2 * 2 * \\ldots * 2 * 2 * 3 * 2 * 1 \\\\ &=& 2^n * 1 \\\\ &=& 2^n \\end{eqnarray} $$ So for any positive constant $c_1$, there exist positive constant $n_1 = 4$, such that $0 \\leq c_1(2^n) < n! \\ for\\ all\\ n \\geq n_1$. So $n! = w(2^n)$. When $n \\geq 2$, we have: $$ \\begin{eqnarray} n! &=& n * (n - 1) * \\ldots * 2 * 1 \\\\ &<& n * n * \\ldots * n * n \\\\ &=& n^n \\end{eqnarray} $$ So for any positive constant $c_2$, there exist positive constant $n_2 = 2$, such that $0 \\leq n! < c_2n^n \\ for\\ all\\ n \\geq n_2$. So $n! = o(n^n)$. 3.2-4 Suppose $\\lceil \\lg{n} \\rceil!$ is polynomially bounded, then there exist positive constants c, k and $n_0$ such that $0 \\leq \\lceil \\lg{n} \\rceil! \\leq cn^k$ for all $n \\geq n_0$. So $\\lg{(\\lceil \\lg{n} \\rceil!)} \\leq \\lg{(cn^k)}$. But: $$ \\begin{eqnarray} \\lg{(\\lceil \\lg{n} \\rceil!)} - \\lg{(cn^k)} &\\geq& \\lg{((\\lg{n})!)} - \\lg{(cn^k)} \\\\ &\\geq& \\frac{1}{2}\\lg{n} * \\lg{(\\lg{n})} - \\lg{(cn^k)} \\text{ (reuse the inequation in 3.2-3)} \\\\ &=& \\frac{1}{2}\\lg{n} * \\lg{(\\lg{n})} - \\lg{c} - k\\lg{n} \\\\ &=& \\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c} \\end{eqnarray} $$ Since both k and c are constants and $\\lg{n}$ is a monotonically increasing function, so $\\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c}$ is monotonically increasing. First we let $(\\frac{1}{2}\\lg{(\\lg{n})} - k) \\geq 1$ and we get $n \\geq 2^{2^{2k + 2}}$. So $\\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c} \\geq \\lg{n} - \\lg{c}$ when $n \\geq 2^{2^{2k + 2}}$. Then let $\\lg{n} - \\lg{c} \\geq 0$ and we get $n \\geq c$. So we find a $n_0 = max(2^{2^{2k + 2}}, c)$ such that $\\lg{(\\lceil \\lg{n} \\rceil!)} - \\lg{(cn^k)} \\geq 0 \\text{ for all } n \\geq n_0$. So we know that the hypothesis is not correct, thus $\\lceil \\lg{n} \\rceil!$ is not polynomially bounded. Now let's prove that $\\lceil \\lg{\\lg{n}} \\rceil!$ is polynomially bounded. $$ \\begin{eqnarray} \\lg{(\\lceil \\lg{\\lg{n}} \\rceil!)} - \\lg{(cn^k)} &\\leq& \\lceil \\lg{\\lg{n}} \\rceil\\lg{(\\lceil \\lg{\\lg{n}} \\rceil)} - \\lg{(cn^k)} \\text{ (reuse the inequation in 3.2-3)} \\\\ &=& \\lceil \\lg{\\lg{n}} \\rceil\\lg{(\\lceil \\lg{\\lg{n}} \\rceil)} - \\lg{c} - k\\lg{n} \\\\ &<& (\\lg{\\lg{n}} + 1)\\lg{(\\lg{\\lg{n}} + 1)} - \\lg{c} - k\\lg{n} \\\\ \\end{eqnarray} $$ Let $\\lg{n} = x, x > 0$, so $(\\lg{\\lg{n}} + 1)\\lg{(\\lg{\\lg{n}} + 1)} - \\lg{c} - k\\lg{n} = (\\lg{x} + 1)\\lg{(\\lg{x} + 1)} - \\lg{c} - kx = f(x)$. So $f'(x) = \\frac{\\lg{(\\lg{x} + 1)} + 1}{x} - k$. Let $g(x) = \\frac{\\lg{(\\lg{x} + 1)} + 1}{x}$, so $g'(x) = \\frac{\\frac{1}{\\lg{x} + 1} - (\\lg{(\\lg{x} + 1)} + 1)}{x^2}$. It's easy to see that g'(x) is a monotonically decreasing function and g'(1) = 0, so $g(x) \\leq 0 \\text{ on } [1, +\\infty)$. So g(x) is monotonically decreasing on $[1, +\\infty)$ and g(1) = 1. So $f'(x) \\leq 1 - k \\text{ on } [1, +\\infty)$. And for some constants k, $f'(x) \\leq 0$. Thus f(x) is also a monotonically decreasing function on $[1, +\\infty)$ for some constants k. And $f(1) = -\\lg{c} - k < 0$, so f(x) < 0 on $[1, +\\infty)$. So there exist positive constants c, k and $n_0 = 2$ such that $\\lg{(\\lceil \\lg{\\lg{n}} \\rceil!)} - \\lg{(cn^k)} < f(x) < 0 \\text{ for all } n \\geq n_0$. So $\\lceil \\lg{\\lg{n}} \\rceil! \\leq cn^k$. So $\\lceil \\lg{\\lg{n}} \\rceil!$ is polynomially bounded. 3.2-5 Here is the definition of $\\lg^*{n}$. So $\\lg{(\\lg^*{n})} = \\lg{(1 + \\lg^*{(\\lg{n})})}$. Let $\\lg^*{(\\lg{n})} = x$, so it's obvious $f(x) = \\lg{(1 + x)}$ grows slower than $g(x) = x$, so $\\lg^*{(\\lg{n})}$ is asymptotically larger. 3.2-6 $$ \\begin{eqnarray} \\phi^2 - \\phi - 1 &=& (\\frac{1 + \\sqrt{5}}{2})^2 - \\frac{1 + \\sqrt{5}}{2} - 1 \\\\ &=& \\frac{1 + 2\\sqrt{5} + 5}{4} - \\frac{2 + 2\\sqrt{5}}{4} - \\frac{4}{4} \\\\ &=& \\frac{6 + 2\\sqrt{5} - 2 - 2\\sqrt{5} - 4}{4} \\\\ &=& 0 \\end{eqnarray} $$ $$ \\begin{eqnarray} \\hat\\phi^2 - \\hat\\phi - 1 &=& (\\frac{1 - \\sqrt{5}}{2})^2 - \\frac{1 - \\sqrt{5}}{2} - 1 \\\\ &=& \\frac{1 - 2\\sqrt{5} + 5}{4} - \\frac{2 - 2\\sqrt{5}}{4} - \\frac{4}{4} \\\\ &=& \\frac{6 - 2\\sqrt{5} - 2 + 2\\sqrt{5} - 4}{4} \\\\ &=& 0 \\end{eqnarray} $$ So the golden ratio $\\phi$ and its conjugate $\\hat\\phi$ both satisfy the equation $x^2 = x + 1$. 3.2-7 Basis : let's show that the statement holds for i = 1 and i = 2. $\\frac{\\phi - \\hat\\phi}{\\sqrt{5}} = \\frac{\\frac{1 + \\sqrt{5}}{2} - \\frac{1 - \\sqrt{5}}{2}}{\\sqrt{5}} = \\frac{\\frac{1 + \\sqrt{5} - 1 + \\sqrt{5}}{2}}{\\sqrt{5}} = \\frac{\\frac{2\\sqrt{5}}{2}}{\\sqrt{5}} = 1 = F_1$ $\\frac{\\phi^2 - \\hat\\phi^2}{\\sqrt{5}} = \\frac{(\\frac{1 + \\sqrt{5}}{2})^2 - (\\frac{1 - \\sqrt{5}}{2})^2}{\\sqrt{5}} = \\frac{\\frac{1 + 2\\sqrt{5} + 5 - 1 + 2\\sqrt{5} - 5}{4}}{\\sqrt{5}} = \\frac{\\frac{4\\sqrt{5}}{4}}{\\sqrt{5}} = 1 = F_2$ Inductive step : if $F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}}$ holds, let's show $F_{i + 1} = \\frac{\\phi^{i + 1} - \\hat\\phi^{i + 1}}{\\sqrt{5}}$ holds. $$ \\begin{eqnarray} F_{i + 1} &=& F_i + F_{i - 1} \\\\ &=& \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}} + \\frac{\\phi^{i - 1} - \\hat\\phi^{i - 1}}{\\sqrt{5}} \\\\ &=& \\frac{\\phi^{i - 1}(\\phi + 1) - \\hat\\phi^{i - 1}(\\hat\\phi + 1)}{\\sqrt{5}} \\\\ &=& \\frac{\\phi^{i - 1}\\phi^2 - \\hat\\phi^{i - 1}\\hat\\phi^2}{\\sqrt{5}} \\text{( reuse the equation in 3.2-7)} \\\\ &=& \\frac{\\phi^{i + 1} - \\hat\\phi^{i + 1}}{\\sqrt{5}} \\end{eqnarray} $$ So $F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}}$ holds for all i. 3.2-8 According to the symmetry property, if $k\\ln{k} = \\Theta(n)$, then $n = \\Theta(k\\ln{k})$. So there exist positive constants $c_1$, $c_2$ and $n_0$ such that: $$0 \\leq c_1k\\ln{k} \\leq n \\leq c_2k\\ln{k} \\text{, for all } n \\geq n_0 \\text{ (1)}$$ So: $$0 \\leq \\ln{(c_1k\\ln{k})} \\leq \\ln{n} \\leq \\ln{(c_2k\\ln{k})}$$ $$0 \\leq \\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})} \\leq \\ln{n} \\leq \\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})} \\text{ (2)}$$ Let's divide equation 1 by equation 2, so we get: $$0 \\leq \\frac{c_1k\\ln{k}}{\\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})}} \\leq \\frac{n}{\\ln{n}} \\leq \\frac{c_2k\\ln{k}}{\\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})}}$$ Because: $$\\frac{n}{\\ln{n}} \\geq \\frac{c_1k\\ln{k}}{\\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})}} \\geq \\frac{c_1k\\ln{k}}{\\ln{k} + \\ln{k} + \\ln{k}} = \\frac{c_1}{3}k$$ $$\\frac{n}{\\ln{n}} \\leq \\frac{c_2k\\ln{k}}{\\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})}} \\leq \\frac{c_2k\\ln{k}}{\\ln{k}} = c_2k$$ So we have $\\frac{c_1}{3}k \\leq \\frac{n}{\\ln{n}} \\leq c_2k$. So there exist positive constants $c_3 = \\frac{c_1}{3}$, $c_4 = c_2$ and $n_1 = n_0$ such that: $$c_3k \\leq \\frac{n}{\\ln{n}} \\leq c_4k \\text{ for all } n \\geq n_1$$ So $\\frac{n}{\\ln{n}} = \\Theta(k)$, which also means $k = \\Theta(\\frac{n}{\\ln{n}})$.","title":"3.2 Standard notations and common functions"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-standard-notations-and-common-functions","text":"","title":"3.2 Standard notations and common functions"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-1","text":"If f(n) and g(n) are monotonically increasing functions, then if $m \\leq n$, we have $f(m) \\leq f(n)$ and $g(m) \\leq g(n)$. So we get $f(m) + g(m) \\leq f(n) + g(n)$ when $m \\leq n$. So the function f(n) + g(n) is monotonically increasing. Let $m_1 = g(m)$ and $n_1 = g(n)$. Because $m_1 \\leq n_1$, so $f(m_1) \\leq f(n_1)$, so f(g(n)) is monotonically increasing. if f(n) and g(n) are nonnegative, because $f(m) \\leq f(n)$ and $g(m) \\leq g(n)$, so we can multiply the inequations and have $f(m) \\cdot g(m) \\leq f(n) \\cdot g(n)$. Thus $f(n) \\cdot g(n)$ is monotonically increasing.","title":"3.2-1"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-2","text":"$a^{\\log_bc} = a^{\\frac{\\log_ac}{\\log_ab}} = (a^{\\log_ac})^\\frac{1}{\\log_ab} = c^\\frac{1}{\\log_ab} = c^{\\log_ba}$.","title":"3.2-2"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-3","text":"According to Stirling's approximation , we have: $$\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n} \\leq n! \\leq en^{n + \\frac{1}{2}}e^{-n}$$ So: $$\\lg{(\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n})} \\leq \\lg{(n!)} \\leq \\lg{(en^{n + \\frac{1}{2}}e^{-n})}$$ Notice that: $$ \\begin{eqnarray} \\lg{(\\sqrt{2\\pi}n^{n + \\frac{1}{2}}e^{-n})} &=& \\lg{(\\sqrt{2\\pi})} + \\lg{n^{n + \\frac{1}{2}}} + \\lg{e^{-n}} \\\\ &=& \\lg{(\\sqrt{2\\pi})} + (n + \\frac{1}{2})\\lg{n} - n\\lg{e} \\\\ &=& n(\\lg{n} - \\lg{e}) + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\ &\\geq& n(\\lg{n} - \\lg{\\sqrt{n}}) + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\text{ (when } n \\geq e^2 \\text{)} \\\\ &=& n\\lg{\\frac{n}{\\sqrt{n}}} + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\ &=& \\frac{1}{2}n\\lg{n} + \\lg{(\\sqrt{2\\pi})} + \\frac{1}{2}\\lg{n} \\\\ &\\geq& \\frac{1}{2}n\\lg{n} \\end{eqnarray} $$ And: $$ \\begin{eqnarray} \\lg{(en^{n + \\frac{1}{2}}e^{-n})} &=& \\lg{e} + \\lg{n^{n + \\frac{1}{2}}} + \\lg{e^{-n}} \\\\ &=& \\lg{e} + (n + \\frac{1}{2})\\lg{n} + \\lg{e^{-n}} \\\\ &=& n\\lg{n} + \\lg{e} + \\frac{1}{2}\\lg{n} + \\lg{e^{-n}} \\\\ &=& n\\lg{n} + \\lg{\\frac{e\\sqrt{n}}{e^n}} \\\\ &\\leq& n\\lg{n} + \\lg{1} \\text{ (when } n \\geq 1 \\text{)} \\\\ &=& n\\lg{n} \\end{eqnarray} $$ So now we get: $$\\frac{1}{2}n\\lg{n} \\leq \\lg{(n!)} \\leq n\\lg{n} \\text{ (when } n \\geq e^2 \\text{)}$$ which means: there exist positive constants $c_1 = \\frac{1}{2}$, $c_2 = 1$ and $n_0 = 8$ such that: $$0 \\leq c_1n\\lg{n} \\leq \\lg(n!) \\leq c_2n\\lg{n},\\ for \\ all \\ n \\geq n_0$$ So $\\lg{(n!)} = \\Theta(n\\lg{n})$. When $n \\geq 4$, we have: $$ \\begin{eqnarray} n! &=& n * (n - 1) * \\ldots * 4 * 3 * 2 * 1 \\\\ &=& n * (n - 1) * \\ldots * 2 * 2 * 3 * 2 * 1 \\\\ &>& 2 * 2 * \\ldots * 2 * 2 * 3 * 2 * 1 \\\\ &=& 2^n * 1 \\\\ &=& 2^n \\end{eqnarray} $$ So for any positive constant $c_1$, there exist positive constant $n_1 = 4$, such that $0 \\leq c_1(2^n) < n! \\ for\\ all\\ n \\geq n_1$. So $n! = w(2^n)$. When $n \\geq 2$, we have: $$ \\begin{eqnarray} n! &=& n * (n - 1) * \\ldots * 2 * 1 \\\\ &<& n * n * \\ldots * n * n \\\\ &=& n^n \\end{eqnarray} $$ So for any positive constant $c_2$, there exist positive constant $n_2 = 2$, such that $0 \\leq n! < c_2n^n \\ for\\ all\\ n \\geq n_2$. So $n! = o(n^n)$.","title":"3.2-3"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-4","text":"Suppose $\\lceil \\lg{n} \\rceil!$ is polynomially bounded, then there exist positive constants c, k and $n_0$ such that $0 \\leq \\lceil \\lg{n} \\rceil! \\leq cn^k$ for all $n \\geq n_0$. So $\\lg{(\\lceil \\lg{n} \\rceil!)} \\leq \\lg{(cn^k)}$. But: $$ \\begin{eqnarray} \\lg{(\\lceil \\lg{n} \\rceil!)} - \\lg{(cn^k)} &\\geq& \\lg{((\\lg{n})!)} - \\lg{(cn^k)} \\\\ &\\geq& \\frac{1}{2}\\lg{n} * \\lg{(\\lg{n})} - \\lg{(cn^k)} \\text{ (reuse the inequation in 3.2-3)} \\\\ &=& \\frac{1}{2}\\lg{n} * \\lg{(\\lg{n})} - \\lg{c} - k\\lg{n} \\\\ &=& \\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c} \\end{eqnarray} $$ Since both k and c are constants and $\\lg{n}$ is a monotonically increasing function, so $\\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c}$ is monotonically increasing. First we let $(\\frac{1}{2}\\lg{(\\lg{n})} - k) \\geq 1$ and we get $n \\geq 2^{2^{2k + 2}}$. So $\\lg{n}(\\frac{1}{2}\\lg{(\\lg{n})} - k) - \\lg{c} \\geq \\lg{n} - \\lg{c}$ when $n \\geq 2^{2^{2k + 2}}$. Then let $\\lg{n} - \\lg{c} \\geq 0$ and we get $n \\geq c$. So we find a $n_0 = max(2^{2^{2k + 2}}, c)$ such that $\\lg{(\\lceil \\lg{n} \\rceil!)} - \\lg{(cn^k)} \\geq 0 \\text{ for all } n \\geq n_0$. So we know that the hypothesis is not correct, thus $\\lceil \\lg{n} \\rceil!$ is not polynomially bounded. Now let's prove that $\\lceil \\lg{\\lg{n}} \\rceil!$ is polynomially bounded. $$ \\begin{eqnarray} \\lg{(\\lceil \\lg{\\lg{n}} \\rceil!)} - \\lg{(cn^k)} &\\leq& \\lceil \\lg{\\lg{n}} \\rceil\\lg{(\\lceil \\lg{\\lg{n}} \\rceil)} - \\lg{(cn^k)} \\text{ (reuse the inequation in 3.2-3)} \\\\ &=& \\lceil \\lg{\\lg{n}} \\rceil\\lg{(\\lceil \\lg{\\lg{n}} \\rceil)} - \\lg{c} - k\\lg{n} \\\\ &<& (\\lg{\\lg{n}} + 1)\\lg{(\\lg{\\lg{n}} + 1)} - \\lg{c} - k\\lg{n} \\\\ \\end{eqnarray} $$ Let $\\lg{n} = x, x > 0$, so $(\\lg{\\lg{n}} + 1)\\lg{(\\lg{\\lg{n}} + 1)} - \\lg{c} - k\\lg{n} = (\\lg{x} + 1)\\lg{(\\lg{x} + 1)} - \\lg{c} - kx = f(x)$. So $f'(x) = \\frac{\\lg{(\\lg{x} + 1)} + 1}{x} - k$. Let $g(x) = \\frac{\\lg{(\\lg{x} + 1)} + 1}{x}$, so $g'(x) = \\frac{\\frac{1}{\\lg{x} + 1} - (\\lg{(\\lg{x} + 1)} + 1)}{x^2}$. It's easy to see that g'(x) is a monotonically decreasing function and g'(1) = 0, so $g(x) \\leq 0 \\text{ on } [1, +\\infty)$. So g(x) is monotonically decreasing on $[1, +\\infty)$ and g(1) = 1. So $f'(x) \\leq 1 - k \\text{ on } [1, +\\infty)$. And for some constants k, $f'(x) \\leq 0$. Thus f(x) is also a monotonically decreasing function on $[1, +\\infty)$ for some constants k. And $f(1) = -\\lg{c} - k < 0$, so f(x) < 0 on $[1, +\\infty)$. So there exist positive constants c, k and $n_0 = 2$ such that $\\lg{(\\lceil \\lg{\\lg{n}} \\rceil!)} - \\lg{(cn^k)} < f(x) < 0 \\text{ for all } n \\geq n_0$. So $\\lceil \\lg{\\lg{n}} \\rceil! \\leq cn^k$. So $\\lceil \\lg{\\lg{n}} \\rceil!$ is polynomially bounded.","title":"3.2-4"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-5","text":"Here is the definition of $\\lg^*{n}$. So $\\lg{(\\lg^*{n})} = \\lg{(1 + \\lg^*{(\\lg{n})})}$. Let $\\lg^*{(\\lg{n})} = x$, so it's obvious $f(x) = \\lg{(1 + x)}$ grows slower than $g(x) = x$, so $\\lg^*{(\\lg{n})}$ is asymptotically larger.","title":"3.2-5"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-6","text":"$$ \\begin{eqnarray} \\phi^2 - \\phi - 1 &=& (\\frac{1 + \\sqrt{5}}{2})^2 - \\frac{1 + \\sqrt{5}}{2} - 1 \\\\ &=& \\frac{1 + 2\\sqrt{5} + 5}{4} - \\frac{2 + 2\\sqrt{5}}{4} - \\frac{4}{4} \\\\ &=& \\frac{6 + 2\\sqrt{5} - 2 - 2\\sqrt{5} - 4}{4} \\\\ &=& 0 \\end{eqnarray} $$ $$ \\begin{eqnarray} \\hat\\phi^2 - \\hat\\phi - 1 &=& (\\frac{1 - \\sqrt{5}}{2})^2 - \\frac{1 - \\sqrt{5}}{2} - 1 \\\\ &=& \\frac{1 - 2\\sqrt{5} + 5}{4} - \\frac{2 - 2\\sqrt{5}}{4} - \\frac{4}{4} \\\\ &=& \\frac{6 - 2\\sqrt{5} - 2 + 2\\sqrt{5} - 4}{4} \\\\ &=& 0 \\end{eqnarray} $$ So the golden ratio $\\phi$ and its conjugate $\\hat\\phi$ both satisfy the equation $x^2 = x + 1$.","title":"3.2-6"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-7","text":"Basis : let's show that the statement holds for i = 1 and i = 2. $\\frac{\\phi - \\hat\\phi}{\\sqrt{5}} = \\frac{\\frac{1 + \\sqrt{5}}{2} - \\frac{1 - \\sqrt{5}}{2}}{\\sqrt{5}} = \\frac{\\frac{1 + \\sqrt{5} - 1 + \\sqrt{5}}{2}}{\\sqrt{5}} = \\frac{\\frac{2\\sqrt{5}}{2}}{\\sqrt{5}} = 1 = F_1$ $\\frac{\\phi^2 - \\hat\\phi^2}{\\sqrt{5}} = \\frac{(\\frac{1 + \\sqrt{5}}{2})^2 - (\\frac{1 - \\sqrt{5}}{2})^2}{\\sqrt{5}} = \\frac{\\frac{1 + 2\\sqrt{5} + 5 - 1 + 2\\sqrt{5} - 5}{4}}{\\sqrt{5}} = \\frac{\\frac{4\\sqrt{5}}{4}}{\\sqrt{5}} = 1 = F_2$ Inductive step : if $F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}}$ holds, let's show $F_{i + 1} = \\frac{\\phi^{i + 1} - \\hat\\phi^{i + 1}}{\\sqrt{5}}$ holds. $$ \\begin{eqnarray} F_{i + 1} &=& F_i + F_{i - 1} \\\\ &=& \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}} + \\frac{\\phi^{i - 1} - \\hat\\phi^{i - 1}}{\\sqrt{5}} \\\\ &=& \\frac{\\phi^{i - 1}(\\phi + 1) - \\hat\\phi^{i - 1}(\\hat\\phi + 1)}{\\sqrt{5}} \\\\ &=& \\frac{\\phi^{i - 1}\\phi^2 - \\hat\\phi^{i - 1}\\hat\\phi^2}{\\sqrt{5}} \\text{( reuse the equation in 3.2-7)} \\\\ &=& \\frac{\\phi^{i + 1} - \\hat\\phi^{i + 1}}{\\sqrt{5}} \\end{eqnarray} $$ So $F_i = \\frac{\\phi^i - \\hat\\phi^i}{\\sqrt{5}}$ holds for all i.","title":"3.2-7"},{"location":"03-Growth-of-Functions/3.2-Standard-notations-and-common-functions/#32-8","text":"According to the symmetry property, if $k\\ln{k} = \\Theta(n)$, then $n = \\Theta(k\\ln{k})$. So there exist positive constants $c_1$, $c_2$ and $n_0$ such that: $$0 \\leq c_1k\\ln{k} \\leq n \\leq c_2k\\ln{k} \\text{, for all } n \\geq n_0 \\text{ (1)}$$ So: $$0 \\leq \\ln{(c_1k\\ln{k})} \\leq \\ln{n} \\leq \\ln{(c_2k\\ln{k})}$$ $$0 \\leq \\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})} \\leq \\ln{n} \\leq \\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})} \\text{ (2)}$$ Let's divide equation 1 by equation 2, so we get: $$0 \\leq \\frac{c_1k\\ln{k}}{\\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})}} \\leq \\frac{n}{\\ln{n}} \\leq \\frac{c_2k\\ln{k}}{\\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})}}$$ Because: $$\\frac{n}{\\ln{n}} \\geq \\frac{c_1k\\ln{k}}{\\ln{c_1} + \\ln{k} + \\ln{(\\ln{k})}} \\geq \\frac{c_1k\\ln{k}}{\\ln{k} + \\ln{k} + \\ln{k}} = \\frac{c_1}{3}k$$ $$\\frac{n}{\\ln{n}} \\leq \\frac{c_2k\\ln{k}}{\\ln{c_2} + \\ln{k} + \\ln{(\\ln{k})}} \\leq \\frac{c_2k\\ln{k}}{\\ln{k}} = c_2k$$ So we have $\\frac{c_1}{3}k \\leq \\frac{n}{\\ln{n}} \\leq c_2k$. So there exist positive constants $c_3 = \\frac{c_1}{3}$, $c_4 = c_2$ and $n_1 = n_0$ such that: $$c_3k \\leq \\frac{n}{\\ln{n}} \\leq c_4k \\text{ for all } n \\geq n_1$$ So $\\frac{n}{\\ln{n}} = \\Theta(k)$, which also means $k = \\Theta(\\frac{n}{\\ln{n}})$.","title":"3.2-8"},{"location":"03-Growth-of-Functions/Problems/","text":"Problems 3-1 Let $a_{max} = max(a_0, a_1, \\ldots, a_d)$. Because $a_d > 0$, so $a_{max} > 0$. Now let's prove there exists a constant $n_1$ such that $p(n) \\geq 0 \\text{ for all } n \\geq n_1$. Let $a_{absmax} = max(abs(a_0), abs(a_1), \\ldots, abs(a_{d - 1}))$. So $p(n) \\geq a_dn^d + \\sum_{i = 0}^{d - 1} (-a_{absmax}n^{d - 1}) = a_dn^d -da_{absmax}n^{d - 1} = n^{d - 1}(a_dn - da_{absmax})$. So $p(n) \\geq 0$ when $n \\geq \\lceil\\frac{da_{absmax}}{a_d}\\rceil$, $n_1 = \\lceil\\frac{da_{absmax}}{a_d}\\rceil$. a If $k \\geq d$, then $p(n) \\leq \\sum_{i = 0}^d a_{max}n^d = (d + 1)a_{max}n^d \\leq (d + 1)a_{max}n^k$. So there exist positive constants $c = (d + 1)a_{max}$ and $n_0 = max(1, n_1)$ such that $0 \\leq p(n) \\leq cn^k \\text{ for all } n \\geq n_0$. Thus $p(n) = O(n^k)$. b We know $p(n) \\geq n^{d - 1}(a_dn - da_{absmax})$. So $n^{d - 1}(a_dn - da_{absmax}) \\geq n^d$ when $n \\geq \\frac{da_{absmax}}{a_d - 1}$. Thus $p(n) \\geq n^d \\geq n^k$ when $n \\geq max(1, \\lceil \\frac{da_{absmax}}{a_d - 1} \\rceil)$. Let $n_2 = max(1, \\lceil \\frac{da_{absmax}}{a_d - 1} \\rceil)$, so there exist positive constants c = 1 and $n_0 = max(n_1, n_2)$ such that $0 \\leq cn^k \\leq p(n) \\text{ for all } n \\geq n_0$. So $p(n) = \\Omega(n^k)$. c Let $n_3 = (n_0 \\text{ in question a})$ and $n_4 = (n_0 \\text{ in question b})$. From the questions a and b we know there exist positive constants $c_1 = 1$, $c_2 = (d + 1)a_{max}$ and $n_0 = max(n_3, n_4)$ such that $0 \\leq c_1n^d \\leq p(n) \\leq c_2n^d \\text{ for all } n \\geq n_0$. Because k = d, so this also holds true for k, so $p(n) = \\Theta(n^k)$. d From question a we know $p(n) \\leq (d + 1)a_{max}n^d$, because k > d, let $(d + 1)a_{max}n^d < cn^k$, then we have $n > (\\frac{(d + 1)a_{max}}{c})^{\\frac{1}{k - d}}$. So for any positive constant c, we can find a positive constant $n_0 = \\lceil (\\frac{(d + 1)a_{max}}{c})^{\\frac{1}{k - d}} \\rceil + 1$ such that $0 \\leq p(n) < cn^k \\text{ for all } n \\geq n_0$. So $p(n) = o(n^k)$. e From question b we know $p(n) \\geq n^{d - 1}(a_dn - da_{absmax})$, let $f(n) = n^{d - 1}(a_dn - da_{absmax}) - cn^k = n^k(n^{d - k}(a_d - \\frac{da_{absmax}}{n}) - c)$, because k < d, so it's obvious that f(n) is a monotonically increasing function. First let $a_d - \\frac{da_{absmax}}{n} > \\frac{a_d}{2}$ and we get $n > \\frac{2da_{absmax}}{a_d}$. So $f(n) > n^k(n^{d - k}\\frac{a_d}{2} - c) \\text{ for all } n >= \\lceil \\frac{2da_{absmax}}{a_d} \\rceil + 1$. Then let $n^{d - k}\\frac{a_d}{2} - c > 0$ and we have $n > (\\frac{2c}{a_d})^{\\frac{1}{d - k}}$. So for any given positive constant c we can find a positive constant $n_0 = max(\\lceil \\frac{2da_{absmax}}{a_d} \\rceil + 1, \\lceil (\\frac{2c}{a_d})^{\\frac{1}{d - k}} \\rceil + 1)$ such that $0 \\leq cn^k < p(n) \\text{ for all } n \\geq n_0$. So $p(n) = w(n^k)$. 3-2 a Note that $n = 2^{\\lg{n}}$. So $\\lg^k{n} = (2^{\\lg{\\lg{n}}})^k = 2^{k\\lg{\\lg{n}}}$, $n^\\epsilon = (2^{\\lg{n}})^{\\epsilon} = 2^{\\epsilon\\lg{n}}$. It's obvious that $\\epsilon\\lg{n}$ grows faster than $k\\lg{\\lg{n}}$. Let $\\lg{n} = x, x > 0$, so $k\\lg{\\lg{n}} = k\\lg{x}$, $\\epsilon\\lg{n} = \\epsilon{x}$. Let $f(x) = \\epsilon{x} - k\\lg{x}$, so $f'(x) = \\epsilon - \\frac{k}{x}$. Because $\\epsilon > 0$ and $k \\geq 1$, we have $f'(x) >= 0 \\text{ when } x \\geq \\frac{k}{\\epsilon}$. So f(x) is a monotonically increasing function when $x \\geq \\frac{k}{\\epsilon}$. In order to solve $\\epsilon{x} - k\\lg{x} > 0$, we only have to solve $\\frac{x}{\\lg{x}} > \\frac{k}{\\epsilon}$, since $\\lim_{x \\to +\\infty} \\frac{x}{\\lg{x}} = +\\infty$, so there exists a constant $x_0$ such that $\\frac{x_0}{\\lg{x_0}} > \\frac{k}{\\epsilon}$, so $f(x_0) > 0$. So we can find a constant $x_0$ such that $f(x) \\geq 0 \\text{ for all } x \\geq x_0$. Thus $\\epsilon\\lg{n} \\geq k\\lg{\\lg{n}} \\text{ for all } n \\geq 2^{x_0}$. Therefore we proved there exist positive constants c = 1 and $n_0 = 2^{x_0}$ such that $0 \\leq \\lg^k{n} \\leq n^{\\epsilon} \\text{ for all } n \\geq n_0$. So $\\lg^k{n} = O(n^{\\epsilon})$. Now let's compare $\\lg^k{n}$ and $cn^{\\epsilon}$. Similarly, $cn^{\\epsilon} = 2^{\\lg{c}}(2^{\\lg{n}})^{\\epsilon} = 2^{\\lg{c} + \\epsilon\\lg{n}}$. So let $\\lg{n} = x$, so $\\lg{c} + \\epsilon\\lg{n} - k\\lg{\\lg{n}} = \\lg{c} + \\epsilon{x} - k\\lg{x}$. Let $g(x) = \\epsilon{x} - k\\lg{x} + \\lg{c}$. And $g'(x) = \\epsilon - \\frac{k}{x}$, $g'(x) >= 0 \\text{ when } x \\geq \\frac{k}{\\epsilon}$. So g(x) is a monotonically increasing function when $x \\geq \\frac{k}{\\epsilon}$. In order to solve $g(x) > 0$, we need to solve $\\epsilon{x} - k\\lg{x} > -\\lg{c}$. Namely, for any given positive constant c, we need to find a $x_0$ such that $g(x_0)$ is greater than $-\\lg{c}$. Notice that $\\epsilon{x} - k\\lg{x} = f(x)$ and f(x) is a a monotonically increasing function. For a given constant, we can find a $x_0$ such that $f(x_0)$ is greater than that constant. So, for any given constant c, we can find a $x_0$ such that $g(x_0) > 0$. Thus, for any positive constant c, there exists positive constant $n_0 = 2^{x_0}$ such that $0 \\leq \\lg^k{n} < cn^{\\epsilon} \\text{ for all } n \\geq n_0$. So $\\lg^k{n} = o(n^{\\epsilon})$. Since $\\lg^k{n} = o(n^{\\epsilon})$, then $\\lg^k{n}$ could not be $\\Omega(n^{\\epsilon})$, $w(n^{\\epsilon})$, $\\Theta(n^{\\epsilon})$. b $n^k = (2^{\\lg{n}})^k = 2^{k\\lg{n}}$, $c^n = (2^{\\lg{c}})^n = 2^{n\\lg{c}}$. So it's also obvious that $n\\lg{c}$ grows faster than $k\\lg{n}$. Let $f(n) = n\\lg{c} - k\\lg{n}$. Because $c > 1$, so $\\lg{c} > 0$. Thus we have the same function $f(x)$ defined in question a. So similarly, we know $n^k = O(c^n)$. Now let's compare $n^k$ and $bc^n$. $bc^n = 2^{\\lg{b}}(2^{\\lg{c}})^n = 2^{\\lg{b} + n\\lg{c}}$. Let $g(n) = \\lg{b} + n\\lg{c} - k\\lg{n}$, and again we have the same function g(x) defined in question a. So $n^k = o(c^n)$. c Let's compare $\\sqrt{n}$ and $cn^{\\sin{n}}$. $\\sqrt{n} = (2^{\\lg{n}})^{\\frac{1}{2}} = 2^{\\frac{1}{2}\\lg{n}}$, $cn^{\\sin{n}} = 2^{\\lg{c}}(2^{\\lg{n}})^{\\sin{n}} = 2^{\\sin{n}\\lg{n} + \\lg{c}}$. Let $f(n) = \\sin{n}\\lg{n} + \\lg{c} - \\frac{1}{2}\\lg{n} = (\\sin{n} - \\frac{1}{2})\\lg{n} + \\lg{c}$. So the question is: does there exist a positive constant c (or for any positive constant c), there exists a positive constant $n_0$, such that f(n) > 0 (or f(n) < 0) for all $n \\geq n_0$? First let's check $f(n) > 0$, nomatter how big c is, we can find a $n_1$ such that $\\lg{n_1} > \\lg{c}$, since $-\\frac{3}{2} \\leq \\sin{n} - \\frac{1}{2} \\leq \\frac{1}{2}$. So there exists a $n_2 \\geq n_1$ such that $\\sin{n_2} - \\frac{1}{2} = -1$, so $f(n_2) < 0$. So for any given constant c, there doesn't exist a constant $n_0$ such that f(n) > 0 for all $n \\geq n_0$. Similarly, for $f(n) < 0$, no matter how small c is, we can find a $n_1$ such that $\\lg{n_1} > 2abs(\\lg{c})$, and there exists a $n_2 \\geq n_1$ such that $\\sin{n_2} - \\frac{1}{2} = \\frac{1}{2}$, so $f(n_2) > 0$. So for any given constant c, there doesn't exist a constant $n_0$ such that f(n) < 0 for all $n \\geq n_0$. Thus, we cannot compare which grows faster. d It's obvious that there exist a positive constant c = 1 and $n_0 = 1$ such that $0 \\leq c2^{\\frac{n}{2}} \\leq 2^n \\text{ for all } n \\geq n_0$. So $2^n = \\Omega(2^{\\frac{n}{2}})$. Now let's compare $2^n$ and $c2^{\\frac{n}{2}}$. $c2^{\\frac{n}{2}} = 2^{\\lg{c}}2^{\\frac{n}{2}} = 2^{\\frac{n}{2} + \\lg{c}}$. Let $f(n) = n - (\\frac{n}{2} + \\lg{c}) = \\frac{n}{2} - \\lg{c}$. Let $f(n) > 0$, we have $n > 2\\lg{c}$. So for any constant c, there exists a positive constant $n_0 = 2\\lg{c} + 1$ such that $2^n > c2^{\\frac{n}{2}} \\text{ for all } n >= n_0$. So $2^n = w(2^{\\frac{n}{2}})$. e $n^{\\lg{c}} = (2^{\\lg{n}})^{\\lg{c}} = 2^{\\lg{c}\\lg{n}}$, $c^{\\lg{n}} = (2^{\\lg{c}})^{\\lg{n}} = 2^{\\lg{c}\\lg{n}}$, so $n^{\\lg{c}} = c^{\\lg{n}}$, thus there exist positive constants $b_1 = 1$ and $n_1 = 1$ such that $0 \\leq n^{\\lg{c}} \\leq b_1c^{\\lg{n}} \\text{ for all } n \\geq n_1$, and there exist positive constants $b_2 = 1$ and $n_2 = 1$ such that $0 \\leq b_2c^{\\lg{n}} \\leq n^{\\lg{c}} \\text{ for all } n \\geq n_2$, and there exist positive constants $b_3 = 1$, $b_4 = 1$ and $n_3 = 1$ such that $0 \\leq b_3c^{\\lg{n}} \\leq n^{\\lg{c}} \\leq b_4c^{\\lg{n}} \\text{ for all } n \\geq n_3$. So $n^{\\lg{c}} = O(c^{\\lg{n}})$, $n^{\\lg{c}} = \\Omega(c^{\\lg{n}})$, $n^{\\lg{c}} = \\Theta(c^{\\lg{n}})$. Since $n^{\\lg{c}}$ and $c^{\\lg{n}}$ are the same functions, so for any positive constant b, we cannot find a positive constant $n_0$ such that $n^{\\lg{c}} < bc^{\\lg{n}}$ or $n^{\\lg{c}} > bc^{\\lg{n}}$ for all $n \\geq n_0$. f In question 3.2-3, we've already proved that $\\lg{(n!)} = \\Theta(n\\lg{n})$, notice that $\\lg(n^n) = n\\lg{n}$, so $\\lg{(n!)} = \\Theta(\\lg{n^n})$. And $\\lg{(n!)} = O(\\lg{n^n})$, $\\lg{(n!)} = \\Omega(\\lg{n^n})$. Summary A B O o $\\Omega$ w $\\Theta$ $\\lg^k{n}$ $n^{\\epsilon}$ yes yes no no no $n^k$ $c^n$ yes yes no no no $\\sqrt{n}$ $n^{\\sin{n}}$ no no no no no $2^n$ $2^{\\frac{n}{2}}$ no no yes yes no $n^{\\lg{c}}$ $c^{\\lg{n}}$ yes no yes no yes $\\lg(n!)$ $\\lg(n^n)$ yes no yes no yes 3-3 a First, let's compare $2^{2^{n + 1}}$ and $2^{2^n}$, it's easy to see $2^{2^n} = O(2^{2^{n + 1}})$. But could $2^{2^n} = \\Omega(2^{2^{n + 1}})$? If it's true, then there exist a positive constant c and $n_0$ such that $0 \\leq c2^{2^{n + 1}} \\leq 2^{2^n}$ for all $n \\geq n_0$. But $\\frac{2^{2^n}}{c2^{2^{n + 1}}} = \\frac{1}{c2^{2^{n + 1} - 2^n}} = \\frac{1}{c2^{2^n}}$. No matter how small c is, $2^{2^n}$ will be greater than $\\frac{1}{c}$. So $2^{2^n} = \\Omega(2^{2^{n + 1}})$ could not be true. Then let's compare $n!$ and $e^n$. We know $n! = 2^{\\lg{(n!)}}$, and $e^n = (2^{\\lg{e}})^n = 2^{n\\lg{e}}$. In question 3.2-3 we know $\\lg{(n!)} = \\Theta(n\\lg{n})$, so $\\lg{(n!)}$ grows faster than $n\\lg{e}$. So $n!$ grows faster than $e^n$, thus $n! = \\Omega(e^n)$. And it's obvious $(n + 1)! = \\Omega(n!)$. Notice that $\\frac{n!}{c(n + 1)!} = \\frac{1}{c(n + 1)}$, which will eventually smaller than 1. So $n!$ could not be $\\Omega((n + 1)!)$. And what about $(n + 1)!$ and $2^{2^n}$? $(n + 1)! = 2^{\\lg{(n + 1)!}} = 2^{\\Theta((n + 1)\\lg{(n + 1)})} = 2^{O((n + 1)^2)} = 2^{O(n^2)}$. So $2^{2^{n}}$ grows faster, $2^{2^n} = \\Omega((n + 1)!)$. Since $e^n = (\\frac{e}{2})^n2^n$, and $(\\frac{e}{2})^n$ grows faster than $n$, so $e^n$ grows faster than $n2^n$, $e^n = \\Omega(n2^n)$. And it's easy to see that $n2^n = \\Omega(2^n)$, $2^n = \\Omega(\\frac{3}{2})^n$. $(\\lg{n})^{\\lg{n}} = (2^{\\lg{\\lg{n}}})^{\\lg{n}} = 2^{\\lg{n}\\lg{\\lg{n}}}$. And $n^{\\lg{\\lg{n}}} = (2^{\\lg{n}})^{\\lg{\\lg{n}}} = 2^{\\lg{n}\\lg{\\lg{n}}}$. So $(\\lg{n})^{\\lg{n}} = n^{\\lg{\\lg{n}}}$. And $(\\frac{3}{2})^n = (2^{\\lg{\\frac{3}{2}}})^n = 2^{n\\lg{\\frac{3}{2}}}$. Let $\\lg{n} = k$, so $\\lg{n}\\lg{\\lg{n}} = k\\lg{k}$, $n\\lg{\\frac{3}{2}} = 2^k\\lg{\\frac{3}{2}}$, so we can see $2^k\\lg{\\frac{3}{2}}$ grows faster. Thus $(\\frac{3}{2})^n = \\Omega((\\lg{n})^{\\lg{n}})$. According to Stirling's approximation , we know $n! = \\Theta(n^{n + \\frac{1}{2}}e^{-n})$. So $(\\lg{n})! = \\Theta((\\lg{n})^{\\lg{n} + \\frac{1}{2}}e^{-\\lg{n}}) = \\Theta((\\lg{n})^{\\lg{n}}\\frac{\\sqrt{\\lg{n}}}{e^{\\lg{n}}}) = O((\\lg{n})^{\\lg{n}})$, since $e^x$ grows much faster than $\\sqrt{x}$. So $(\\lg{n})^{\\lg{n}} = \\Omega((\\lg{n})!)$. Let $\\lg{n} = x$, so $n = 2^x$, $n^3 = 2^{3x}$. $(\\lg{n})! = \\Theta((\\lg{n})^{\\lg{n}}\\frac{\\sqrt{\\lg{n}}}{e^{\\lg{n}}}) = \\Theta(x^{x + \\frac{1}{2}}e^{-x}) = \\Theta(e^{x(\\ln{x} - 1) + \\frac{1}{2}\\ln{x}})$. And $2^{3x} = e^{(3\\ln{2})x}$. So $(\\lg{n})!$ grows faster than $n^3$. $4^{\\lg{n}} = (2^2)^{\\lg{n}} = (2^{\\lg{n}})^2 = n^2$, so $n^2 = 4^{\\lg{n}}$. And it's obvious $n^3 = \\Omega(n^2)$, $n^2 = \\Omega(n\\lg{n})$. And in question 3.2-3 we already proved $\\lg({n!}) = \\Theta(n\\lg{n})$. And it's easy to know $n\\lg{n} = \\Omega(n)$, $n = 2^{\\lg{n}}$. Similarly, $(\\sqrt{2})^{\\lg{n}} = (2^{\\lg{n}})^{\\frac{1}{2}} = \\sqrt{n}$, so $n = \\Omega({(\\sqrt{2})^{\\lg{n}}})$. $\\sqrt{n} = n^{\\frac{1}{2}} = (2^{\\lg{n}})^{\\frac{1}{2}} = 2^{\\frac{1}{2}\\lg{n}}$, it grows faster than $2^{\\sqrt{2\\lg{n}}}$, so $(\\sqrt{2})^{\\lg{n}} = \\Omega(2^{\\sqrt{2\\lg{n}}})$. $\\lg^2{n} = (2^{\\lg{\\lg{n}}})^2 = 2^{2\\lg{\\lg{n}}}$, in question 3-2 we know $\\lg^k{n} = o(n^{\\epsilon})$ for $k \\geq 1$ and $\\epsilon > 0$, so here we have $k = 1$, $\\epsilon = \\frac{1}{2}$, so $\\sqrt{2\\lg{n}}$ grows faster than $2\\lg{\\lg{n}}$, so $2^{\\sqrt{2\\lg{n}}} = \\Omega(\\lg^2{n})$. And $\\lg^2{n} = \\Omega(\\ln{n})$, $\\ln{n} = \\Omega(\\sqrt{\\lg{n}})$. Then let's compare $\\sqrt{\\lg{n}}$ and $\\ln{\\ln{n}}$. $\\sqrt{\\lg{n}} = \\sqrt{\\frac{\\ln{n}}{\\ln{2}}}$, from previous proof, we know $\\sqrt{\\frac{\\ln{n}}{\\ln{2}}}$ grows faster than $\\ln{\\ln{n}}$. So $\\sqrt{\\lg{n}} = \\Omega(\\ln{\\ln{n}})$. $\\ln{\\ln{n}} = 2^{\\lg{\\ln{\\ln{n}}}}$, and according to the definition of Iterated logarithm , we can see $\\lg{\\ln{\\ln{n}}}$ grows faster than $\\lg^*{n}$. So $\\ln{\\ln{n}} = \\Omega(2^{\\lg^*{n}})$. And $2^{\\lg^*{n}} = \\Omega(\\lg^*{n})$. Since $\\lg^*{n} = 1 + \\lg^*{\\lg{n}}$, so $\\lg^*{\\lg{n}} = \\Theta(\\lg^*{n})$. In question 3.2-5 we proved that $\\lg^*{(\\lg{n})}$ is asymptotically larger than $\\lg{(\\lg^*{n})}$. So $\\lg^*{(\\lg{n})} = \\Omega(\\lg{(\\lg^*{n})})$. $n^{\\frac{1}{\\lg{n}}} = (2^{\\lg{n}})^{\\frac{1}{\\lg{n}}} = 2$, so $n^{\\frac{1}{\\lg{n}}} = \\Theta(1)$. Summary $g_1 = 2^{2^{n + 1}}$ $g_2 = 2^{2^n}$ $g_3 = (n + 1)!$ $g_4 = n!$ $g_5 = e^n$ $g_6 = n2^n$ $g_7 = 2^n$ $g_8 = (\\frac{3}{2})^n$ $g_9 = (\\lg{n})^{\\lg{n}}$ $g_{10} = n^{\\lg{\\lg{n}}}, g_9 = \\Theta(g_{10})$ $g_{11} = (\\lg{n})!$ $g_{12} = n^3$ $g_{13} = n^2$ $g_{14} = 4^{\\lg{n}}, g_{13} = \\Theta(g_{14})$ $g_{15} = n\\lg{n}$ $g_{16} = \\lg({n!}), g_{15} = \\Theta(g_{16})$ $g_{17} = n$ $g_{18} = 2^{\\lg{n}}, g_{17} = \\Theta(g_{18})$ $g_{19} = (\\sqrt{2})^{\\lg{n}}$ $g_{20} = 2^{\\sqrt{2\\lg{n}}}$ $g_{21} = \\lg^2{n}$ $g_{22} = \\ln{n}$ $g_{23} = \\sqrt{\\lg{n}}$ $g_{24} = \\ln{\\ln{n}}$ $g_{25} = 2^{\\lg^*{n}}$ $g_{26} = \\lg^*{n}$ $g_{27} = \\lg^*{\\lg{n}}, g_{26} = \\Theta(g_{27})$ $g_{28} = \\lg{(\\lg^*{n})}$ $g_{29} = n^{\\frac{1}{\\lg{n}}}$ $g_{30} = 1, g_{29} = \\Theta(g_{30})$ b How do we find a function like this? In question 3-2, we know that $n^{\\sin{n}}$ is neither $o(\\sqrt{n})$ nor $w(\\sqrt{n})$. We can use the feature of $\\sin{n}$. So we can build a function that can be quite big and quite small. And it has to be not smaller than the largest function in $g_i(n)$ sometimes, so we can simply have a function like $n2^{2^{n + 1}}$, which is $\\Omega(g_i(n))$. Then we multiply $|\\sin{n}|$, so the function is $|\\sin{n}|n2^{2^{n + 1}}$, which is neither $O(g_i(n))$ nor $\\Omega(g_i(n))$. 3-4 a This is not true. If $g(n) = O(f(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq g(n) \\leq c_1f(n) \\text{ for all } n \\geq n_1$, since $f(n) = O(g(n))$, so there also exist positive constants $c_2$ and $n_2$ such that $0 \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_2$, so we have $0 \\leq \\frac{1}{c_1}g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq max(n_1, n_2)$, which means $f(n) = \\Theta(g(n))$. This is not 100% true. b This is not true. Let $f(n) = n$ and $g(n) = \\lg{n}$, it's easy to see $f(n) + g(n) = \\Theta(n) \\neq \\Theta(min(f(n), g(n)))$. c This is true. Because $f(n) = O(g(n))$, so there exist positive constants c and $n_0$ such that $0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0$. So: $$ \\begin{eqnarray} 0 &\\leq& \\lg{(f(n))} \\\\ &\\leq& \\lg{(cg(n))} \\\\ &=& \\lg{c} + \\lg{(g(n))} \\\\ &=& \\lg{c} * 1 + \\lg{(g(n))} \\\\ &\\leq& \\lg{c} * \\lg{(g(n))} + \\lg{(g(n))} \\\\ &=& (\\lg{c} + 1)\\lg{(g(n))} \\end{eqnarray} $$ So we find a positive constant $c_1 = \\lg{c} + 1$ such that $0 \\leq \\lg{(f(n))} \\leq c_1\\lg{(g(n))} \\text{ for all } n \\geq n_0$, so $\\lg{(f(n))} = O(\\lg{(g(n))}$. d This is not true. If $g(n) = O(f(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq g(n) \\leq c_1f(n) \\text{ for all } n \\geq n_1$. Suppose $2^{f(n)} = O(2^{g(n)})$, then there exist positive constants $c_2$ and $n_2$ such that $0 \\leq 2^{f(n)} \\leq {c_2}2^{g(n)} \\text{ for all } n \\geq n_2$. And ${c_2}2^{g(n)} = 2^{\\lg{c_2} + g(n)}$. So we have $f(n) \\leq \\lg{c_2} + g(n)$. But this is not always true, let $f(n) = n + \\lg{n}$ and $g(n) = n$, so $f(n) \\leq 2g(n) \\text{ for all } n \\geq 1$. But for any given positive constant $c_2$, we can always find a positive constant $n_3$ such that $n + \\lg{n} > \\lg{c_2} + n \\text{ for all } n \\geq n_3$. So $2^{f(n)} \\neq O(2^{g(n)})$ in this situation. e This is not true. Let $f(n) = \\frac{1}{n}$, if $f(n) = O((f(n))^2)$, then there exist positive constants c and $n_0$ such that $0 \\leq \\frac{1}{n} \\leq \\frac{c}{n^2} \\text{ for all } n \\geq n_0$. But $\\frac{c}{n^2} - \\frac{1}{n} = \\frac{c - n}{n^2} \\leq 0 \\text{ for all } n \\geq c$. So we cannot find such $n_0$ for any positive constant $c$. f This is true. If $f(n) = O(g(n))$, then there exist positive constants c and $n_0$ such that $0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0$, so we have $0 \\leq \\frac{1}{c}f(n) \\leq g(n) \\text{ for all } n \\geq n_0$, which is the definition of $g(n) = \\Omega(f(n))$. g This is not true. Let $f(n) = 2^{2n}$, so $f(\\frac{n}{2}) = 2^n$. So it's obvious $f(n) \\neq \\Theta(f(\\frac{n}{2}))$. h This is true. Suppose $f(n) + o(f(n)) = \\Theta(f(n))$, then we need to prove that there exist positive constatns $c_1$, $c_2$ and $n_0$ such that $0 \\leq c_1f(n) \\leq f(n) + o(f(n)) \\leq c_2f(n)$. It's easy to find $c_1 = 1$ since $o(f(n)) \\geq 0$. And according to the definition of $o(f(n))$, we know for any positive constant c there exists positive constant $n_1$ such that $0 \\leq o(f(n)) < cf(n) \\text{ for all } n \\geq n_1$. so we can choose $c = 1$, so $f(n) + o(f(n)) \\leq f(n) + f(n) = 2f(n)$, so we have $c_2 = 2$ and $n_0 = n_1$. So $f(n) + o(f(n)) = \\Theta(f(n))$. 3-5 a Suppose both $f(n) = O(g(n))$ and $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ are not true. If $f(n) = O(g(n))$ is not true, then there are two cases, first, we can not find any positive constant c such that $0 \\leq f(n) \\leq cg(n)$ for any integer n. It means for any positive constant c we have $f(n) > cg(n)$. So it satisfies $f(n) \\geq cg(n) \\geq 0$ for infinitely many integers n. So it shows $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ is true, thus, the hypothesis is wrong. Second, we can find a constant c such that $0 \\leq f(n) \\leq cg(n)$ for some integers n. If the set of integers n is finite, then there is an infinite set such that $f(n) \\geq cg(n) \\geq 0$, so $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$. If the set is infinite but we cannot find a positive constant $n_0$ that satisfies $f(n) = O(g(n))$, there is also a infinite set such that $f(n) > cg(n) \\geq 0$, so the hypothesis is wrong. So we proved either $f(n) = O(g(n))$ or $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ or both. In problem 3-2 we proved both $\\sqrt{n} = O(n^{\\sin{n}})$ and $\\sqrt{n} = \\Omega(n^{\\sin{n}})$ are wrong. So it's not true if we use $\\Omega$ in place of $\\mathop{\\Omega}^{\\infty}$. b The advantage is that we can describe the relationship of two functions when we cannot use $\\Omega$ notation. The disadvantage is that sometimes we cannot clearly know the running time of a function. c If $f(n) = \\Theta(g(n))$, then we have $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$. If we have $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$, $f(n) = \\Theta(g(n))$ is also true. Since $f(n) = \\Omega(g(n))$ guarantees $f(n) \\geq 0$. d $ \\begin{aligned} \\tilde{\\Omega}(g(n)) = & \\lbrace f(n): \\text{ there exist positive constants } c, \\text{ } k, \\text{ and } n_0 \\text{ such that } \\\\ & 0 \\leq cg(n)\\lg^k{n} \\leq f(n) \\text{ for all } n \\geq n_0 \\rbrace \\end{aligned} $ $ \\begin{aligned} \\tilde{\\Theta}(g(n)) = & \\lbrace f(n): \\text{ there exist positive constants } c_1, \\text{ } c_2, \\text{ } k_1, \\text{ } k_2 \\text{ and } n_0 \\text{ such that } \\\\ & 0 \\leq c_1g(n)\\lg^{k_1}{n} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{n} \\text{ for all } n \\geq n_0 \\rbrace \\end{aligned} $ Prove Theorem 3.1: If $f(n) = \\tilde{\\Theta}(g(n))$, then there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, $n_0$ such that $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_0$. It means: $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\text{ for all } n \\geq n_0$ and $0 \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_0$. They are the definition of $f(n) = \\tilde{O}(g(n))$ and $f(n) = \\tilde{\\Omega}(g(n))$. If $f(n) = \\tilde{O}(g(n))$ and $f(n) = \\tilde{\\Omega}(g(n))$. Then there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, $n_1$, $n_2$ such that $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\text{ for all } n \\geq n_1$ and $0 \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_2$. Then we combine them together: $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq max(n_1, n_2)$. So $f(n) = \\tilde{\\Theta}(g(n))$. 3-6 a $f^2(n) = n - 1 - 1 = n - 2$, $f^3(n) = n - 1 - 2 = n - 3$, so $f^k(n) = n - k$. It's easy to see $f^k(n) = 0$ after nth iterations. b $f^2(n) = \\lg{\\lg{n}}$, $f^3(n) = \\lg{\\lg{\\lg{n}}}$, so $f^k(n) = \\underbrace{\\lg{\\lg{\\ldots\\lg{n}}}}_\\text{k}$. Let $\\lg{\\lg{\\ldots\\lg{n}}} = 1$, so we have $f^{k - 1}(n) = 2$, and similiarly, $f^{k - 2}(n) = 2^2$, so $f^{k - (k - 1)}(n) = 2^{k - 1}$, so $k = \\lg{\\lg{n}} + 1$. c $f^2(n) = \\frac{n}{2^2}$, $f^k(n) = \\frac{n}{2^k}$, let $f^k(n) = \\frac{n}{2^k} = 1$, we have $k = \\lg{n}$. d Let $f^k(n) = \\frac{n}{2^k} = 2$, we have $k = \\lg{n} - 1$. e $f^2(n) = n^{\\frac{1}{2^2}}$, $f^k(n) = n^{\\frac{1}{2^k}}$, let $f^k(n) = n^{\\frac{1}{2^k}} = 2$, so $k = \\lg{\\lg{n}}$. f Because $\\frac{1}{2^k}$ could not be 0, so $f^k(n) = n^{\\frac{1}{2^k}}$ could not be 1. g $f^2(n) = n^{\\frac{1}{3^2}}$, $f^k(n) = n^{\\frac{1}{3^k}}$, let $f^k(n) = n^{\\frac{1}{3^k}} = 2$, so $k = \\frac{\\lg{\\lg{n}}}{\\lg{3}}$. h ? Summary $f(n)$ $c$ $f_c^*(n)$ $n - 1$ 0 $n$ $\\lg{n}$ 1 $\\lceil \\lg{\\lg{n}} + 1 \\rceil$ $\\frac{n}{2}$ 1 $\\lceil \\lg{n} \\rceil$ $\\frac{n}{2}$ 2 $\\lceil \\lg{n} - 1 \\rceil$ $\\sqrt{n}$ 2 $\\lceil \\lg{\\lg{n}} \\rceil$ $\\sqrt{n}$ 1 $+\\infty$ $n^{\\frac{1}{3}}$ 2 $\\lceil \\frac{\\lg{\\lg{n}}}{\\lg{3}} \\rceil$ $\\frac{n}{\\lg{n}}$ 2 ?","title":"Problems"},{"location":"03-Growth-of-Functions/Problems/#problems","text":"","title":"Problems"},{"location":"03-Growth-of-Functions/Problems/#3-1","text":"Let $a_{max} = max(a_0, a_1, \\ldots, a_d)$. Because $a_d > 0$, so $a_{max} > 0$. Now let's prove there exists a constant $n_1$ such that $p(n) \\geq 0 \\text{ for all } n \\geq n_1$. Let $a_{absmax} = max(abs(a_0), abs(a_1), \\ldots, abs(a_{d - 1}))$. So $p(n) \\geq a_dn^d + \\sum_{i = 0}^{d - 1} (-a_{absmax}n^{d - 1}) = a_dn^d -da_{absmax}n^{d - 1} = n^{d - 1}(a_dn - da_{absmax})$. So $p(n) \\geq 0$ when $n \\geq \\lceil\\frac{da_{absmax}}{a_d}\\rceil$, $n_1 = \\lceil\\frac{da_{absmax}}{a_d}\\rceil$.","title":"3-1"},{"location":"03-Growth-of-Functions/Problems/#a","text":"If $k \\geq d$, then $p(n) \\leq \\sum_{i = 0}^d a_{max}n^d = (d + 1)a_{max}n^d \\leq (d + 1)a_{max}n^k$. So there exist positive constants $c = (d + 1)a_{max}$ and $n_0 = max(1, n_1)$ such that $0 \\leq p(n) \\leq cn^k \\text{ for all } n \\geq n_0$. Thus $p(n) = O(n^k)$.","title":"a"},{"location":"03-Growth-of-Functions/Problems/#b","text":"We know $p(n) \\geq n^{d - 1}(a_dn - da_{absmax})$. So $n^{d - 1}(a_dn - da_{absmax}) \\geq n^d$ when $n \\geq \\frac{da_{absmax}}{a_d - 1}$. Thus $p(n) \\geq n^d \\geq n^k$ when $n \\geq max(1, \\lceil \\frac{da_{absmax}}{a_d - 1} \\rceil)$. Let $n_2 = max(1, \\lceil \\frac{da_{absmax}}{a_d - 1} \\rceil)$, so there exist positive constants c = 1 and $n_0 = max(n_1, n_2)$ such that $0 \\leq cn^k \\leq p(n) \\text{ for all } n \\geq n_0$. So $p(n) = \\Omega(n^k)$.","title":"b"},{"location":"03-Growth-of-Functions/Problems/#c","text":"Let $n_3 = (n_0 \\text{ in question a})$ and $n_4 = (n_0 \\text{ in question b})$. From the questions a and b we know there exist positive constants $c_1 = 1$, $c_2 = (d + 1)a_{max}$ and $n_0 = max(n_3, n_4)$ such that $0 \\leq c_1n^d \\leq p(n) \\leq c_2n^d \\text{ for all } n \\geq n_0$. Because k = d, so this also holds true for k, so $p(n) = \\Theta(n^k)$.","title":"c"},{"location":"03-Growth-of-Functions/Problems/#d","text":"From question a we know $p(n) \\leq (d + 1)a_{max}n^d$, because k > d, let $(d + 1)a_{max}n^d < cn^k$, then we have $n > (\\frac{(d + 1)a_{max}}{c})^{\\frac{1}{k - d}}$. So for any positive constant c, we can find a positive constant $n_0 = \\lceil (\\frac{(d + 1)a_{max}}{c})^{\\frac{1}{k - d}} \\rceil + 1$ such that $0 \\leq p(n) < cn^k \\text{ for all } n \\geq n_0$. So $p(n) = o(n^k)$.","title":"d"},{"location":"03-Growth-of-Functions/Problems/#e","text":"From question b we know $p(n) \\geq n^{d - 1}(a_dn - da_{absmax})$, let $f(n) = n^{d - 1}(a_dn - da_{absmax}) - cn^k = n^k(n^{d - k}(a_d - \\frac{da_{absmax}}{n}) - c)$, because k < d, so it's obvious that f(n) is a monotonically increasing function. First let $a_d - \\frac{da_{absmax}}{n} > \\frac{a_d}{2}$ and we get $n > \\frac{2da_{absmax}}{a_d}$. So $f(n) > n^k(n^{d - k}\\frac{a_d}{2} - c) \\text{ for all } n >= \\lceil \\frac{2da_{absmax}}{a_d} \\rceil + 1$. Then let $n^{d - k}\\frac{a_d}{2} - c > 0$ and we have $n > (\\frac{2c}{a_d})^{\\frac{1}{d - k}}$. So for any given positive constant c we can find a positive constant $n_0 = max(\\lceil \\frac{2da_{absmax}}{a_d} \\rceil + 1, \\lceil (\\frac{2c}{a_d})^{\\frac{1}{d - k}} \\rceil + 1)$ such that $0 \\leq cn^k < p(n) \\text{ for all } n \\geq n_0$. So $p(n) = w(n^k)$.","title":"e"},{"location":"03-Growth-of-Functions/Problems/#3-2","text":"","title":"3-2"},{"location":"03-Growth-of-Functions/Problems/#a_1","text":"Note that $n = 2^{\\lg{n}}$. So $\\lg^k{n} = (2^{\\lg{\\lg{n}}})^k = 2^{k\\lg{\\lg{n}}}$, $n^\\epsilon = (2^{\\lg{n}})^{\\epsilon} = 2^{\\epsilon\\lg{n}}$. It's obvious that $\\epsilon\\lg{n}$ grows faster than $k\\lg{\\lg{n}}$. Let $\\lg{n} = x, x > 0$, so $k\\lg{\\lg{n}} = k\\lg{x}$, $\\epsilon\\lg{n} = \\epsilon{x}$. Let $f(x) = \\epsilon{x} - k\\lg{x}$, so $f'(x) = \\epsilon - \\frac{k}{x}$. Because $\\epsilon > 0$ and $k \\geq 1$, we have $f'(x) >= 0 \\text{ when } x \\geq \\frac{k}{\\epsilon}$. So f(x) is a monotonically increasing function when $x \\geq \\frac{k}{\\epsilon}$. In order to solve $\\epsilon{x} - k\\lg{x} > 0$, we only have to solve $\\frac{x}{\\lg{x}} > \\frac{k}{\\epsilon}$, since $\\lim_{x \\to +\\infty} \\frac{x}{\\lg{x}} = +\\infty$, so there exists a constant $x_0$ such that $\\frac{x_0}{\\lg{x_0}} > \\frac{k}{\\epsilon}$, so $f(x_0) > 0$. So we can find a constant $x_0$ such that $f(x) \\geq 0 \\text{ for all } x \\geq x_0$. Thus $\\epsilon\\lg{n} \\geq k\\lg{\\lg{n}} \\text{ for all } n \\geq 2^{x_0}$. Therefore we proved there exist positive constants c = 1 and $n_0 = 2^{x_0}$ such that $0 \\leq \\lg^k{n} \\leq n^{\\epsilon} \\text{ for all } n \\geq n_0$. So $\\lg^k{n} = O(n^{\\epsilon})$. Now let's compare $\\lg^k{n}$ and $cn^{\\epsilon}$. Similarly, $cn^{\\epsilon} = 2^{\\lg{c}}(2^{\\lg{n}})^{\\epsilon} = 2^{\\lg{c} + \\epsilon\\lg{n}}$. So let $\\lg{n} = x$, so $\\lg{c} + \\epsilon\\lg{n} - k\\lg{\\lg{n}} = \\lg{c} + \\epsilon{x} - k\\lg{x}$. Let $g(x) = \\epsilon{x} - k\\lg{x} + \\lg{c}$. And $g'(x) = \\epsilon - \\frac{k}{x}$, $g'(x) >= 0 \\text{ when } x \\geq \\frac{k}{\\epsilon}$. So g(x) is a monotonically increasing function when $x \\geq \\frac{k}{\\epsilon}$. In order to solve $g(x) > 0$, we need to solve $\\epsilon{x} - k\\lg{x} > -\\lg{c}$. Namely, for any given positive constant c, we need to find a $x_0$ such that $g(x_0)$ is greater than $-\\lg{c}$. Notice that $\\epsilon{x} - k\\lg{x} = f(x)$ and f(x) is a a monotonically increasing function. For a given constant, we can find a $x_0$ such that $f(x_0)$ is greater than that constant. So, for any given constant c, we can find a $x_0$ such that $g(x_0) > 0$. Thus, for any positive constant c, there exists positive constant $n_0 = 2^{x_0}$ such that $0 \\leq \\lg^k{n} < cn^{\\epsilon} \\text{ for all } n \\geq n_0$. So $\\lg^k{n} = o(n^{\\epsilon})$. Since $\\lg^k{n} = o(n^{\\epsilon})$, then $\\lg^k{n}$ could not be $\\Omega(n^{\\epsilon})$, $w(n^{\\epsilon})$, $\\Theta(n^{\\epsilon})$.","title":"a"},{"location":"03-Growth-of-Functions/Problems/#b_1","text":"$n^k = (2^{\\lg{n}})^k = 2^{k\\lg{n}}$, $c^n = (2^{\\lg{c}})^n = 2^{n\\lg{c}}$. So it's also obvious that $n\\lg{c}$ grows faster than $k\\lg{n}$. Let $f(n) = n\\lg{c} - k\\lg{n}$. Because $c > 1$, so $\\lg{c} > 0$. Thus we have the same function $f(x)$ defined in question a. So similarly, we know $n^k = O(c^n)$. Now let's compare $n^k$ and $bc^n$. $bc^n = 2^{\\lg{b}}(2^{\\lg{c}})^n = 2^{\\lg{b} + n\\lg{c}}$. Let $g(n) = \\lg{b} + n\\lg{c} - k\\lg{n}$, and again we have the same function g(x) defined in question a. So $n^k = o(c^n)$.","title":"b"},{"location":"03-Growth-of-Functions/Problems/#c_1","text":"Let's compare $\\sqrt{n}$ and $cn^{\\sin{n}}$. $\\sqrt{n} = (2^{\\lg{n}})^{\\frac{1}{2}} = 2^{\\frac{1}{2}\\lg{n}}$, $cn^{\\sin{n}} = 2^{\\lg{c}}(2^{\\lg{n}})^{\\sin{n}} = 2^{\\sin{n}\\lg{n} + \\lg{c}}$. Let $f(n) = \\sin{n}\\lg{n} + \\lg{c} - \\frac{1}{2}\\lg{n} = (\\sin{n} - \\frac{1}{2})\\lg{n} + \\lg{c}$. So the question is: does there exist a positive constant c (or for any positive constant c), there exists a positive constant $n_0$, such that f(n) > 0 (or f(n) < 0) for all $n \\geq n_0$? First let's check $f(n) > 0$, nomatter how big c is, we can find a $n_1$ such that $\\lg{n_1} > \\lg{c}$, since $-\\frac{3}{2} \\leq \\sin{n} - \\frac{1}{2} \\leq \\frac{1}{2}$. So there exists a $n_2 \\geq n_1$ such that $\\sin{n_2} - \\frac{1}{2} = -1$, so $f(n_2) < 0$. So for any given constant c, there doesn't exist a constant $n_0$ such that f(n) > 0 for all $n \\geq n_0$. Similarly, for $f(n) < 0$, no matter how small c is, we can find a $n_1$ such that $\\lg{n_1} > 2abs(\\lg{c})$, and there exists a $n_2 \\geq n_1$ such that $\\sin{n_2} - \\frac{1}{2} = \\frac{1}{2}$, so $f(n_2) > 0$. So for any given constant c, there doesn't exist a constant $n_0$ such that f(n) < 0 for all $n \\geq n_0$. Thus, we cannot compare which grows faster.","title":"c"},{"location":"03-Growth-of-Functions/Problems/#d_1","text":"It's obvious that there exist a positive constant c = 1 and $n_0 = 1$ such that $0 \\leq c2^{\\frac{n}{2}} \\leq 2^n \\text{ for all } n \\geq n_0$. So $2^n = \\Omega(2^{\\frac{n}{2}})$. Now let's compare $2^n$ and $c2^{\\frac{n}{2}}$. $c2^{\\frac{n}{2}} = 2^{\\lg{c}}2^{\\frac{n}{2}} = 2^{\\frac{n}{2} + \\lg{c}}$. Let $f(n) = n - (\\frac{n}{2} + \\lg{c}) = \\frac{n}{2} - \\lg{c}$. Let $f(n) > 0$, we have $n > 2\\lg{c}$. So for any constant c, there exists a positive constant $n_0 = 2\\lg{c} + 1$ such that $2^n > c2^{\\frac{n}{2}} \\text{ for all } n >= n_0$. So $2^n = w(2^{\\frac{n}{2}})$.","title":"d"},{"location":"03-Growth-of-Functions/Problems/#e_1","text":"$n^{\\lg{c}} = (2^{\\lg{n}})^{\\lg{c}} = 2^{\\lg{c}\\lg{n}}$, $c^{\\lg{n}} = (2^{\\lg{c}})^{\\lg{n}} = 2^{\\lg{c}\\lg{n}}$, so $n^{\\lg{c}} = c^{\\lg{n}}$, thus there exist positive constants $b_1 = 1$ and $n_1 = 1$ such that $0 \\leq n^{\\lg{c}} \\leq b_1c^{\\lg{n}} \\text{ for all } n \\geq n_1$, and there exist positive constants $b_2 = 1$ and $n_2 = 1$ such that $0 \\leq b_2c^{\\lg{n}} \\leq n^{\\lg{c}} \\text{ for all } n \\geq n_2$, and there exist positive constants $b_3 = 1$, $b_4 = 1$ and $n_3 = 1$ such that $0 \\leq b_3c^{\\lg{n}} \\leq n^{\\lg{c}} \\leq b_4c^{\\lg{n}} \\text{ for all } n \\geq n_3$. So $n^{\\lg{c}} = O(c^{\\lg{n}})$, $n^{\\lg{c}} = \\Omega(c^{\\lg{n}})$, $n^{\\lg{c}} = \\Theta(c^{\\lg{n}})$. Since $n^{\\lg{c}}$ and $c^{\\lg{n}}$ are the same functions, so for any positive constant b, we cannot find a positive constant $n_0$ such that $n^{\\lg{c}} < bc^{\\lg{n}}$ or $n^{\\lg{c}} > bc^{\\lg{n}}$ for all $n \\geq n_0$.","title":"e"},{"location":"03-Growth-of-Functions/Problems/#f","text":"In question 3.2-3, we've already proved that $\\lg{(n!)} = \\Theta(n\\lg{n})$, notice that $\\lg(n^n) = n\\lg{n}$, so $\\lg{(n!)} = \\Theta(\\lg{n^n})$. And $\\lg{(n!)} = O(\\lg{n^n})$, $\\lg{(n!)} = \\Omega(\\lg{n^n})$.","title":"f"},{"location":"03-Growth-of-Functions/Problems/#summary","text":"A B O o $\\Omega$ w $\\Theta$ $\\lg^k{n}$ $n^{\\epsilon}$ yes yes no no no $n^k$ $c^n$ yes yes no no no $\\sqrt{n}$ $n^{\\sin{n}}$ no no no no no $2^n$ $2^{\\frac{n}{2}}$ no no yes yes no $n^{\\lg{c}}$ $c^{\\lg{n}}$ yes no yes no yes $\\lg(n!)$ $\\lg(n^n)$ yes no yes no yes","title":"Summary"},{"location":"03-Growth-of-Functions/Problems/#3-3","text":"","title":"3-3"},{"location":"03-Growth-of-Functions/Problems/#a_2","text":"First, let's compare $2^{2^{n + 1}}$ and $2^{2^n}$, it's easy to see $2^{2^n} = O(2^{2^{n + 1}})$. But could $2^{2^n} = \\Omega(2^{2^{n + 1}})$? If it's true, then there exist a positive constant c and $n_0$ such that $0 \\leq c2^{2^{n + 1}} \\leq 2^{2^n}$ for all $n \\geq n_0$. But $\\frac{2^{2^n}}{c2^{2^{n + 1}}} = \\frac{1}{c2^{2^{n + 1} - 2^n}} = \\frac{1}{c2^{2^n}}$. No matter how small c is, $2^{2^n}$ will be greater than $\\frac{1}{c}$. So $2^{2^n} = \\Omega(2^{2^{n + 1}})$ could not be true. Then let's compare $n!$ and $e^n$. We know $n! = 2^{\\lg{(n!)}}$, and $e^n = (2^{\\lg{e}})^n = 2^{n\\lg{e}}$. In question 3.2-3 we know $\\lg{(n!)} = \\Theta(n\\lg{n})$, so $\\lg{(n!)}$ grows faster than $n\\lg{e}$. So $n!$ grows faster than $e^n$, thus $n! = \\Omega(e^n)$. And it's obvious $(n + 1)! = \\Omega(n!)$. Notice that $\\frac{n!}{c(n + 1)!} = \\frac{1}{c(n + 1)}$, which will eventually smaller than 1. So $n!$ could not be $\\Omega((n + 1)!)$. And what about $(n + 1)!$ and $2^{2^n}$? $(n + 1)! = 2^{\\lg{(n + 1)!}} = 2^{\\Theta((n + 1)\\lg{(n + 1)})} = 2^{O((n + 1)^2)} = 2^{O(n^2)}$. So $2^{2^{n}}$ grows faster, $2^{2^n} = \\Omega((n + 1)!)$. Since $e^n = (\\frac{e}{2})^n2^n$, and $(\\frac{e}{2})^n$ grows faster than $n$, so $e^n$ grows faster than $n2^n$, $e^n = \\Omega(n2^n)$. And it's easy to see that $n2^n = \\Omega(2^n)$, $2^n = \\Omega(\\frac{3}{2})^n$. $(\\lg{n})^{\\lg{n}} = (2^{\\lg{\\lg{n}}})^{\\lg{n}} = 2^{\\lg{n}\\lg{\\lg{n}}}$. And $n^{\\lg{\\lg{n}}} = (2^{\\lg{n}})^{\\lg{\\lg{n}}} = 2^{\\lg{n}\\lg{\\lg{n}}}$. So $(\\lg{n})^{\\lg{n}} = n^{\\lg{\\lg{n}}}$. And $(\\frac{3}{2})^n = (2^{\\lg{\\frac{3}{2}}})^n = 2^{n\\lg{\\frac{3}{2}}}$. Let $\\lg{n} = k$, so $\\lg{n}\\lg{\\lg{n}} = k\\lg{k}$, $n\\lg{\\frac{3}{2}} = 2^k\\lg{\\frac{3}{2}}$, so we can see $2^k\\lg{\\frac{3}{2}}$ grows faster. Thus $(\\frac{3}{2})^n = \\Omega((\\lg{n})^{\\lg{n}})$. According to Stirling's approximation , we know $n! = \\Theta(n^{n + \\frac{1}{2}}e^{-n})$. So $(\\lg{n})! = \\Theta((\\lg{n})^{\\lg{n} + \\frac{1}{2}}e^{-\\lg{n}}) = \\Theta((\\lg{n})^{\\lg{n}}\\frac{\\sqrt{\\lg{n}}}{e^{\\lg{n}}}) = O((\\lg{n})^{\\lg{n}})$, since $e^x$ grows much faster than $\\sqrt{x}$. So $(\\lg{n})^{\\lg{n}} = \\Omega((\\lg{n})!)$. Let $\\lg{n} = x$, so $n = 2^x$, $n^3 = 2^{3x}$. $(\\lg{n})! = \\Theta((\\lg{n})^{\\lg{n}}\\frac{\\sqrt{\\lg{n}}}{e^{\\lg{n}}}) = \\Theta(x^{x + \\frac{1}{2}}e^{-x}) = \\Theta(e^{x(\\ln{x} - 1) + \\frac{1}{2}\\ln{x}})$. And $2^{3x} = e^{(3\\ln{2})x}$. So $(\\lg{n})!$ grows faster than $n^3$. $4^{\\lg{n}} = (2^2)^{\\lg{n}} = (2^{\\lg{n}})^2 = n^2$, so $n^2 = 4^{\\lg{n}}$. And it's obvious $n^3 = \\Omega(n^2)$, $n^2 = \\Omega(n\\lg{n})$. And in question 3.2-3 we already proved $\\lg({n!}) = \\Theta(n\\lg{n})$. And it's easy to know $n\\lg{n} = \\Omega(n)$, $n = 2^{\\lg{n}}$. Similarly, $(\\sqrt{2})^{\\lg{n}} = (2^{\\lg{n}})^{\\frac{1}{2}} = \\sqrt{n}$, so $n = \\Omega({(\\sqrt{2})^{\\lg{n}}})$. $\\sqrt{n} = n^{\\frac{1}{2}} = (2^{\\lg{n}})^{\\frac{1}{2}} = 2^{\\frac{1}{2}\\lg{n}}$, it grows faster than $2^{\\sqrt{2\\lg{n}}}$, so $(\\sqrt{2})^{\\lg{n}} = \\Omega(2^{\\sqrt{2\\lg{n}}})$. $\\lg^2{n} = (2^{\\lg{\\lg{n}}})^2 = 2^{2\\lg{\\lg{n}}}$, in question 3-2 we know $\\lg^k{n} = o(n^{\\epsilon})$ for $k \\geq 1$ and $\\epsilon > 0$, so here we have $k = 1$, $\\epsilon = \\frac{1}{2}$, so $\\sqrt{2\\lg{n}}$ grows faster than $2\\lg{\\lg{n}}$, so $2^{\\sqrt{2\\lg{n}}} = \\Omega(\\lg^2{n})$. And $\\lg^2{n} = \\Omega(\\ln{n})$, $\\ln{n} = \\Omega(\\sqrt{\\lg{n}})$. Then let's compare $\\sqrt{\\lg{n}}$ and $\\ln{\\ln{n}}$. $\\sqrt{\\lg{n}} = \\sqrt{\\frac{\\ln{n}}{\\ln{2}}}$, from previous proof, we know $\\sqrt{\\frac{\\ln{n}}{\\ln{2}}}$ grows faster than $\\ln{\\ln{n}}$. So $\\sqrt{\\lg{n}} = \\Omega(\\ln{\\ln{n}})$. $\\ln{\\ln{n}} = 2^{\\lg{\\ln{\\ln{n}}}}$, and according to the definition of Iterated logarithm , we can see $\\lg{\\ln{\\ln{n}}}$ grows faster than $\\lg^*{n}$. So $\\ln{\\ln{n}} = \\Omega(2^{\\lg^*{n}})$. And $2^{\\lg^*{n}} = \\Omega(\\lg^*{n})$. Since $\\lg^*{n} = 1 + \\lg^*{\\lg{n}}$, so $\\lg^*{\\lg{n}} = \\Theta(\\lg^*{n})$. In question 3.2-5 we proved that $\\lg^*{(\\lg{n})}$ is asymptotically larger than $\\lg{(\\lg^*{n})}$. So $\\lg^*{(\\lg{n})} = \\Omega(\\lg{(\\lg^*{n})})$. $n^{\\frac{1}{\\lg{n}}} = (2^{\\lg{n}})^{\\frac{1}{\\lg{n}}} = 2$, so $n^{\\frac{1}{\\lg{n}}} = \\Theta(1)$.","title":"a"},{"location":"03-Growth-of-Functions/Problems/#summary_1","text":"$g_1 = 2^{2^{n + 1}}$ $g_2 = 2^{2^n}$ $g_3 = (n + 1)!$ $g_4 = n!$ $g_5 = e^n$ $g_6 = n2^n$ $g_7 = 2^n$ $g_8 = (\\frac{3}{2})^n$ $g_9 = (\\lg{n})^{\\lg{n}}$ $g_{10} = n^{\\lg{\\lg{n}}}, g_9 = \\Theta(g_{10})$ $g_{11} = (\\lg{n})!$ $g_{12} = n^3$ $g_{13} = n^2$ $g_{14} = 4^{\\lg{n}}, g_{13} = \\Theta(g_{14})$ $g_{15} = n\\lg{n}$ $g_{16} = \\lg({n!}), g_{15} = \\Theta(g_{16})$ $g_{17} = n$ $g_{18} = 2^{\\lg{n}}, g_{17} = \\Theta(g_{18})$ $g_{19} = (\\sqrt{2})^{\\lg{n}}$ $g_{20} = 2^{\\sqrt{2\\lg{n}}}$ $g_{21} = \\lg^2{n}$ $g_{22} = \\ln{n}$ $g_{23} = \\sqrt{\\lg{n}}$ $g_{24} = \\ln{\\ln{n}}$ $g_{25} = 2^{\\lg^*{n}}$ $g_{26} = \\lg^*{n}$ $g_{27} = \\lg^*{\\lg{n}}, g_{26} = \\Theta(g_{27})$ $g_{28} = \\lg{(\\lg^*{n})}$ $g_{29} = n^{\\frac{1}{\\lg{n}}}$ $g_{30} = 1, g_{29} = \\Theta(g_{30})$","title":"Summary"},{"location":"03-Growth-of-Functions/Problems/#b_2","text":"How do we find a function like this? In question 3-2, we know that $n^{\\sin{n}}$ is neither $o(\\sqrt{n})$ nor $w(\\sqrt{n})$. We can use the feature of $\\sin{n}$. So we can build a function that can be quite big and quite small. And it has to be not smaller than the largest function in $g_i(n)$ sometimes, so we can simply have a function like $n2^{2^{n + 1}}$, which is $\\Omega(g_i(n))$. Then we multiply $|\\sin{n}|$, so the function is $|\\sin{n}|n2^{2^{n + 1}}$, which is neither $O(g_i(n))$ nor $\\Omega(g_i(n))$.","title":"b"},{"location":"03-Growth-of-Functions/Problems/#3-4","text":"","title":"3-4"},{"location":"03-Growth-of-Functions/Problems/#a_3","text":"This is not true. If $g(n) = O(f(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq g(n) \\leq c_1f(n) \\text{ for all } n \\geq n_1$, since $f(n) = O(g(n))$, so there also exist positive constants $c_2$ and $n_2$ such that $0 \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_2$, so we have $0 \\leq \\frac{1}{c_1}g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq max(n_1, n_2)$, which means $f(n) = \\Theta(g(n))$. This is not 100% true.","title":"a"},{"location":"03-Growth-of-Functions/Problems/#b_3","text":"This is not true. Let $f(n) = n$ and $g(n) = \\lg{n}$, it's easy to see $f(n) + g(n) = \\Theta(n) \\neq \\Theta(min(f(n), g(n)))$.","title":"b"},{"location":"03-Growth-of-Functions/Problems/#c_2","text":"This is true. Because $f(n) = O(g(n))$, so there exist positive constants c and $n_0$ such that $0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0$. So: $$ \\begin{eqnarray} 0 &\\leq& \\lg{(f(n))} \\\\ &\\leq& \\lg{(cg(n))} \\\\ &=& \\lg{c} + \\lg{(g(n))} \\\\ &=& \\lg{c} * 1 + \\lg{(g(n))} \\\\ &\\leq& \\lg{c} * \\lg{(g(n))} + \\lg{(g(n))} \\\\ &=& (\\lg{c} + 1)\\lg{(g(n))} \\end{eqnarray} $$ So we find a positive constant $c_1 = \\lg{c} + 1$ such that $0 \\leq \\lg{(f(n))} \\leq c_1\\lg{(g(n))} \\text{ for all } n \\geq n_0$, so $\\lg{(f(n))} = O(\\lg{(g(n))}$.","title":"c"},{"location":"03-Growth-of-Functions/Problems/#d_2","text":"This is not true. If $g(n) = O(f(n))$, then there exist positive constants $c_1$ and $n_1$ such that $0 \\leq g(n) \\leq c_1f(n) \\text{ for all } n \\geq n_1$. Suppose $2^{f(n)} = O(2^{g(n)})$, then there exist positive constants $c_2$ and $n_2$ such that $0 \\leq 2^{f(n)} \\leq {c_2}2^{g(n)} \\text{ for all } n \\geq n_2$. And ${c_2}2^{g(n)} = 2^{\\lg{c_2} + g(n)}$. So we have $f(n) \\leq \\lg{c_2} + g(n)$. But this is not always true, let $f(n) = n + \\lg{n}$ and $g(n) = n$, so $f(n) \\leq 2g(n) \\text{ for all } n \\geq 1$. But for any given positive constant $c_2$, we can always find a positive constant $n_3$ such that $n + \\lg{n} > \\lg{c_2} + n \\text{ for all } n \\geq n_3$. So $2^{f(n)} \\neq O(2^{g(n)})$ in this situation.","title":"d"},{"location":"03-Growth-of-Functions/Problems/#e_2","text":"This is not true. Let $f(n) = \\frac{1}{n}$, if $f(n) = O((f(n))^2)$, then there exist positive constants c and $n_0$ such that $0 \\leq \\frac{1}{n} \\leq \\frac{c}{n^2} \\text{ for all } n \\geq n_0$. But $\\frac{c}{n^2} - \\frac{1}{n} = \\frac{c - n}{n^2} \\leq 0 \\text{ for all } n \\geq c$. So we cannot find such $n_0$ for any positive constant $c$.","title":"e"},{"location":"03-Growth-of-Functions/Problems/#f_1","text":"This is true. If $f(n) = O(g(n))$, then there exist positive constants c and $n_0$ such that $0 \\leq f(n) \\leq cg(n) \\text{ for all } n \\geq n_0$, so we have $0 \\leq \\frac{1}{c}f(n) \\leq g(n) \\text{ for all } n \\geq n_0$, which is the definition of $g(n) = \\Omega(f(n))$.","title":"f"},{"location":"03-Growth-of-Functions/Problems/#g","text":"This is not true. Let $f(n) = 2^{2n}$, so $f(\\frac{n}{2}) = 2^n$. So it's obvious $f(n) \\neq \\Theta(f(\\frac{n}{2}))$.","title":"g"},{"location":"03-Growth-of-Functions/Problems/#h","text":"This is true. Suppose $f(n) + o(f(n)) = \\Theta(f(n))$, then we need to prove that there exist positive constatns $c_1$, $c_2$ and $n_0$ such that $0 \\leq c_1f(n) \\leq f(n) + o(f(n)) \\leq c_2f(n)$. It's easy to find $c_1 = 1$ since $o(f(n)) \\geq 0$. And according to the definition of $o(f(n))$, we know for any positive constant c there exists positive constant $n_1$ such that $0 \\leq o(f(n)) < cf(n) \\text{ for all } n \\geq n_1$. so we can choose $c = 1$, so $f(n) + o(f(n)) \\leq f(n) + f(n) = 2f(n)$, so we have $c_2 = 2$ and $n_0 = n_1$. So $f(n) + o(f(n)) = \\Theta(f(n))$.","title":"h"},{"location":"03-Growth-of-Functions/Problems/#3-5","text":"","title":"3-5"},{"location":"03-Growth-of-Functions/Problems/#a_4","text":"Suppose both $f(n) = O(g(n))$ and $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ are not true. If $f(n) = O(g(n))$ is not true, then there are two cases, first, we can not find any positive constant c such that $0 \\leq f(n) \\leq cg(n)$ for any integer n. It means for any positive constant c we have $f(n) > cg(n)$. So it satisfies $f(n) \\geq cg(n) \\geq 0$ for infinitely many integers n. So it shows $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ is true, thus, the hypothesis is wrong. Second, we can find a constant c such that $0 \\leq f(n) \\leq cg(n)$ for some integers n. If the set of integers n is finite, then there is an infinite set such that $f(n) \\geq cg(n) \\geq 0$, so $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$. If the set is infinite but we cannot find a positive constant $n_0$ that satisfies $f(n) = O(g(n))$, there is also a infinite set such that $f(n) > cg(n) \\geq 0$, so the hypothesis is wrong. So we proved either $f(n) = O(g(n))$ or $f(n) = \\mathop{\\Omega}^{\\infty}(g(n))$ or both. In problem 3-2 we proved both $\\sqrt{n} = O(n^{\\sin{n}})$ and $\\sqrt{n} = \\Omega(n^{\\sin{n}})$ are wrong. So it's not true if we use $\\Omega$ in place of $\\mathop{\\Omega}^{\\infty}$.","title":"a"},{"location":"03-Growth-of-Functions/Problems/#b_4","text":"The advantage is that we can describe the relationship of two functions when we cannot use $\\Omega$ notation. The disadvantage is that sometimes we cannot clearly know the running time of a function.","title":"b"},{"location":"03-Growth-of-Functions/Problems/#c_3","text":"If $f(n) = \\Theta(g(n))$, then we have $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$. If we have $f(n) = O'(g(n))$ and $f(n) = \\Omega(g(n))$, $f(n) = \\Theta(g(n))$ is also true. Since $f(n) = \\Omega(g(n))$ guarantees $f(n) \\geq 0$.","title":"c"},{"location":"03-Growth-of-Functions/Problems/#d_3","text":"$ \\begin{aligned} \\tilde{\\Omega}(g(n)) = & \\lbrace f(n): \\text{ there exist positive constants } c, \\text{ } k, \\text{ and } n_0 \\text{ such that } \\\\ & 0 \\leq cg(n)\\lg^k{n} \\leq f(n) \\text{ for all } n \\geq n_0 \\rbrace \\end{aligned} $ $ \\begin{aligned} \\tilde{\\Theta}(g(n)) = & \\lbrace f(n): \\text{ there exist positive constants } c_1, \\text{ } c_2, \\text{ } k_1, \\text{ } k_2 \\text{ and } n_0 \\text{ such that } \\\\ & 0 \\leq c_1g(n)\\lg^{k_1}{n} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{n} \\text{ for all } n \\geq n_0 \\rbrace \\end{aligned} $ Prove Theorem 3.1: If $f(n) = \\tilde{\\Theta}(g(n))$, then there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, $n_0$ such that $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_0$. It means: $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\text{ for all } n \\geq n_0$ and $0 \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_0$. They are the definition of $f(n) = \\tilde{O}(g(n))$ and $f(n) = \\tilde{\\Omega}(g(n))$. If $f(n) = \\tilde{O}(g(n))$ and $f(n) = \\tilde{\\Omega}(g(n))$. Then there exist positive constants $c_1$, $c_2$, $k_1$, $k_2$, $n_1$, $n_2$ such that $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\text{ for all } n \\geq n_1$ and $0 \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq n_2$. Then we combine them together: $0 \\leq c_1g(n)\\lg^{k_1}{(n)} \\leq f(n) \\leq c_2g(n)\\lg^{k_2}{(n)} \\text{ for all } n \\geq max(n_1, n_2)$. So $f(n) = \\tilde{\\Theta}(g(n))$.","title":"d"},{"location":"03-Growth-of-Functions/Problems/#3-6","text":"","title":"3-6"},{"location":"03-Growth-of-Functions/Problems/#a_5","text":"$f^2(n) = n - 1 - 1 = n - 2$, $f^3(n) = n - 1 - 2 = n - 3$, so $f^k(n) = n - k$. It's easy to see $f^k(n) = 0$ after nth iterations.","title":"a"},{"location":"03-Growth-of-Functions/Problems/#b_5","text":"$f^2(n) = \\lg{\\lg{n}}$, $f^3(n) = \\lg{\\lg{\\lg{n}}}$, so $f^k(n) = \\underbrace{\\lg{\\lg{\\ldots\\lg{n}}}}_\\text{k}$. Let $\\lg{\\lg{\\ldots\\lg{n}}} = 1$, so we have $f^{k - 1}(n) = 2$, and similiarly, $f^{k - 2}(n) = 2^2$, so $f^{k - (k - 1)}(n) = 2^{k - 1}$, so $k = \\lg{\\lg{n}} + 1$.","title":"b"},{"location":"03-Growth-of-Functions/Problems/#c_4","text":"$f^2(n) = \\frac{n}{2^2}$, $f^k(n) = \\frac{n}{2^k}$, let $f^k(n) = \\frac{n}{2^k} = 1$, we have $k = \\lg{n}$.","title":"c"},{"location":"03-Growth-of-Functions/Problems/#d_4","text":"Let $f^k(n) = \\frac{n}{2^k} = 2$, we have $k = \\lg{n} - 1$.","title":"d"},{"location":"03-Growth-of-Functions/Problems/#e_3","text":"$f^2(n) = n^{\\frac{1}{2^2}}$, $f^k(n) = n^{\\frac{1}{2^k}}$, let $f^k(n) = n^{\\frac{1}{2^k}} = 2$, so $k = \\lg{\\lg{n}}$.","title":"e"},{"location":"03-Growth-of-Functions/Problems/#f_2","text":"Because $\\frac{1}{2^k}$ could not be 0, so $f^k(n) = n^{\\frac{1}{2^k}}$ could not be 1.","title":"f"},{"location":"03-Growth-of-Functions/Problems/#g_1","text":"$f^2(n) = n^{\\frac{1}{3^2}}$, $f^k(n) = n^{\\frac{1}{3^k}}$, let $f^k(n) = n^{\\frac{1}{3^k}} = 2$, so $k = \\frac{\\lg{\\lg{n}}}{\\lg{3}}$.","title":"g"},{"location":"03-Growth-of-Functions/Problems/#h_1","text":"?","title":"h"},{"location":"03-Growth-of-Functions/Problems/#summary_2","text":"$f(n)$ $c$ $f_c^*(n)$ $n - 1$ 0 $n$ $\\lg{n}$ 1 $\\lceil \\lg{\\lg{n}} + 1 \\rceil$ $\\frac{n}{2}$ 1 $\\lceil \\lg{n} \\rceil$ $\\frac{n}{2}$ 2 $\\lceil \\lg{n} - 1 \\rceil$ $\\sqrt{n}$ 2 $\\lceil \\lg{\\lg{n}} \\rceil$ $\\sqrt{n}$ 1 $+\\infty$ $n^{\\frac{1}{3}}$ 2 $\\lceil \\frac{\\lg{\\lg{n}}}{\\lg{3}} \\rceil$ $\\frac{n}{\\lg{n}}$ 2 ?","title":"Summary"},{"location":"04-Divide-and-Conquer/4.1-The-maximum-subarray-problem/","text":"4.1 The maximum-subarray problem 4.1-1 It returns the index and value of the biggest negative number in A. 4.1-2 FIND-MAXIMUM-SUBARRAY-BRUTE-FORCE(A, low, high) max = -\u221e start = -1, end = -1 for i = low to high sum = 0 for j = i to high sum += A[j] if sum > max max = sum start = i end = j return (start, end, max) 4.1-3 Brute force: def find_maximum_subarray_brute_force(numbers, low, high): start = -1 end = -1 max_sum = -float('inf') for i in range(low, high + 1): current_sum = 0 for j in range(i, high + 1): current_sum += numbers[j] if current_sum > max_sum: max_sum = current_sum start, end = i, j return (start, end, max_sum) Divide and conquer: def find_maximum_subarray_divide_and_conquer(numbers, low, high): if len(numbers) == 0: return (-1, -1, -float('inf')) elif low == high: return (low, high, numbers[low]) else: middle = (low + high) // 2 left_maximum_subarray = find_maximum_subarray_divide_and_conquer( numbers, low, middle) right_maximum_subarray = find_maximum_subarray_divide_and_conquer( numbers, middle + 1, high) cross_maximum_subarray = find_max_crossing_subarray( numbers, low, middle, high) if (left_maximum_subarray[2] >= right_maximum_subarray[2] and left_maximum_subarray[2] >= cross_maximum_subarray[2]): return left_maximum_subarray elif (right_maximum_subarray[2] >= left_maximum_subarray[2] and right_maximum_subarray[2] >= cross_maximum_subarray[2]): return right_maximum_subarray else: return cross_maximum_subarray def find_max_crossing_subarray(numbers, low, middle, high): start = middle end = middle current_sum = 0 left_sum = -float('inf') right_sum = -float('inf') for i in range(middle, low - 1, -1): current_sum += numbers[i] if current_sum > left_sum: left_sum = current_sum start = i current_sum = 0 for i in range(middle + 1, high + 1): current_sum += numbers[i] if current_sum > right_sum: right_sum = current_sum end = i return (start, end, left_sum + right_sum) In my env, when $n_0 = 64$ gives the crossover point. I changed the base of the recursive algorithm, but that doesn't change the crossover point. 4.1-4 Currently the implementation already supports empty array, but it returns (-1, -1, -float('inf')) , we can change it to an empty array. 4.1-5 def find_maximum_subarray_linear_time(numbers, low, high): if len(numbers) == 0: return (-1, -1, -float('inf')) start = -1 max_sum_start = -1 max_sum_end = -1 max_sum_so_far = -float('inf') max_sum_ending_here = -float('inf') for i in range(low, high + 1): if numbers[i] > max_sum_ending_here + numbers[i]: max_sum_ending_here = numbers[i] start = i else: max_sum_ending_here += numbers[i] if max_sum_ending_here > max_sum_so_far: max_sum_so_far = max_sum_ending_here max_sum_start = start max_sum_end = i return (max_sum_start, max_sum_end, max_sum_so_far)","title":"4.1 The maximum-subarray problem"},{"location":"04-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-the-maximum-subarray-problem","text":"","title":"4.1 The maximum-subarray problem"},{"location":"04-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-1","text":"It returns the index and value of the biggest negative number in A.","title":"4.1-1"},{"location":"04-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-2","text":"FIND-MAXIMUM-SUBARRAY-BRUTE-FORCE(A, low, high) max = -\u221e start = -1, end = -1 for i = low to high sum = 0 for j = i to high sum += A[j] if sum > max max = sum start = i end = j return (start, end, max)","title":"4.1-2"},{"location":"04-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-3","text":"Brute force: def find_maximum_subarray_brute_force(numbers, low, high): start = -1 end = -1 max_sum = -float('inf') for i in range(low, high + 1): current_sum = 0 for j in range(i, high + 1): current_sum += numbers[j] if current_sum > max_sum: max_sum = current_sum start, end = i, j return (start, end, max_sum) Divide and conquer: def find_maximum_subarray_divide_and_conquer(numbers, low, high): if len(numbers) == 0: return (-1, -1, -float('inf')) elif low == high: return (low, high, numbers[low]) else: middle = (low + high) // 2 left_maximum_subarray = find_maximum_subarray_divide_and_conquer( numbers, low, middle) right_maximum_subarray = find_maximum_subarray_divide_and_conquer( numbers, middle + 1, high) cross_maximum_subarray = find_max_crossing_subarray( numbers, low, middle, high) if (left_maximum_subarray[2] >= right_maximum_subarray[2] and left_maximum_subarray[2] >= cross_maximum_subarray[2]): return left_maximum_subarray elif (right_maximum_subarray[2] >= left_maximum_subarray[2] and right_maximum_subarray[2] >= cross_maximum_subarray[2]): return right_maximum_subarray else: return cross_maximum_subarray def find_max_crossing_subarray(numbers, low, middle, high): start = middle end = middle current_sum = 0 left_sum = -float('inf') right_sum = -float('inf') for i in range(middle, low - 1, -1): current_sum += numbers[i] if current_sum > left_sum: left_sum = current_sum start = i current_sum = 0 for i in range(middle + 1, high + 1): current_sum += numbers[i] if current_sum > right_sum: right_sum = current_sum end = i return (start, end, left_sum + right_sum) In my env, when $n_0 = 64$ gives the crossover point. I changed the base of the recursive algorithm, but that doesn't change the crossover point.","title":"4.1-3"},{"location":"04-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-4","text":"Currently the implementation already supports empty array, but it returns (-1, -1, -float('inf')) , we can change it to an empty array.","title":"4.1-4"},{"location":"04-Divide-and-Conquer/4.1-The-maximum-subarray-problem/#41-5","text":"def find_maximum_subarray_linear_time(numbers, low, high): if len(numbers) == 0: return (-1, -1, -float('inf')) start = -1 max_sum_start = -1 max_sum_end = -1 max_sum_so_far = -float('inf') max_sum_ending_here = -float('inf') for i in range(low, high + 1): if numbers[i] > max_sum_ending_here + numbers[i]: max_sum_ending_here = numbers[i] start = i else: max_sum_ending_here += numbers[i] if max_sum_ending_here > max_sum_so_far: max_sum_so_far = max_sum_ending_here max_sum_start = start max_sum_end = i return (max_sum_start, max_sum_end, max_sum_so_far)","title":"4.1-5"},{"location":"04-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/","text":"4.2 Strassen's algorithm for matrix multiplication 4.2-1 Step 1, we partition each of matrix A, B into four $\\frac{n}{2} \\text{ * } \\frac{n}{2}$ matrices. So we have: $$ A = \\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\\\ \\end{pmatrix}, A_{11} = \\begin{pmatrix} 1 \\end{pmatrix}, A_{12} = \\begin{pmatrix} 3 \\end{pmatrix}, A_{21} = \\begin{pmatrix} 7 \\end{pmatrix}, A_{22} = \\begin{pmatrix} 5 \\end{pmatrix} $$ $$ B = \\begin{pmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ \\end{pmatrix}, B_{11} = \\begin{pmatrix} 6 \\end{pmatrix}, B_{12} = \\begin{pmatrix} 8 \\end{pmatrix}, B_{21} = \\begin{pmatrix} 4 \\end{pmatrix}, B_{22} = \\begin{pmatrix} 2 \\end{pmatrix} $$ Step 2, we create 10 matrices: $$S_1 = B_{12} - B_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$ $$S_2 = A_{11} + A_{12} = \\begin{pmatrix} 4 \\end{pmatrix}$$ $$S_3 = A_{21} + A_{22} = \\begin{pmatrix} 12 \\end{pmatrix}$$ $$S_4 = B_{21} - B_{11} = \\begin{pmatrix} -2 \\end{pmatrix}$$ $$S_5 = A_{11} + A_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$ $$S_6 = B_{11} + B_{22} = \\begin{pmatrix} 8 \\end{pmatrix}$$ $$S_7 = A_{12} - A_{22} = \\begin{pmatrix} -2 \\end{pmatrix}$$ $$S_8 = B_{21} + B_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$ $$S_9 = A_{11} - A_{21} = \\begin{pmatrix} -6 \\end{pmatrix}$$ $$S_{10} = B_{11} + B_{12} = \\begin{pmatrix} 14 \\end{pmatrix}$$ Step 3, we compute the seven matrix products: $$P_1 = A_{11} * S_{1} = \\begin{pmatrix} 6 \\end{pmatrix}$$ $$P_2 = S_{2} * B_{22} = \\begin{pmatrix} 8 \\end{pmatrix}$$ $$P_3 = S_{3} * B_{11} = \\begin{pmatrix} 72 \\end{pmatrix}$$ $$P_4 = A_{22} * S_{4} = \\begin{pmatrix} -10 \\end{pmatrix}$$ $$P_5 = S_{5} * S_{6} = \\begin{pmatrix} 48 \\end{pmatrix}$$ $$P_6 = S_{7} * S_{8} = \\begin{pmatrix} -12 \\end{pmatrix}$$ $$P_7 = S_{9} * S_{10} = \\begin{pmatrix} -84 \\end{pmatrix}$$ Step 4, compute the desired submatrices: $$C_{11} = P_5 + P_4 - P_2 + P_6 = \\begin{pmatrix} 18 \\end{pmatrix}$$ $$C_{12} = P_1 + P_2 = \\begin{pmatrix} 14 \\end{pmatrix}$$ $$C_{21} = P_3 + P_4 = \\begin{pmatrix} 62 \\end{pmatrix}$$ $$C_{22} = P_5 + P_1 - P_3 - P_7 = \\begin{pmatrix} 66 \\end{pmatrix}$$ So $C = \\begin{pmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\\\ \\end{pmatrix} = \\begin{pmatrix} 18 & 14 \\\\ 62 & 66 \\\\ \\end{pmatrix}$. 4.2-2 SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A, B) n = A.rows let C be a new n * n matrix if n == 1 C11 = A11 * B11 else partition A, B, and C as in equations (4.9) S1 = B12 - B22 S2 = A11 + A12 S3 = A21 + A22 S4 = B21 - B11 S5 = A11 + A22 S6 = B11 + B22 S7 = A12 - A22 S8 = B21 + B22 S9 = A11 - A21 S10 = B11 + B12 P1 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A11, S1) P2 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S2, B22) P3 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S3, B11) P4 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A22, S4) P5 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S5, S6) P6 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S7, S8) P7 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S9, S10) C11 = P5 + P4 - P2 + P6 C12 = P1 + P2 C21 = P3 + P4 C22 = P5 + P1 - P3 - P7 return C def square_matrix_multiply_strassen_algorithm(a, b): n = len(a) c = [[0] * n for i in range(n)] if n == 1: c[0][0] = a[0][0] * b[0][0] else: half = n // 2 a11 = [[0] * half for i in range(half)] a12 = [[0] * half for i in range(half)] a21 = [[0] * half for i in range(half)] a22 = [[0] * half for i in range(half)] b11 = [[0] * half for i in range(half)] b12 = [[0] * half for i in range(half)] b21 = [[0] * half for i in range(half)] b22 = [[0] * half for i in range(half)] for i in range(half): for j in range(half): a11[i][j] = a[i][j] a12[i][j] = a[i][j + half] a21[i][j] = a[i + half][j] a22[i][j] = a[i + half][j + half] b11[i][j] = b[i][j] b12[i][j] = b[i][j + half] b21[i][j] = b[i + half][j] b22[i][j] = b[i + half][j + half] s1 = subtract(b12, b22) s2 = add(a11, a12) s3 = add(a21, a22) s4 = subtract(b21, b11) s5 = add(a11, a22) s6 = add(b11, b22) s7 = subtract(a12, a22) s8 = add(b21, b22) s9 = subtract(a11, a21) s10 = add(b11, b12) p1 = square_matrix_multiply_strassen_algorithm(a11, s1) p2 = square_matrix_multiply_strassen_algorithm(s2, b22) p3 = square_matrix_multiply_strassen_algorithm(s3, b11) p4 = square_matrix_multiply_strassen_algorithm(a22, s4) p5 = square_matrix_multiply_strassen_algorithm(s5, s6) p6 = square_matrix_multiply_strassen_algorithm(s7, s8) p7 = square_matrix_multiply_strassen_algorithm(s9, s10) c11 = add(subtract(add(p5, p4), p2), p6) c12 = add(p1, p2) c21 = add(p3, p4) c22 = subtract(subtract(add(p5, p1), p3), p7) for i in range(half): for j in range(half): c[i][j] = c11[i][j] c[i][j + half] = c12[i][j] c[i + half][j] = c21[i][j] c[i + half][j + half] = c22[i][j] return c def add(a, b): n = len(a) c = [[0] * n for i in range(n)] for i in range(n): for j in range(n): c[i][j] = a[i][j] + b[i][j] return c def subtract(a, b): n = len(a) c = [[0] * n for i in range(n)] for i in range(n): for j in range(n): c[i][j] = a[i][j] - b[i][j] return c 4.2-3 If n is not an exact power of 2, then we can pad 0 to matrix to make it an exact power of 2. Let $k = \\lfloor \\lg{n} \\rfloor$, Let $m = 2^{k + 1}$, so we have a new m * m matrix. Thus, $T(m) = 7T(\\frac{m}{2}) + \\Theta(m^2)$. According to master method, we have $T(m) = \\Theta(m^{\\lg7})$. So there exist postive constants $c_1$, $c_2$ and $n_0$ such that $c_1m^{\\lg7} \\leq T(m) \\leq c_2m^{\\lg7} \\text{ for all } n \\geq n_0$. We have $m^{\\lg7} = (2^{\\lfloor \\lg{n} \\rfloor + 1})^{\\lg7} \\leq (2^{\\lg{n} + 1})^{\\lg7} = 7n^{\\lg7}$. And $m^{\\lg7} = (2^{\\lfloor \\lg{n} \\rfloor + 1})^{\\lg7} \\geq (2^{\\lg{n} - 1 + 1})^{\\lg7} = n^{\\lg7}$. So $min(c_1, 1)n^{\\lg7} \\leq T(m) \\leq max(c_2, 7)n^{\\lg7} \\text{ for all } n \\geq n_0$. Thus $T(m) = \\Theta(n^{\\lg7})$, the resulting algorithm still runs in time $\\Theta(n^{\\lg7})$. 4.2-4 We have $T(n) = kT(\\frac{n}{3}) + \\Theta(n^2)$ with $b = 3, a = k$. If we want to run the algorithm in time $o(n^{\\lg7})$, then case 3 cannot apply, since $f(n) = \\Omega(n^{\\lg7})$. If case 2 applies, then $f(n) = \\Theta(n^{\\log_3k})$, so $\\log_3k = 2$, but the algorithm runs in time $\\Theta(n^2\\lg{n})$. So case 1 applies, the algorithm runs in time $\\Theta(n^{\\log_3k}) = \\Theta(n^{\\frac{\\lg{k}}{\\lg3}})$. If we want the algorithm to runs in time $o(n^{\\lg7})$, then we have $\\log_3k < \\lg7, k < 3^{\\lg7}$, so the largest k is 21. 4.2-5 We know the best running time of multiplying b * b matrices using a multiplications is $\\Theta(n^{\\log_ba})$. So $\\log_68{132464} = 2.7951284873613815$, $\\log_70{143640} = 2.795122689748337$, $\\log_72{155424} = 2.795147391093449$. So multiplying 72 * 72 matrices using 155,424 multiplications yields the best asymptotic running time. Since $\\lg7 = 2.807354922057604$, so the new algorithm is faster than Strassen's algorithm. 4.2-6 We can partition A, B into k n * n matrices, and partition C into $k^2$ n * n matrices: $$ A = \\begin{pmatrix} A_{11} \\\\ A_{21} \\\\ \\ldots \\\\ A_{k1} \\end{pmatrix}, B = \\begin{pmatrix} B_{11} & B_{12} & \\ldots & B_{1k} \\end{pmatrix}, C = \\begin{pmatrix} C_{11} & C_{12} & \\ldots & C_{1k} \\\\ \\ldots \\\\ C_{k1} & C_{k2} & \\ldots & C_{kk} \\end{pmatrix} $$ So that we rewrite the equation C = A * B as: $$ \\begin{pmatrix} C_{11} & C_{12} & \\ldots & C_{1k} \\\\ \\ldots \\\\ C_{k1} & C_{k2} & \\ldots & C_{kk} \\end{pmatrix} = \\begin{pmatrix} A_{11} \\\\ A_{21} \\\\ \\ldots \\\\ A_{k1} \\end{pmatrix} * \\begin{pmatrix} B_{11} & B_{12} & \\ldots & B_{1k} \\end{pmatrix}, C_{ij} = A_{i1} * B_{1j} $$ Then we can use Strassen's algorithm as a subroutine to calculate $C_{ij}$. let C be a new kn * kn matrix for i = 1 to k for j = 1 to k Cij = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(Ai1 * B1j) return C If the input matrices are reversed, we can partition A, B into k n * n matrices: $$ A = \\begin{pmatrix} A_{11} & A_{12} & \\ldots & A_{1k} \\end{pmatrix}, B = \\begin{pmatrix} B_{11} \\\\ B_{21} \\\\ \\ldots \\\\ B_{k1} \\end{pmatrix} $$ So that we rewrite the equation C = A * B as: $$ C = \\begin{pmatrix} A_{11} & A_{12} & \\ldots & A_{1k} \\end{pmatrix} * \\begin{pmatrix} B_{11} \\\\ B_{21} \\\\ \\ldots \\\\ B_{k1} \\end{pmatrix} $$ Then we can use Strassen's algorithm as a subroutine to calculate C. let C be a new n * n matrix for i = 1 to k C11 += SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A1i * Bi1) return C 4.2-7 MULTIPLY-COMPLEX-NUMBERS(a, b, c, d) n1 = (a + b) * (c + d) n2 = a * c n3 = b * d return (n2 - n3, n1 - n2 - n3)","title":"4.2 Strassen's algorithm for matrix multiplication"},{"location":"04-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-strassens-algorithm-for-matrix-multiplication","text":"","title":"4.2 Strassen's algorithm for matrix multiplication"},{"location":"04-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-1","text":"Step 1, we partition each of matrix A, B into four $\\frac{n}{2} \\text{ * } \\frac{n}{2}$ matrices. So we have: $$ A = \\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\\\ \\end{pmatrix}, A_{11} = \\begin{pmatrix} 1 \\end{pmatrix}, A_{12} = \\begin{pmatrix} 3 \\end{pmatrix}, A_{21} = \\begin{pmatrix} 7 \\end{pmatrix}, A_{22} = \\begin{pmatrix} 5 \\end{pmatrix} $$ $$ B = \\begin{pmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\ \\end{pmatrix}, B_{11} = \\begin{pmatrix} 6 \\end{pmatrix}, B_{12} = \\begin{pmatrix} 8 \\end{pmatrix}, B_{21} = \\begin{pmatrix} 4 \\end{pmatrix}, B_{22} = \\begin{pmatrix} 2 \\end{pmatrix} $$ Step 2, we create 10 matrices: $$S_1 = B_{12} - B_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$ $$S_2 = A_{11} + A_{12} = \\begin{pmatrix} 4 \\end{pmatrix}$$ $$S_3 = A_{21} + A_{22} = \\begin{pmatrix} 12 \\end{pmatrix}$$ $$S_4 = B_{21} - B_{11} = \\begin{pmatrix} -2 \\end{pmatrix}$$ $$S_5 = A_{11} + A_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$ $$S_6 = B_{11} + B_{22} = \\begin{pmatrix} 8 \\end{pmatrix}$$ $$S_7 = A_{12} - A_{22} = \\begin{pmatrix} -2 \\end{pmatrix}$$ $$S_8 = B_{21} + B_{22} = \\begin{pmatrix} 6 \\end{pmatrix}$$ $$S_9 = A_{11} - A_{21} = \\begin{pmatrix} -6 \\end{pmatrix}$$ $$S_{10} = B_{11} + B_{12} = \\begin{pmatrix} 14 \\end{pmatrix}$$ Step 3, we compute the seven matrix products: $$P_1 = A_{11} * S_{1} = \\begin{pmatrix} 6 \\end{pmatrix}$$ $$P_2 = S_{2} * B_{22} = \\begin{pmatrix} 8 \\end{pmatrix}$$ $$P_3 = S_{3} * B_{11} = \\begin{pmatrix} 72 \\end{pmatrix}$$ $$P_4 = A_{22} * S_{4} = \\begin{pmatrix} -10 \\end{pmatrix}$$ $$P_5 = S_{5} * S_{6} = \\begin{pmatrix} 48 \\end{pmatrix}$$ $$P_6 = S_{7} * S_{8} = \\begin{pmatrix} -12 \\end{pmatrix}$$ $$P_7 = S_{9} * S_{10} = \\begin{pmatrix} -84 \\end{pmatrix}$$ Step 4, compute the desired submatrices: $$C_{11} = P_5 + P_4 - P_2 + P_6 = \\begin{pmatrix} 18 \\end{pmatrix}$$ $$C_{12} = P_1 + P_2 = \\begin{pmatrix} 14 \\end{pmatrix}$$ $$C_{21} = P_3 + P_4 = \\begin{pmatrix} 62 \\end{pmatrix}$$ $$C_{22} = P_5 + P_1 - P_3 - P_7 = \\begin{pmatrix} 66 \\end{pmatrix}$$ So $C = \\begin{pmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\\\ \\end{pmatrix} = \\begin{pmatrix} 18 & 14 \\\\ 62 & 66 \\\\ \\end{pmatrix}$.","title":"4.2-1"},{"location":"04-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-2","text":"SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A, B) n = A.rows let C be a new n * n matrix if n == 1 C11 = A11 * B11 else partition A, B, and C as in equations (4.9) S1 = B12 - B22 S2 = A11 + A12 S3 = A21 + A22 S4 = B21 - B11 S5 = A11 + A22 S6 = B11 + B22 S7 = A12 - A22 S8 = B21 + B22 S9 = A11 - A21 S10 = B11 + B12 P1 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A11, S1) P2 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S2, B22) P3 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S3, B11) P4 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A22, S4) P5 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S5, S6) P6 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S7, S8) P7 = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(S9, S10) C11 = P5 + P4 - P2 + P6 C12 = P1 + P2 C21 = P3 + P4 C22 = P5 + P1 - P3 - P7 return C def square_matrix_multiply_strassen_algorithm(a, b): n = len(a) c = [[0] * n for i in range(n)] if n == 1: c[0][0] = a[0][0] * b[0][0] else: half = n // 2 a11 = [[0] * half for i in range(half)] a12 = [[0] * half for i in range(half)] a21 = [[0] * half for i in range(half)] a22 = [[0] * half for i in range(half)] b11 = [[0] * half for i in range(half)] b12 = [[0] * half for i in range(half)] b21 = [[0] * half for i in range(half)] b22 = [[0] * half for i in range(half)] for i in range(half): for j in range(half): a11[i][j] = a[i][j] a12[i][j] = a[i][j + half] a21[i][j] = a[i + half][j] a22[i][j] = a[i + half][j + half] b11[i][j] = b[i][j] b12[i][j] = b[i][j + half] b21[i][j] = b[i + half][j] b22[i][j] = b[i + half][j + half] s1 = subtract(b12, b22) s2 = add(a11, a12) s3 = add(a21, a22) s4 = subtract(b21, b11) s5 = add(a11, a22) s6 = add(b11, b22) s7 = subtract(a12, a22) s8 = add(b21, b22) s9 = subtract(a11, a21) s10 = add(b11, b12) p1 = square_matrix_multiply_strassen_algorithm(a11, s1) p2 = square_matrix_multiply_strassen_algorithm(s2, b22) p3 = square_matrix_multiply_strassen_algorithm(s3, b11) p4 = square_matrix_multiply_strassen_algorithm(a22, s4) p5 = square_matrix_multiply_strassen_algorithm(s5, s6) p6 = square_matrix_multiply_strassen_algorithm(s7, s8) p7 = square_matrix_multiply_strassen_algorithm(s9, s10) c11 = add(subtract(add(p5, p4), p2), p6) c12 = add(p1, p2) c21 = add(p3, p4) c22 = subtract(subtract(add(p5, p1), p3), p7) for i in range(half): for j in range(half): c[i][j] = c11[i][j] c[i][j + half] = c12[i][j] c[i + half][j] = c21[i][j] c[i + half][j + half] = c22[i][j] return c def add(a, b): n = len(a) c = [[0] * n for i in range(n)] for i in range(n): for j in range(n): c[i][j] = a[i][j] + b[i][j] return c def subtract(a, b): n = len(a) c = [[0] * n for i in range(n)] for i in range(n): for j in range(n): c[i][j] = a[i][j] - b[i][j] return c","title":"4.2-2"},{"location":"04-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-3","text":"If n is not an exact power of 2, then we can pad 0 to matrix to make it an exact power of 2. Let $k = \\lfloor \\lg{n} \\rfloor$, Let $m = 2^{k + 1}$, so we have a new m * m matrix. Thus, $T(m) = 7T(\\frac{m}{2}) + \\Theta(m^2)$. According to master method, we have $T(m) = \\Theta(m^{\\lg7})$. So there exist postive constants $c_1$, $c_2$ and $n_0$ such that $c_1m^{\\lg7} \\leq T(m) \\leq c_2m^{\\lg7} \\text{ for all } n \\geq n_0$. We have $m^{\\lg7} = (2^{\\lfloor \\lg{n} \\rfloor + 1})^{\\lg7} \\leq (2^{\\lg{n} + 1})^{\\lg7} = 7n^{\\lg7}$. And $m^{\\lg7} = (2^{\\lfloor \\lg{n} \\rfloor + 1})^{\\lg7} \\geq (2^{\\lg{n} - 1 + 1})^{\\lg7} = n^{\\lg7}$. So $min(c_1, 1)n^{\\lg7} \\leq T(m) \\leq max(c_2, 7)n^{\\lg7} \\text{ for all } n \\geq n_0$. Thus $T(m) = \\Theta(n^{\\lg7})$, the resulting algorithm still runs in time $\\Theta(n^{\\lg7})$.","title":"4.2-3"},{"location":"04-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-4","text":"We have $T(n) = kT(\\frac{n}{3}) + \\Theta(n^2)$ with $b = 3, a = k$. If we want to run the algorithm in time $o(n^{\\lg7})$, then case 3 cannot apply, since $f(n) = \\Omega(n^{\\lg7})$. If case 2 applies, then $f(n) = \\Theta(n^{\\log_3k})$, so $\\log_3k = 2$, but the algorithm runs in time $\\Theta(n^2\\lg{n})$. So case 1 applies, the algorithm runs in time $\\Theta(n^{\\log_3k}) = \\Theta(n^{\\frac{\\lg{k}}{\\lg3}})$. If we want the algorithm to runs in time $o(n^{\\lg7})$, then we have $\\log_3k < \\lg7, k < 3^{\\lg7}$, so the largest k is 21.","title":"4.2-4"},{"location":"04-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-5","text":"We know the best running time of multiplying b * b matrices using a multiplications is $\\Theta(n^{\\log_ba})$. So $\\log_68{132464} = 2.7951284873613815$, $\\log_70{143640} = 2.795122689748337$, $\\log_72{155424} = 2.795147391093449$. So multiplying 72 * 72 matrices using 155,424 multiplications yields the best asymptotic running time. Since $\\lg7 = 2.807354922057604$, so the new algorithm is faster than Strassen's algorithm.","title":"4.2-5"},{"location":"04-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-6","text":"We can partition A, B into k n * n matrices, and partition C into $k^2$ n * n matrices: $$ A = \\begin{pmatrix} A_{11} \\\\ A_{21} \\\\ \\ldots \\\\ A_{k1} \\end{pmatrix}, B = \\begin{pmatrix} B_{11} & B_{12} & \\ldots & B_{1k} \\end{pmatrix}, C = \\begin{pmatrix} C_{11} & C_{12} & \\ldots & C_{1k} \\\\ \\ldots \\\\ C_{k1} & C_{k2} & \\ldots & C_{kk} \\end{pmatrix} $$ So that we rewrite the equation C = A * B as: $$ \\begin{pmatrix} C_{11} & C_{12} & \\ldots & C_{1k} \\\\ \\ldots \\\\ C_{k1} & C_{k2} & \\ldots & C_{kk} \\end{pmatrix} = \\begin{pmatrix} A_{11} \\\\ A_{21} \\\\ \\ldots \\\\ A_{k1} \\end{pmatrix} * \\begin{pmatrix} B_{11} & B_{12} & \\ldots & B_{1k} \\end{pmatrix}, C_{ij} = A_{i1} * B_{1j} $$ Then we can use Strassen's algorithm as a subroutine to calculate $C_{ij}$. let C be a new kn * kn matrix for i = 1 to k for j = 1 to k Cij = SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(Ai1 * B1j) return C If the input matrices are reversed, we can partition A, B into k n * n matrices: $$ A = \\begin{pmatrix} A_{11} & A_{12} & \\ldots & A_{1k} \\end{pmatrix}, B = \\begin{pmatrix} B_{11} \\\\ B_{21} \\\\ \\ldots \\\\ B_{k1} \\end{pmatrix} $$ So that we rewrite the equation C = A * B as: $$ C = \\begin{pmatrix} A_{11} & A_{12} & \\ldots & A_{1k} \\end{pmatrix} * \\begin{pmatrix} B_{11} \\\\ B_{21} \\\\ \\ldots \\\\ B_{k1} \\end{pmatrix} $$ Then we can use Strassen's algorithm as a subroutine to calculate C. let C be a new n * n matrix for i = 1 to k C11 += SQUARE-MATRIX-MULTIPLY-STRASSEN-ALGORITHM(A1i * Bi1) return C","title":"4.2-6"},{"location":"04-Divide-and-Conquer/4.2-Strassen's-algorithm-for-matrix-multiplication/#42-7","text":"MULTIPLY-COMPLEX-NUMBERS(a, b, c, d) n1 = (a + b) * (c + d) n2 = a * c n3 = b * d return (n2 - n3, n1 - n2 - n3)","title":"4.2-7"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/","text":"4.3 The substitution method for solving recurrences 4.3-1 We start by assuming that this bound holds for all positive m < n, in particular for m = n - 1, yielding $T(n - 1) \\leq c(n - 1)^2$. Substituting into the recurrence yields: $$T(n) = T(n - 1) + n \\leq c(n - 1)^2 + n = cn^2 + (1 - 2c)n + c \\leq cn^2$$ where the last step holds as long as $c > \\frac{1}{2}$ and $n \\geq \\frac{c}{2c - 1}$. 4.3-2 We start by assuming that $T(n) \\leq c\\lg{n}$ holds for all positive m < n, in particular for $m = \\lceil \\frac{n}{2} \\rceil$, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\leq c\\lg{\\lceil \\frac{n}{2} \\rceil}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& T(\\lceil \\frac{n}{2} \\rceil) + 1 \\\\ &\\leq& c\\lg{\\lceil \\frac{n}{2} \\rceil} + 1 \\leq c(\\lg{(\\frac{n + (2 - 1)}{2})}) + 1 \\text{ (inequation 3.6) } \\\\ &=& c\\lg{(n + 1)} - c + 1 \\end{eqnarray} $$ But it's not easy to prove that $c\\lg{(n + 1)} - c + 1 \\leq c\\lg{n}$, so we reguess $T(n) = O(\\lg{(n - 1)})$, since if $T(n) = O(\\lg{(n - 1)})$, then it's obviously $O(\\lg{n})$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &\\leq& c\\lg{(\\lceil \\frac{n}{2} \\rceil -1)} + 1 \\\\ &\\leq& c\\lg{(\\frac{n + (2 - 1)}{2} -1)} + 1 \\\\ &=& c\\lg{(n - 1)} - c + 1 \\leq c\\lg{(n - 1)} < c\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $c \\geq 1$. 4.3-3 We start by assuming that $T(n) \\geq cn\\lg{n}$ holds for all positive m < n, in particular for $m = \\lfloor \\frac{n}{2} \\rfloor$, yielding $T(\\lfloor \\frac{n}{2} \\rfloor) \\geq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields: $$T(n) = 2T(\\lfloor \\frac{n}{2} \\rfloor) + n \\geq 2c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor} + n \\geq cn\\lg(\\frac{n}{2}) + n = cn\\lg{n} + (1 - c)n \\geq cn\\lg{n}$$ where the last step holds as long as $c \\leq 1$. Since $T(n) = O(n\\lg{n})$ and $T(n) = \\Omega(n\\lg{n})$, so $T(n) = \\Theta(n\\lg{n})$. 4.3-4 We can choose $T(n) = O(n\\lg{n} + 1)$, then $T(n) \\leq cn\\lg{n} + 1$. It holds for the base case, since $c1\\lg{1} + 1 = 1 \\geq T(1)$. 4.3-5 First, let's prove $T(n) = O(n\\lg{n})$. We start by assuming that $T(n) \\leq cn\\lg{n}$ holds for all positive m < n, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\leq c\\lceil \\frac{n}{2} \\rceil\\lg{\\lceil \\frac{n}{2} \\rceil}$ and $T(\\lfloor \\frac{n}{2} \\rfloor) \\leq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\ &\\leq& c\\lceil \\frac{n}{2} \\rceil\\lg{\\lceil \\frac{n}{2} \\rceil} + c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor} + \\Theta(n) \\\\ &\\leq& c(\\frac{n + (2 - 1)}{2})\\lg{(\\frac{n + (2 - 1)}{2})} + c(\\frac{n}{2})\\lg{(\\frac{n}{2})} + \\Theta(n) \\\\ &=& c\\frac{n + 1}{2}\\lg{(n + 1)} + c\\frac{n}{2}\\lg{n} - cn + \\Theta(n) - \\frac{c}{2} \\end{eqnarray} $$ It's also not easy to prove that $T(n) \\leq cn\\lg{n}$, so we try to guess $T(n) = O((n - 1)\\lg{(n - 1)})$, so: $$ \\begin{eqnarray} T(n) &=& T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\ &\\leq& c(\\lceil \\frac{n}{2} \\rceil - 1)\\lg{(\\lceil \\frac{n}{2} \\rceil - 1)} + c(\\lfloor \\frac{n}{2} \\rfloor - 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor - 1)} + \\Theta(n) \\\\ &\\leq& c(\\frac{n + (2 - 1)}{2} - 1)\\lg{(\\frac{n + (2 - 1)}{2} - 1)} + c(\\frac{n}{2} - 1)\\lg{(\\frac{n}{2} - 1)} + \\Theta(n) \\\\ &=& c\\frac{n - 1}{2}\\lg{(n - 1)} + c\\frac{n - 2}{2}\\lg{(n - 2)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &<& c\\frac{n - 1}{2}\\lg{(n - 1)} + c\\frac{n - 1}{2}\\lg{(n - 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &=& c(n - 1)\\lg{(n - 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &\\leq& c(n - 1)\\lg{(n - 1)} - cn + c_1n - \\frac{3c}{2} \\\\ &=& c(n - 1)\\lg{(n - 1)} + (c_1 - c)n - \\frac{3c}{2} \\\\ &<& c(n - 1)\\lg{(n - 1)} \\\\ &<& cn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $c > c_1$ and $n \\geq \\frac{3c}{2(c - c_1)}$. Then let's prove $T(n) = \\Omega(n\\lg{n})$. We start by assuming that $T(n) \\geq c(n + 1)\\lg{(n + 1)}$ holds for all positive m < n, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\geq c(\\lceil \\frac{n}{2} \\rceil + 1)\\lg{(\\lceil \\frac{n}{2} \\rceil + 1)}$ and $T(\\lfloor \\frac{n}{2} \\rfloor) \\geq c(\\lfloor \\frac{n}{2} \\rfloor + 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 1)}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\ &\\geq& c(\\lceil \\frac{n}{2} \\rceil + 1)\\lg{(\\lceil \\frac{n}{2} \\rceil + 1)} + c(\\lfloor \\frac{n}{2} \\rfloor + 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 1)} + \\Theta(n) \\\\ &\\geq& c(\\frac{n}{2} + 1)\\lg{(\\frac{n}{2} + 1)} + c(\\frac{n - (2 - 1)}{2} + 1)\\lg{(\\frac{n - (2 - 1)}{2} + 1)} + \\Theta(n) \\\\ &=& c\\frac{n + 2}{2}\\lg{(n + 2)} + c\\frac{n + 1}{2}\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &>& c\\frac{n + 1}{2}\\lg{(n + 1)} + c\\frac{n + 1}{2}\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &=& c(n + 1)\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &\\geq& c(n + 1)\\lg{(n + 1)} - cn + c_1n - \\frac{3c}{2} \\\\ &=& c(n + 1)\\lg{(n + 1)} + (c_1 - c)n - \\frac{3c}{2} \\\\ &>& c(n + 1)\\lg{(n + 1)} \\\\ &>& cn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $c < c_1$ and $n \\geq \\frac{3c}{2(c_1 - c)} $. So $T(n) = \\Theta(n\\lg{n})$. 4.3-6 We start by assuming that $T(n) \\leq cn\\lg{n}$ holds for all positive m < n, in particular for $m = \\lfloor \\frac{n}{2} \\rfloor$, yielding $T(\\lfloor \\frac{n}{2} \\rfloor) \\leq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 2T(\\lfloor \\frac{n}{2} \\rfloor + 17) + n \\\\ &\\leq& 2c(\\lfloor \\frac{n}{2} \\rfloor + 17)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 17)} + n \\\\ &\\leq& 2c(\\frac{n}{2} + 17)\\lg{(\\frac{n}{2} + 17)} + n \\\\ &=& c(n + 34)\\lg{(n + 34)} + (1 - c)n - 34c \\end{eqnarray} $$ It's not easy to prove $T(n) \\leq cn\\lg{n}$. So we try to guess $T(n) = O((n - k)\\lg{(n - k)})$. But we don't know what k is now, substituting into the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 2T(\\lfloor \\frac{n}{2} \\rfloor + 17) + n \\\\ &\\leq& 2c(\\lfloor \\frac{n}{2} \\rfloor + 17 - k)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 17 - k)} + n \\\\ &\\leq& 2c(\\frac{n}{2} + 17 - k)\\lg{(\\frac{n}{2} + 17 - k)} + n \\\\ &=& c(n + 34 - 2k)\\lg{(n + 34 - 2k)} + (1 - c)n + (2k - 34)c \\end{eqnarray} $$ Let 34 - 2k = -k, we get k = 34. So $T(n) \\leq c(n - 34)\\lg{(n - 34)} + (1 - c)n + 34c < c(n - 34)\\lg{(n - 34)} < cn\\lg{n}$ where the last step holds as long as $c > 1$ and $n \\geq \\frac{34c}{c - 1}$. So $T(n) = O(n\\lg{n})$. 4.3-7 Here, we have a = 4, b = 3, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{4}}$. Since $n^{\\log_3{4}}$ is polynomially larger than f(n) (that is, $f(n) = O(n^{\\log_3{4} - \\epsilon})$ for $\\epsilon \\approx 0.2618595071429148$), case 1 applies, and $T(n) = \\Theta(n^{\\log_3{4}})$. We start by assuming that this bound holds for all positive m < n, in particular for $m = \\frac{n}{3}$, yielding $T(\\frac{n}{3}) \\leq c(\\frac{n}{3})^{\\log_3{4}}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{3}) + n \\\\ &\\leq& 4c(\\frac{n}{3})^{\\log_3{4}} + n \\\\ &=& cn^{\\log_3{4}} + n \\end{eqnarray} $$ So it fails, we cannot prove $T(n) \\leq cn^{\\log_3{4}}$. Then let's subtract n from our original guess, let's guess $T(n) \\leq cn^{\\log_3{4}} - n$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{3}) + n \\\\ &\\leq& 4(c(\\frac{n}{3})^{\\log_3{4}} - \\frac{n}{3}) + n \\\\ &=& cn^{\\log_3{4}} -\\frac{n}{3} \\\\ &<& cn^{\\log_3{4}} \\end{eqnarray} $$ where the last step holds for all n. 4.3-8 Here, we have a = 4, b = 2, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{4}} = n^2$. So case 2 applies, $T(n) = n^2\\lg{n}$. But it says it's $\\Theta(n^2)$ in the book, so I think it's an error here. If we want to keep the solution to $\\Theta(n^2)$, then f(n) should be $n$, not $n^2$. So $T(n) = 4T(\\frac{n}{2}) + n$. Since $n^2$ is polynomially larger than f(n) (that is, $f(n) = O(n^{2 - \\epsilon})$ for $\\epsilon = \\frac{1}{2})$, case 1 applies, and $T(n) = \\Theta(n^2)$. We start by assuming that this bound holds for all positive m < n, in particular for $m = \\frac{n}{2}$, yielding $T(\\frac{n}{2}) \\leq c(\\frac{n}{2})^2$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{2}) + n \\\\ &\\leq& 4c(\\frac{n}{2})^2 + n \\\\ &=& cn^2 + n \\end{eqnarray} $$ So we cannot prove $T(n) \\leq cn^2$. Then let's subtract n from our original guess, let's guess $T(n) \\leq cn^2 - n$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{2}) + n \\\\ &\\leq& 4(c(\\frac{n}{2})^2 - \\frac{n}{2}) + n \\\\ &=& cn^2 -n \\\\ &<& cn^2 \\end{eqnarray} $$ where the last step holds for all n. 4.3-9 Renaming $m = \\lg{n}$ yields $T(2^m) = 3T(2^{\\frac{m}{2}}) + m$. We can new rename $S(m) = T(2^m)$ to produce the new recurrence $S(m) = 3S(\\frac{m}{2}) + m$. We start by assuming that $S(n) \\leq cm\\lg{m}$ holds for all positive p < m, in particular for $p = \\frac{m}{2}$, yielding $S(\\frac{m}{2}) \\leq c\\frac{m}{2}\\lg{\\frac{m}{2}}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} S(m) &=& 3S(\\frac{m}{2}) + m \\\\ &\\leq& 3c\\frac{m}{2}\\lg{\\frac{m}{2}} + m \\\\ &=& \\frac{3c}{2}m\\lg{m} + (1 - \\frac{3c}{2})m \\end{eqnarray} $$ It's not easy to prove $\\frac{3c}{2}m\\lg{m} + (1 - \\frac{3c}{2})m \\leq cm\\lg{m}$, so let's try to guess $S(m) \\leq c\\frac{2m}{3}\\lg{m}$. So: $$ \\begin{eqnarray} S(m) &\\leq& 3c\\frac{m}{3}\\lg{\\frac{m}{2}} + m \\\\ &=& cm\\lg{m} + (1 - c)m \\\\ &\\leq& cm\\lg{m} \\end{eqnarray} $$ where the last step holds as long as $c \\geq 1$. So $S(m) = O(m\\lg{m})$, so $T(n) = O(\\lg{n}\\lg{\\lg{n}})$.","title":"4.3 The substitution method for solving recurrences"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-the-substitution-method-for-solving-recurrences","text":"","title":"4.3 The substitution method for solving recurrences"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-1","text":"We start by assuming that this bound holds for all positive m < n, in particular for m = n - 1, yielding $T(n - 1) \\leq c(n - 1)^2$. Substituting into the recurrence yields: $$T(n) = T(n - 1) + n \\leq c(n - 1)^2 + n = cn^2 + (1 - 2c)n + c \\leq cn^2$$ where the last step holds as long as $c > \\frac{1}{2}$ and $n \\geq \\frac{c}{2c - 1}$.","title":"4.3-1"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-2","text":"We start by assuming that $T(n) \\leq c\\lg{n}$ holds for all positive m < n, in particular for $m = \\lceil \\frac{n}{2} \\rceil$, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\leq c\\lg{\\lceil \\frac{n}{2} \\rceil}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& T(\\lceil \\frac{n}{2} \\rceil) + 1 \\\\ &\\leq& c\\lg{\\lceil \\frac{n}{2} \\rceil} + 1 \\leq c(\\lg{(\\frac{n + (2 - 1)}{2})}) + 1 \\text{ (inequation 3.6) } \\\\ &=& c\\lg{(n + 1)} - c + 1 \\end{eqnarray} $$ But it's not easy to prove that $c\\lg{(n + 1)} - c + 1 \\leq c\\lg{n}$, so we reguess $T(n) = O(\\lg{(n - 1)})$, since if $T(n) = O(\\lg{(n - 1)})$, then it's obviously $O(\\lg{n})$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &\\leq& c\\lg{(\\lceil \\frac{n}{2} \\rceil -1)} + 1 \\\\ &\\leq& c\\lg{(\\frac{n + (2 - 1)}{2} -1)} + 1 \\\\ &=& c\\lg{(n - 1)} - c + 1 \\leq c\\lg{(n - 1)} < c\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $c \\geq 1$.","title":"4.3-2"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-3","text":"We start by assuming that $T(n) \\geq cn\\lg{n}$ holds for all positive m < n, in particular for $m = \\lfloor \\frac{n}{2} \\rfloor$, yielding $T(\\lfloor \\frac{n}{2} \\rfloor) \\geq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields: $$T(n) = 2T(\\lfloor \\frac{n}{2} \\rfloor) + n \\geq 2c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor} + n \\geq cn\\lg(\\frac{n}{2}) + n = cn\\lg{n} + (1 - c)n \\geq cn\\lg{n}$$ where the last step holds as long as $c \\leq 1$. Since $T(n) = O(n\\lg{n})$ and $T(n) = \\Omega(n\\lg{n})$, so $T(n) = \\Theta(n\\lg{n})$.","title":"4.3-3"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-4","text":"We can choose $T(n) = O(n\\lg{n} + 1)$, then $T(n) \\leq cn\\lg{n} + 1$. It holds for the base case, since $c1\\lg{1} + 1 = 1 \\geq T(1)$.","title":"4.3-4"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-5","text":"First, let's prove $T(n) = O(n\\lg{n})$. We start by assuming that $T(n) \\leq cn\\lg{n}$ holds for all positive m < n, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\leq c\\lceil \\frac{n}{2} \\rceil\\lg{\\lceil \\frac{n}{2} \\rceil}$ and $T(\\lfloor \\frac{n}{2} \\rfloor) \\leq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\ &\\leq& c\\lceil \\frac{n}{2} \\rceil\\lg{\\lceil \\frac{n}{2} \\rceil} + c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor} + \\Theta(n) \\\\ &\\leq& c(\\frac{n + (2 - 1)}{2})\\lg{(\\frac{n + (2 - 1)}{2})} + c(\\frac{n}{2})\\lg{(\\frac{n}{2})} + \\Theta(n) \\\\ &=& c\\frac{n + 1}{2}\\lg{(n + 1)} + c\\frac{n}{2}\\lg{n} - cn + \\Theta(n) - \\frac{c}{2} \\end{eqnarray} $$ It's also not easy to prove that $T(n) \\leq cn\\lg{n}$, so we try to guess $T(n) = O((n - 1)\\lg{(n - 1)})$, so: $$ \\begin{eqnarray} T(n) &=& T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\ &\\leq& c(\\lceil \\frac{n}{2} \\rceil - 1)\\lg{(\\lceil \\frac{n}{2} \\rceil - 1)} + c(\\lfloor \\frac{n}{2} \\rfloor - 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor - 1)} + \\Theta(n) \\\\ &\\leq& c(\\frac{n + (2 - 1)}{2} - 1)\\lg{(\\frac{n + (2 - 1)}{2} - 1)} + c(\\frac{n}{2} - 1)\\lg{(\\frac{n}{2} - 1)} + \\Theta(n) \\\\ &=& c\\frac{n - 1}{2}\\lg{(n - 1)} + c\\frac{n - 2}{2}\\lg{(n - 2)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &<& c\\frac{n - 1}{2}\\lg{(n - 1)} + c\\frac{n - 1}{2}\\lg{(n - 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &=& c(n - 1)\\lg{(n - 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &\\leq& c(n - 1)\\lg{(n - 1)} - cn + c_1n - \\frac{3c}{2} \\\\ &=& c(n - 1)\\lg{(n - 1)} + (c_1 - c)n - \\frac{3c}{2} \\\\ &<& c(n - 1)\\lg{(n - 1)} \\\\ &<& cn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $c > c_1$ and $n \\geq \\frac{3c}{2(c - c_1)}$. Then let's prove $T(n) = \\Omega(n\\lg{n})$. We start by assuming that $T(n) \\geq c(n + 1)\\lg{(n + 1)}$ holds for all positive m < n, yielding $T(\\lceil \\frac{n}{2} \\rceil) \\geq c(\\lceil \\frac{n}{2} \\rceil + 1)\\lg{(\\lceil \\frac{n}{2} \\rceil + 1)}$ and $T(\\lfloor \\frac{n}{2} \\rfloor) \\geq c(\\lfloor \\frac{n}{2} \\rfloor + 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 1)}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& T(\\lceil \\frac{n}{2} \\rceil) + T(\\lfloor \\frac{n}{2} \\rfloor) + \\Theta(n) \\\\ &\\geq& c(\\lceil \\frac{n}{2} \\rceil + 1)\\lg{(\\lceil \\frac{n}{2} \\rceil + 1)} + c(\\lfloor \\frac{n}{2} \\rfloor + 1)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 1)} + \\Theta(n) \\\\ &\\geq& c(\\frac{n}{2} + 1)\\lg{(\\frac{n}{2} + 1)} + c(\\frac{n - (2 - 1)}{2} + 1)\\lg{(\\frac{n - (2 - 1)}{2} + 1)} + \\Theta(n) \\\\ &=& c\\frac{n + 2}{2}\\lg{(n + 2)} + c\\frac{n + 1}{2}\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &>& c\\frac{n + 1}{2}\\lg{(n + 1)} + c\\frac{n + 1}{2}\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &=& c(n + 1)\\lg{(n + 1)} - cn + \\Theta(n) - \\frac{3c}{2} \\\\ &\\geq& c(n + 1)\\lg{(n + 1)} - cn + c_1n - \\frac{3c}{2} \\\\ &=& c(n + 1)\\lg{(n + 1)} + (c_1 - c)n - \\frac{3c}{2} \\\\ &>& c(n + 1)\\lg{(n + 1)} \\\\ &>& cn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $c < c_1$ and $n \\geq \\frac{3c}{2(c_1 - c)} $. So $T(n) = \\Theta(n\\lg{n})$.","title":"4.3-5"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-6","text":"We start by assuming that $T(n) \\leq cn\\lg{n}$ holds for all positive m < n, in particular for $m = \\lfloor \\frac{n}{2} \\rfloor$, yielding $T(\\lfloor \\frac{n}{2} \\rfloor) \\leq c\\lfloor \\frac{n}{2} \\rfloor\\lg{\\lfloor \\frac{n}{2} \\rfloor}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 2T(\\lfloor \\frac{n}{2} \\rfloor + 17) + n \\\\ &\\leq& 2c(\\lfloor \\frac{n}{2} \\rfloor + 17)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 17)} + n \\\\ &\\leq& 2c(\\frac{n}{2} + 17)\\lg{(\\frac{n}{2} + 17)} + n \\\\ &=& c(n + 34)\\lg{(n + 34)} + (1 - c)n - 34c \\end{eqnarray} $$ It's not easy to prove $T(n) \\leq cn\\lg{n}$. So we try to guess $T(n) = O((n - k)\\lg{(n - k)})$. But we don't know what k is now, substituting into the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 2T(\\lfloor \\frac{n}{2} \\rfloor + 17) + n \\\\ &\\leq& 2c(\\lfloor \\frac{n}{2} \\rfloor + 17 - k)\\lg{(\\lfloor \\frac{n}{2} \\rfloor + 17 - k)} + n \\\\ &\\leq& 2c(\\frac{n}{2} + 17 - k)\\lg{(\\frac{n}{2} + 17 - k)} + n \\\\ &=& c(n + 34 - 2k)\\lg{(n + 34 - 2k)} + (1 - c)n + (2k - 34)c \\end{eqnarray} $$ Let 34 - 2k = -k, we get k = 34. So $T(n) \\leq c(n - 34)\\lg{(n - 34)} + (1 - c)n + 34c < c(n - 34)\\lg{(n - 34)} < cn\\lg{n}$ where the last step holds as long as $c > 1$ and $n \\geq \\frac{34c}{c - 1}$. So $T(n) = O(n\\lg{n})$.","title":"4.3-6"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-7","text":"Here, we have a = 4, b = 3, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{4}}$. Since $n^{\\log_3{4}}$ is polynomially larger than f(n) (that is, $f(n) = O(n^{\\log_3{4} - \\epsilon})$ for $\\epsilon \\approx 0.2618595071429148$), case 1 applies, and $T(n) = \\Theta(n^{\\log_3{4}})$. We start by assuming that this bound holds for all positive m < n, in particular for $m = \\frac{n}{3}$, yielding $T(\\frac{n}{3}) \\leq c(\\frac{n}{3})^{\\log_3{4}}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{3}) + n \\\\ &\\leq& 4c(\\frac{n}{3})^{\\log_3{4}} + n \\\\ &=& cn^{\\log_3{4}} + n \\end{eqnarray} $$ So it fails, we cannot prove $T(n) \\leq cn^{\\log_3{4}}$. Then let's subtract n from our original guess, let's guess $T(n) \\leq cn^{\\log_3{4}} - n$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{3}) + n \\\\ &\\leq& 4(c(\\frac{n}{3})^{\\log_3{4}} - \\frac{n}{3}) + n \\\\ &=& cn^{\\log_3{4}} -\\frac{n}{3} \\\\ &<& cn^{\\log_3{4}} \\end{eqnarray} $$ where the last step holds for all n.","title":"4.3-7"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-8","text":"Here, we have a = 4, b = 2, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{4}} = n^2$. So case 2 applies, $T(n) = n^2\\lg{n}$. But it says it's $\\Theta(n^2)$ in the book, so I think it's an error here. If we want to keep the solution to $\\Theta(n^2)$, then f(n) should be $n$, not $n^2$. So $T(n) = 4T(\\frac{n}{2}) + n$. Since $n^2$ is polynomially larger than f(n) (that is, $f(n) = O(n^{2 - \\epsilon})$ for $\\epsilon = \\frac{1}{2})$, case 1 applies, and $T(n) = \\Theta(n^2)$. We start by assuming that this bound holds for all positive m < n, in particular for $m = \\frac{n}{2}$, yielding $T(\\frac{n}{2}) \\leq c(\\frac{n}{2})^2$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{2}) + n \\\\ &\\leq& 4c(\\frac{n}{2})^2 + n \\\\ &=& cn^2 + n \\end{eqnarray} $$ So we cannot prove $T(n) \\leq cn^2$. Then let's subtract n from our original guess, let's guess $T(n) \\leq cn^2 - n$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{2}) + n \\\\ &\\leq& 4(c(\\frac{n}{2})^2 - \\frac{n}{2}) + n \\\\ &=& cn^2 -n \\\\ &<& cn^2 \\end{eqnarray} $$ where the last step holds for all n.","title":"4.3-8"},{"location":"04-Divide-and-Conquer/4.3-The-substitution-method-for-solving-recurrences/#43-9","text":"Renaming $m = \\lg{n}$ yields $T(2^m) = 3T(2^{\\frac{m}{2}}) + m$. We can new rename $S(m) = T(2^m)$ to produce the new recurrence $S(m) = 3S(\\frac{m}{2}) + m$. We start by assuming that $S(n) \\leq cm\\lg{m}$ holds for all positive p < m, in particular for $p = \\frac{m}{2}$, yielding $S(\\frac{m}{2}) \\leq c\\frac{m}{2}\\lg{\\frac{m}{2}}$. Substituting ino the recurrence yields: $$ \\begin{eqnarray} S(m) &=& 3S(\\frac{m}{2}) + m \\\\ &\\leq& 3c\\frac{m}{2}\\lg{\\frac{m}{2}} + m \\\\ &=& \\frac{3c}{2}m\\lg{m} + (1 - \\frac{3c}{2})m \\end{eqnarray} $$ It's not easy to prove $\\frac{3c}{2}m\\lg{m} + (1 - \\frac{3c}{2})m \\leq cm\\lg{m}$, so let's try to guess $S(m) \\leq c\\frac{2m}{3}\\lg{m}$. So: $$ \\begin{eqnarray} S(m) &\\leq& 3c\\frac{m}{3}\\lg{\\frac{m}{2}} + m \\\\ &=& cm\\lg{m} + (1 - c)m \\\\ &\\leq& cm\\lg{m} \\end{eqnarray} $$ where the last step holds as long as $c \\geq 1$. So $S(m) = O(m\\lg{m})$, so $T(n) = O(\\lg{n}\\lg{\\lg{n}})$.","title":"4.3-9"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/","text":"4.4 The recursion-tree method for solving recurrences 4.4-1 First let's create a recursion tree for the recurrence $T(n) = 3T(\\frac{n}{2}) + n$ and assume that n is an exact power of 2. Each level has three times more nodes than the level above, so the number of nodes at depth i is $3^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n}{2^i}$. So the total cost over all nodes at depth i, is $3^i\\frac{n}{2^i} = (\\frac{3}{2})^in$. The bottom level, at depth $\\lg{n}$, has $3^{\\lg{n}} = n^{\\lg{3}}$ nodes, each contributing cost $T(1)$, for a total cost of $n^{\\lg{3}}T(1)$, which is $\\Theta(n^{\\lg{3}})$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{n} - 1}(\\frac{3}{2})^in + \\Theta(n^{\\lg{3}}) \\\\ &=& n\\frac{1 - (\\frac{3}{2})^{\\lg{n}}}{1 - \\frac{3}{2}} + \\Theta(n^{\\lg3}) \\\\ &=& 2n\\frac{3^{\\lg{n}} - 2^{\\lg{n}}}{2^{\\lg{n}}} + \\Theta(n^{\\lg3}) \\\\ &=& 2n\\frac{3^{\\lg{n}} - 2^{\\lg{n}}}{n} + \\Theta(n^{\\lg3}) \\\\ &=& 2(3^{\\lg{n}} - 2^{\\lg{n}}) + \\Theta(n^{\\lg3}) \\\\ &=& 2(n^{\\lg{3}} -n) + \\Theta(n^{\\lg3}) \\\\ &<& 2n^{\\lg3} + \\Theta(n^{\\lg3}) \\\\ &=& O(n^{\\lg3}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n^{\\lg3})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^{\\lg3}$ for some constant $c > 0$. So: $$ \\begin{eqnarray} T(n) &=& 3T(\\lfloor \\frac{n}{2} \\rfloor) + n \\\\ &\\leq& 3c\\lfloor \\frac{n}{2} \\rfloor^{\\lg3} + n \\\\ &\\leq& 3c(\\frac{n}{2})^{\\lg3} + n \\\\ &=& 3c(\\frac{n}{2})^{\\lg3} + n \\\\ &=& 3c\\frac{n^{\\lg3}}{2^{\\lg3}} + n \\\\ &=& 3c\\frac{n^{\\lg3}}{3} + n \\\\ &=& cn^{\\lg{3}} + n \\end{eqnarray} $$ But $cn^{\\lg{3}} + n > cn^{\\lg{3}}$, so we need to try another guess. Let's try $T(n) \\leq cn^{\\lg3} - \\frac{2n}{3}$. So: $$ \\begin{eqnarray} T(n) &=& 3T(\\lfloor \\frac{n}{2} \\rfloor) + n \\\\ &\\leq& 3(c\\lfloor \\frac{n}{2} \\rfloor^{\\lg3} - \\frac{n}{3}) + n \\\\ &\\leq& 3(c(\\frac{n}{2})^{\\lg3} - \\frac{n}{3}) + n \\\\ &=& 3c(\\frac{n}{2})^{\\lg3} \\\\ &=& cn^{\\lg3} \\end{eqnarray} $$ 4.4-2 First let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{2}) + n^2$ and assume that n is an exact power of 2. The number of nodes at depth i is 1. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n^2}{4^i}$. So the total cost over all nodes at depth i, is $\\frac{n^2}{4^i}$. The bottom level, at depth $\\lg{n}$, has 1 node, which contributing cost $T(1)$, for a total cost of $T(1)$, which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{n} - 1}\\frac{n^2}{4^i} + \\Theta(1) \\\\ &<& \\sum_{i = 0}^{\\infty}\\frac{n^2}{4^i} + \\Theta(1) \\\\ &=& \\frac{1}{1 - \\frac{1}{4}}n^2 + \\Theta(1) \\\\ &=& \\frac{4}{3}n^2 + \\Theta(1) \\\\ &=& O(n^2) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^2$ for some constant $c > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + n^2 \\\\ &\\leq& c(\\frac{n}{2})^2 + n^2 \\\\ &=& (\\frac{c}{4} + 1)n^2 \\\\ &\\leq& cn^2 \\end{eqnarray} $$ where the last step holds as long as $c \\geq \\frac{4}{3}$. 4.4-3 When n is large, the difference between $\\frac{n}{2} + 2$ and $\\frac{n}{2}$ not that large, so it's also a sloppiness that we can tolerate. Then let's create a recursion tree for the recurrence $T(n) = 4T(\\frac{n}{2}) + n$ and assume that n is an exact power of 2. Each level has four times more nodes than the level above, so the number of nodes at depth i is $4^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n}{2^i}$. So the total cost over all nodes at depth i, is $4^i\\frac{n}{2^i} = 2^in$. The bottom level, at depth $\\lg{n}$, has $4^{\\lg{n}} = n^2$ nodes, each contributing cost $T(1)$, for a total cost of $n^2T(1)$, which is $\\Theta(n^2)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{n} - 1}2^in + \\Theta(n^2) \\\\ &=& n\\frac{1 - 2^{\\lg{n}}}{1 - 2} + \\Theta(n^2) \\\\ &=& n(n - 1) + \\Theta(n^2) \\\\ &=& n^2 - n + \\Theta(n^2) \\\\ &=& O(n^2) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^2$ for some constant $c > 0$. So: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{2} + 2) + n \\\\ &\\leq& 4c(\\frac{n}{2} + 2)^2 + n \\\\ &=& 4c(\\frac{n^2}{4} + 2n + 4) + n \\\\ &=& cn^2 + 8cn + 16c + n \\\\ &=& cn^2 + (8c + 1)n + 16c \\end{eqnarray} $$ But $cn^2 + (8c + 1)n + 16c > cn^2$, so we need to try another guess, let's try $T(n) \\leq cn^2 - 5n$. So: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{2} + 2) + n \\\\ &\\leq& 4c((\\frac{n}{2} + 2)^2 - 5(\\frac{n}{2} + 2)) + n \\\\ &=& 4c(\\frac{n^2}{4} + 2n + 4 - \\frac{5n}{2} - 10) + n \\\\ &=& 4c(\\frac{n^2}{4} - \\frac{n}{2} - 6) + n \\\\ &=& cn^2 + (1 - 2c)n - 24c \\\\ &<& cn^2 \\end{eqnarray} $$ where the last step holds as long as $c \\geq \\frac{1}{2}$. 4.4-4 First let's create a recursion tree for the recurrence $T(n) = 2T(n - 1) + 1$. Each level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, n - 1 - 1$, has a cost of 1. So the total cost over all nodes at depth i, is $2^i$. The bottom level, at depth n, has $2^{n - 1}$ nodes, each contributing cost $T(1)$, for a total cost of $2^{n - 1}T(1)$, which is $\\Theta(2^n)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{n - 1 - 1}2^i + \\Theta(2^n) \\\\ &=& \\frac{1 - 2^{n - 1}}{1 - 2} + \\Theta(2^n) \\\\ &=& 2^{n - 1} - 1 + \\Theta(2^n) \\\\ &=& O(2^n) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(2^n)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq c2^n$ for some constant $c > 0$. So: $$ \\begin{eqnarray} T(n) &=& 2T(n - 1) + 1 \\\\ &\\leq& 2c2^{n - 1} + 1 \\\\ &=& c2^n + 1 \\end{eqnarray} $$ But $c2^n + 1 > c2^n$, so we need to try another guess. Lets try $T(n) \\leq c2^n - 1$. So: $$ \\begin{eqnarray} T(n) &=& 2T(n - 1) + 1 \\\\ &\\leq& 2(c2^{n - 1} - 1) + 1 \\\\ &=& c2^n -1 \\\\ &<& c2^n \\end{eqnarray} $$ 4.4-5 4.4-6 First let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{3}) + T(\\frac{2n}{3})$ and assume that n is an exact power of 3. Each level has 2 times more nodes than the level above, so the number of nodes at depth i is $2^i$. But in this question, not all leaves reach at the bottom at the same time. The left most branch reaches the bottom first. So when the left most branch reaches the bottom, we assume other nodes also reach the bottom. Then the depth of recursion tree is $\\log_3{n}$. The total cost over all nodes at depth i, is $cn$ and the cost at bottom is also cn So: $$ \\begin{eqnarray} T(n) &\\geq& \\sum_{i = 0}^{\\log_3{n} - 1}cn + cn \\\\ &=& cn\\log_3{n} + cn \\\\ &=& \\frac{c}{\\lg3}n\\lg{n} + cn \\\\ &=& \\Theta(n\\lg{n}) \\end{eqnarray} $$ But remember we've cut some branches of the recursion tree, so $T(n) = \\Omega(n\\lg{n})$. 4.4-7 When n is large, the difference between $\\frac{n}{2}$ and $\\lfloor \\frac{n}{2} \\rfloor$ not that large, so it's also a sloppiness that we can tolerate. Then let's create a recursion tree for the recurrence $T(n) = 4T(\\frac{n}{2}) + cn$ and assume that n is an exact power of 2. Each level has 4 times more nodes than the level above, so the number of nodes at depth i is $4^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{cn}{2^i}$. So the total cost over all nodes at depth i, is $4^i\\frac{cn}{2^i} = c2^in$. The bottom level, at depth $\\lg{n}$, has $4^{\\lg{n}} = n^2$ nodes, each contributing cost $T(1)$, for a total cost of $n^2T(1)$, which is $\\Theta(n^2)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{n} - 1}c2^in + \\Theta(n^2) \\\\ &=& cn\\frac{1 - 2^{\\lg{n}}}{1 - 2} + \\Theta(n^2) \\\\ &=& \\Theta(n^2) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq dn^2$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& 4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\ &\\leq& 4T(\\frac{n}{2}) + cn \\\\ &\\leq& 4d(\\frac{n}{2})^2 + cn \\\\ &=& dn^2 + cn \\end{eqnarray} $$ But $dn^2 + cn > dn^2$, so let's try another guess, let's try $T(n) \\leq dn^2 - dn$. $$ \\begin{eqnarray} T(n) &=& 4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\ &\\leq& 4T(\\frac{n}{2}) + cn \\\\ &\\leq& 4d((\\frac{n}{2})^2 - \\frac{n}{2}) + cn \\\\ &=& dn^2 + (c - 2d)n \\\\ &\\leq& dn^2 \\end{eqnarray} $$ where the last step holds as long as $d \\geq \\frac{c}{2}$. So $T(n) = O(n^2)$. Then we want to show that $T(n) \\geq dn^2$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& 4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\ &\\geq& 4T(\\frac{n - (2 - 1)}{2}) + cn \\\\ &=& 4T(\\frac{n - 1}{2}) + cn \\\\ &\\geq& 4d(\\frac{n - 1}{2})^2 + cn \\\\ &=& dn^2 + (c - 2d)n + d \\\\ &>& dn^2 \\end{eqnarray} $$ where the last step holds as long as $d < \\frac{c}{2}$. So $T(n) = \\Omega(n^2)$, so $T(n) = \\Theta(n^2)$. 4.4-8 First let's create a recursion tree for the recurrence $T(n) = T(n - a) + T(a) + cn$ and assume that n - 1 is divisible by a. The total cost over all nodes at depth i, for $i = 0, 1, 2, \\ldots, \\frac{n - 1}{a} - 1$, is $c(n - (i - 1)a), \\text{ for } i \\geq 1$. The bottom level, at depth $\\frac{n - 1}{a}$, the total cost is $T(1) + ca$, which is $\\Theta(a)$. So: $$ \\begin{eqnarray} T(n) &=& cn + \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}c(n - (i - 1)a) + \\Theta(a) \\\\ &=& cn + \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}cn - \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}(i - 1)ca + \\Theta(a) \\\\ &=& \\frac{n - 1}{a}cn - \\frac{n^2 - 2a -3na + 3a + 2a^2 + 1}{2a}c \\\\ &=& \\frac{n^2 +3na - 3a - 2a^2 - 1}{2a}c \\\\ &=& \\Theta(n^2) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Theta(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq dn^2$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(n - a) + T(a) + cn \\\\ &\\leq& d(n - a)^2 + da^2 + cn \\\\ &=& dn^2 + (c - ad)n + (2a^2 - an)d \\\\ &\\leq& dn^2 \\end{eqnarray} $$ where the last step holds as long as $d \\geq \\frac{c}{a}$ and $n \\geq 2a$. So $T(n) = O(n^2)$. Then we want to show that $T(n) \\geq dn^2$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(n - a) + T(a) + cn \\\\ &\\geq& d(n - a)^2 + da^2 + cn \\\\ &=& dn^2 + (c - 2ad)n + 2a^2d \\\\ &>& dn^2 \\end{eqnarray} $$ where the last step holds as long as $d < \\frac{c}{2a}$. So $T(n) = \\Omega(n^2)$. So $T(n) = \\Theta(n^2)$. 4.4-9 First let's create a recursion tree for the recurrence $T(n) = T(\\alpha{n}) + T((1 - \\alpha)n) + cn$. So we can see not each branch reaches at the bottom at the same time, it might be the left most branch reaches at the bottom first, or the right most branch reaches at the bottom first. It depends on which is smaller, $\\alpha$ or $1 - \\alpha$. Let $\\alpha \\leq 1 - \\alpha$, so we get $\\alpha \\leq \\frac{1}{2}$. That is, when $0 < \\alpha \\leq \\frac{1}{2}$, the left most branch reaches at the bottom first, when $\\frac{1}{2} < \\alpha < 1$, the right most branch reaches at the bottom first. If $0 < \\alpha \\leq \\frac{1}{2}$, when the left most branch reaches at the bottom, let's assume other branches also reach at the bottom, so the total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{\\alpha}{\\frac{1}{n}} - 1$, is cn. The cost at the bottom is also cn. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\geq& \\sum_{i = 0}^{\\log_{\\alpha}\\frac{1}{n} - 1}cn + cn \\\\ &=& cn(\\log_{\\alpha}\\frac{1}{n}) + cn \\\\ &=& cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} + cn \\\\ &>& cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Omega(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq dn\\lg{n}$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\geq& d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ &=& d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ &=& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ &\\geq& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d\\alpha\\lg{\\alpha}n + cn \\, (0 < \\alpha \\leq \\frac{1}{2}) \\\\ &=& dn\\lg{n} + (c + 2d\\alpha\\lg{\\alpha})n \\\\ &\\geq& dn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $d \\leq \\frac{c}{2\\alpha\\lg{\\frac{1}{\\alpha}}}$. Now let's check the right most branch. When it reaches at the bottom, other branches have already reached at the bottom, if the tree depth is k, let's assume other branches become T(1) at depth k - 1. Each level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And total cost over all nodes at depth i, for $i = 0, 1, 2, ..., \\log_{1 - \\alpha}\\frac{1}{n}$, is cn. The cost at the bottom is $T(1)$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\leq& \\sum_{i = 0}^{\\log_{1 - \\alpha}\\frac{1}{n} - 1}cn + \\Theta(1) \\\\ &=& cn(\\log_{1 - \\alpha}\\frac{1}{n}) + \\Theta(1) \\\\ &=& cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} + \\Theta(1) \\\\ &<& cn\\frac{\\lg{n}}{\\lg{\\frac{1}{1 - \\alpha}}} + n\\lg{n}\\\\ &=& O(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to veify that our guess was correct. We want to show that $T(n) \\leq dn\\lg{n}$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\leq& d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ &=& d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ &=& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ &\\leq& dn\\lg{n} + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\, (0 < \\alpha \\leq \\frac{1}{2}) \\\\ &=& dn\\lg{n} + (c + 2d(1 - \\alpha)\\lg{(1 - \\alpha)})n \\\\ &\\leq& dn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $d \\geq \\frac{c}{2(1 - \\alpha)\\lg{\\frac{1}{1 - \\alpha}}}$. So we proved $T(n) = \\Omega(n\\lg{n})$ and $T(n) = O(n\\lg{n})$. So $T(n) = \\Theta(n\\lg{n})$ when $0 < \\alpha \\leq \\frac{1}{2}$. Then let's check another case, when $\\frac{1}{2} < \\alpha < 1$. In this case, the right most branch reaches at the bottom first. Let's assume other branches also reach at the bottom, so the total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{1 - \\alpha}\\frac{1}{n}$, is cn. The cost at the bottom is cn. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\geq& \\sum_{i = 0}^{\\log_{1 - \\alpha}\\frac{1}{n} - 1}cn + cn \\\\ &=& cn(\\log_{1 - \\alpha}\\frac{1}{n}) + cn \\\\ &=& cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} + cn \\\\ &>& cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Omega(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq dn\\lg{n}$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\geq& d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ &=& d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ &=& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ &>& dn\\lg{n} + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\, (\\frac{1}{2} < \\alpha < 1) \\\\ &=& dn\\lg{n} + (c + 2d(1 - \\alpha)\\lg{(1 - \\alpha)})n \\\\ &\\geq& dn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $d \\leq \\frac{c}{2(1 - \\alpha)\\lg{\\frac{1}{1 - \\alpha}}}$. Now let's check the left most branch. When it reaches at the bottom, other branches have already reached at the bottom, if the tree depth is k, let's assume other branches become T(1) at depth k - 1. Each level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{\\alpha}\\frac{1}{n} - 1$, is cn. The cost at the bottom is T(1). So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &<& \\sum_{i = 0}^{\\log_{\\alpha}\\frac{1}{n} - 1}cn + \\Theta(1) \\\\ &=& cn(\\log_{\\alpha}\\frac{1}{n}) + \\Theta(1) \\\\ &=& cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{\\alpha}}} + \\Theta(1) \\\\ &<& cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} + n\\lg{n}\\\\ &=& O(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to veify that our guess was correct. We want to show that $T(n) \\leq dn\\lg{n}$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\leq& d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ &=& d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ &=& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ &<& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d\\alpha\\lg{\\alpha}n + cn \\, (\\frac{1}{2} < \\alpha < 1) \\\\ &=& dn\\lg{n} + (c + 2d\\alpha\\lg{\\alpha})n \\\\ &\\leq& dn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $d \\geq \\frac{c}{2\\alpha\\lg{\\frac{1}{\\alpha}}}$. So we proved $T(n) = \\Omega(n\\lg{n})$ and $T(n) = O(n\\lg{n})$. So $T(n) = \\Theta(n\\lg{n})$ when $\\frac{1}{2} < \\alpha < 1$. Thus $T(n) = \\Theta(n\\lg{n})$ for all $0 < \\alpha < 1$.","title":"4.4 The recursion-tree method for solving recurrences"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-the-recursion-tree-method-for-solving-recurrences","text":"","title":"4.4 The recursion-tree method for solving recurrences"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-1","text":"First let's create a recursion tree for the recurrence $T(n) = 3T(\\frac{n}{2}) + n$ and assume that n is an exact power of 2. Each level has three times more nodes than the level above, so the number of nodes at depth i is $3^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n}{2^i}$. So the total cost over all nodes at depth i, is $3^i\\frac{n}{2^i} = (\\frac{3}{2})^in$. The bottom level, at depth $\\lg{n}$, has $3^{\\lg{n}} = n^{\\lg{3}}$ nodes, each contributing cost $T(1)$, for a total cost of $n^{\\lg{3}}T(1)$, which is $\\Theta(n^{\\lg{3}})$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{n} - 1}(\\frac{3}{2})^in + \\Theta(n^{\\lg{3}}) \\\\ &=& n\\frac{1 - (\\frac{3}{2})^{\\lg{n}}}{1 - \\frac{3}{2}} + \\Theta(n^{\\lg3}) \\\\ &=& 2n\\frac{3^{\\lg{n}} - 2^{\\lg{n}}}{2^{\\lg{n}}} + \\Theta(n^{\\lg3}) \\\\ &=& 2n\\frac{3^{\\lg{n}} - 2^{\\lg{n}}}{n} + \\Theta(n^{\\lg3}) \\\\ &=& 2(3^{\\lg{n}} - 2^{\\lg{n}}) + \\Theta(n^{\\lg3}) \\\\ &=& 2(n^{\\lg{3}} -n) + \\Theta(n^{\\lg3}) \\\\ &<& 2n^{\\lg3} + \\Theta(n^{\\lg3}) \\\\ &=& O(n^{\\lg3}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n^{\\lg3})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^{\\lg3}$ for some constant $c > 0$. So: $$ \\begin{eqnarray} T(n) &=& 3T(\\lfloor \\frac{n}{2} \\rfloor) + n \\\\ &\\leq& 3c\\lfloor \\frac{n}{2} \\rfloor^{\\lg3} + n \\\\ &\\leq& 3c(\\frac{n}{2})^{\\lg3} + n \\\\ &=& 3c(\\frac{n}{2})^{\\lg3} + n \\\\ &=& 3c\\frac{n^{\\lg3}}{2^{\\lg3}} + n \\\\ &=& 3c\\frac{n^{\\lg3}}{3} + n \\\\ &=& cn^{\\lg{3}} + n \\end{eqnarray} $$ But $cn^{\\lg{3}} + n > cn^{\\lg{3}}$, so we need to try another guess. Let's try $T(n) \\leq cn^{\\lg3} - \\frac{2n}{3}$. So: $$ \\begin{eqnarray} T(n) &=& 3T(\\lfloor \\frac{n}{2} \\rfloor) + n \\\\ &\\leq& 3(c\\lfloor \\frac{n}{2} \\rfloor^{\\lg3} - \\frac{n}{3}) + n \\\\ &\\leq& 3(c(\\frac{n}{2})^{\\lg3} - \\frac{n}{3}) + n \\\\ &=& 3c(\\frac{n}{2})^{\\lg3} \\\\ &=& cn^{\\lg3} \\end{eqnarray} $$","title":"4.4-1"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-2","text":"First let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{2}) + n^2$ and assume that n is an exact power of 2. The number of nodes at depth i is 1. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n^2}{4^i}$. So the total cost over all nodes at depth i, is $\\frac{n^2}{4^i}$. The bottom level, at depth $\\lg{n}$, has 1 node, which contributing cost $T(1)$, for a total cost of $T(1)$, which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{n} - 1}\\frac{n^2}{4^i} + \\Theta(1) \\\\ &<& \\sum_{i = 0}^{\\infty}\\frac{n^2}{4^i} + \\Theta(1) \\\\ &=& \\frac{1}{1 - \\frac{1}{4}}n^2 + \\Theta(1) \\\\ &=& \\frac{4}{3}n^2 + \\Theta(1) \\\\ &=& O(n^2) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^2$ for some constant $c > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + n^2 \\\\ &\\leq& c(\\frac{n}{2})^2 + n^2 \\\\ &=& (\\frac{c}{4} + 1)n^2 \\\\ &\\leq& cn^2 \\end{eqnarray} $$ where the last step holds as long as $c \\geq \\frac{4}{3}$.","title":"4.4-2"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-3","text":"When n is large, the difference between $\\frac{n}{2} + 2$ and $\\frac{n}{2}$ not that large, so it's also a sloppiness that we can tolerate. Then let's create a recursion tree for the recurrence $T(n) = 4T(\\frac{n}{2}) + n$ and assume that n is an exact power of 2. Each level has four times more nodes than the level above, so the number of nodes at depth i is $4^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{n}{2^i}$. So the total cost over all nodes at depth i, is $4^i\\frac{n}{2^i} = 2^in$. The bottom level, at depth $\\lg{n}$, has $4^{\\lg{n}} = n^2$ nodes, each contributing cost $T(1)$, for a total cost of $n^2T(1)$, which is $\\Theta(n^2)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{n} - 1}2^in + \\Theta(n^2) \\\\ &=& n\\frac{1 - 2^{\\lg{n}}}{1 - 2} + \\Theta(n^2) \\\\ &=& n(n - 1) + \\Theta(n^2) \\\\ &=& n^2 - n + \\Theta(n^2) \\\\ &=& O(n^2) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn^2$ for some constant $c > 0$. So: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{2} + 2) + n \\\\ &\\leq& 4c(\\frac{n}{2} + 2)^2 + n \\\\ &=& 4c(\\frac{n^2}{4} + 2n + 4) + n \\\\ &=& cn^2 + 8cn + 16c + n \\\\ &=& cn^2 + (8c + 1)n + 16c \\end{eqnarray} $$ But $cn^2 + (8c + 1)n + 16c > cn^2$, so we need to try another guess, let's try $T(n) \\leq cn^2 - 5n$. So: $$ \\begin{eqnarray} T(n) &=& 4T(\\frac{n}{2} + 2) + n \\\\ &\\leq& 4c((\\frac{n}{2} + 2)^2 - 5(\\frac{n}{2} + 2)) + n \\\\ &=& 4c(\\frac{n^2}{4} + 2n + 4 - \\frac{5n}{2} - 10) + n \\\\ &=& 4c(\\frac{n^2}{4} - \\frac{n}{2} - 6) + n \\\\ &=& cn^2 + (1 - 2c)n - 24c \\\\ &<& cn^2 \\end{eqnarray} $$ where the last step holds as long as $c \\geq \\frac{1}{2}$.","title":"4.4-3"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-4","text":"First let's create a recursion tree for the recurrence $T(n) = 2T(n - 1) + 1$. Each level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, n - 1 - 1$, has a cost of 1. So the total cost over all nodes at depth i, is $2^i$. The bottom level, at depth n, has $2^{n - 1}$ nodes, each contributing cost $T(1)$, for a total cost of $2^{n - 1}T(1)$, which is $\\Theta(2^n)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{n - 1 - 1}2^i + \\Theta(2^n) \\\\ &=& \\frac{1 - 2^{n - 1}}{1 - 2} + \\Theta(2^n) \\\\ &=& 2^{n - 1} - 1 + \\Theta(2^n) \\\\ &=& O(2^n) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(2^n)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq c2^n$ for some constant $c > 0$. So: $$ \\begin{eqnarray} T(n) &=& 2T(n - 1) + 1 \\\\ &\\leq& 2c2^{n - 1} + 1 \\\\ &=& c2^n + 1 \\end{eqnarray} $$ But $c2^n + 1 > c2^n$, so we need to try another guess. Lets try $T(n) \\leq c2^n - 1$. So: $$ \\begin{eqnarray} T(n) &=& 2T(n - 1) + 1 \\\\ &\\leq& 2(c2^{n - 1} - 1) + 1 \\\\ &=& c2^n -1 \\\\ &<& c2^n \\end{eqnarray} $$","title":"4.4-4"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-5","text":"","title":"4.4-5"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-6","text":"First let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{3}) + T(\\frac{2n}{3})$ and assume that n is an exact power of 3. Each level has 2 times more nodes than the level above, so the number of nodes at depth i is $2^i$. But in this question, not all leaves reach at the bottom at the same time. The left most branch reaches the bottom first. So when the left most branch reaches the bottom, we assume other nodes also reach the bottom. Then the depth of recursion tree is $\\log_3{n}$. The total cost over all nodes at depth i, is $cn$ and the cost at bottom is also cn So: $$ \\begin{eqnarray} T(n) &\\geq& \\sum_{i = 0}^{\\log_3{n} - 1}cn + cn \\\\ &=& cn\\log_3{n} + cn \\\\ &=& \\frac{c}{\\lg3}n\\lg{n} + cn \\\\ &=& \\Theta(n\\lg{n}) \\end{eqnarray} $$ But remember we've cut some branches of the recursion tree, so $T(n) = \\Omega(n\\lg{n})$.","title":"4.4-6"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-7","text":"When n is large, the difference between $\\frac{n}{2}$ and $\\lfloor \\frac{n}{2} \\rfloor$ not that large, so it's also a sloppiness that we can tolerate. Then let's create a recursion tree for the recurrence $T(n) = 4T(\\frac{n}{2}) + cn$ and assume that n is an exact power of 2. Each level has 4 times more nodes than the level above, so the number of nodes at depth i is $4^i$. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\lg{n} - 1$, has a cost of $\\frac{cn}{2^i}$. So the total cost over all nodes at depth i, is $4^i\\frac{cn}{2^i} = c2^in$. The bottom level, at depth $\\lg{n}$, has $4^{\\lg{n}} = n^2$ nodes, each contributing cost $T(1)$, for a total cost of $n^2T(1)$, which is $\\Theta(n^2)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{n} - 1}c2^in + \\Theta(n^2) \\\\ &=& cn\\frac{1 - 2^{\\lg{n}}}{1 - 2} + \\Theta(n^2) \\\\ &=& \\Theta(n^2) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq dn^2$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& 4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\ &\\leq& 4T(\\frac{n}{2}) + cn \\\\ &\\leq& 4d(\\frac{n}{2})^2 + cn \\\\ &=& dn^2 + cn \\end{eqnarray} $$ But $dn^2 + cn > dn^2$, so let's try another guess, let's try $T(n) \\leq dn^2 - dn$. $$ \\begin{eqnarray} T(n) &=& 4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\ &\\leq& 4T(\\frac{n}{2}) + cn \\\\ &\\leq& 4d((\\frac{n}{2})^2 - \\frac{n}{2}) + cn \\\\ &=& dn^2 + (c - 2d)n \\\\ &\\leq& dn^2 \\end{eqnarray} $$ where the last step holds as long as $d \\geq \\frac{c}{2}$. So $T(n) = O(n^2)$. Then we want to show that $T(n) \\geq dn^2$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& 4T(\\lfloor \\frac{n}{2} \\rfloor) + cn \\\\ &\\geq& 4T(\\frac{n - (2 - 1)}{2}) + cn \\\\ &=& 4T(\\frac{n - 1}{2}) + cn \\\\ &\\geq& 4d(\\frac{n - 1}{2})^2 + cn \\\\ &=& dn^2 + (c - 2d)n + d \\\\ &>& dn^2 \\end{eqnarray} $$ where the last step holds as long as $d < \\frac{c}{2}$. So $T(n) = \\Omega(n^2)$, so $T(n) = \\Theta(n^2)$.","title":"4.4-7"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-8","text":"First let's create a recursion tree for the recurrence $T(n) = T(n - a) + T(a) + cn$ and assume that n - 1 is divisible by a. The total cost over all nodes at depth i, for $i = 0, 1, 2, \\ldots, \\frac{n - 1}{a} - 1$, is $c(n - (i - 1)a), \\text{ for } i \\geq 1$. The bottom level, at depth $\\frac{n - 1}{a}$, the total cost is $T(1) + ca$, which is $\\Theta(a)$. So: $$ \\begin{eqnarray} T(n) &=& cn + \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}c(n - (i - 1)a) + \\Theta(a) \\\\ &=& cn + \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}cn - \\sum_{i = 1}^{\\frac{n - 1}{a} - 1}(i - 1)ca + \\Theta(a) \\\\ &=& \\frac{n - 1}{a}cn - \\frac{n^2 - 2a -3na + 3a + 2a^2 + 1}{2a}c \\\\ &=& \\frac{n^2 +3na - 3a - 2a^2 - 1}{2a}c \\\\ &=& \\Theta(n^2) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Theta(n^2)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq dn^2$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(n - a) + T(a) + cn \\\\ &\\leq& d(n - a)^2 + da^2 + cn \\\\ &=& dn^2 + (c - ad)n + (2a^2 - an)d \\\\ &\\leq& dn^2 \\end{eqnarray} $$ where the last step holds as long as $d \\geq \\frac{c}{a}$ and $n \\geq 2a$. So $T(n) = O(n^2)$. Then we want to show that $T(n) \\geq dn^2$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(n - a) + T(a) + cn \\\\ &\\geq& d(n - a)^2 + da^2 + cn \\\\ &=& dn^2 + (c - 2ad)n + 2a^2d \\\\ &>& dn^2 \\end{eqnarray} $$ where the last step holds as long as $d < \\frac{c}{2a}$. So $T(n) = \\Omega(n^2)$. So $T(n) = \\Theta(n^2)$.","title":"4.4-8"},{"location":"04-Divide-and-Conquer/4.4-The-recursion-tree-method-for-solving-recurrences/#44-9","text":"First let's create a recursion tree for the recurrence $T(n) = T(\\alpha{n}) + T((1 - \\alpha)n) + cn$. So we can see not each branch reaches at the bottom at the same time, it might be the left most branch reaches at the bottom first, or the right most branch reaches at the bottom first. It depends on which is smaller, $\\alpha$ or $1 - \\alpha$. Let $\\alpha \\leq 1 - \\alpha$, so we get $\\alpha \\leq \\frac{1}{2}$. That is, when $0 < \\alpha \\leq \\frac{1}{2}$, the left most branch reaches at the bottom first, when $\\frac{1}{2} < \\alpha < 1$, the right most branch reaches at the bottom first. If $0 < \\alpha \\leq \\frac{1}{2}$, when the left most branch reaches at the bottom, let's assume other branches also reach at the bottom, so the total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{\\alpha}{\\frac{1}{n}} - 1$, is cn. The cost at the bottom is also cn. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\geq& \\sum_{i = 0}^{\\log_{\\alpha}\\frac{1}{n} - 1}cn + cn \\\\ &=& cn(\\log_{\\alpha}\\frac{1}{n}) + cn \\\\ &=& cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} + cn \\\\ &>& cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Omega(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq dn\\lg{n}$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\geq& d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ &=& d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ &=& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ &\\geq& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d\\alpha\\lg{\\alpha}n + cn \\, (0 < \\alpha \\leq \\frac{1}{2}) \\\\ &=& dn\\lg{n} + (c + 2d\\alpha\\lg{\\alpha})n \\\\ &\\geq& dn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $d \\leq \\frac{c}{2\\alpha\\lg{\\frac{1}{\\alpha}}}$. Now let's check the right most branch. When it reaches at the bottom, other branches have already reached at the bottom, if the tree depth is k, let's assume other branches become T(1) at depth k - 1. Each level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And total cost over all nodes at depth i, for $i = 0, 1, 2, ..., \\log_{1 - \\alpha}\\frac{1}{n}$, is cn. The cost at the bottom is $T(1)$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\leq& \\sum_{i = 0}^{\\log_{1 - \\alpha}\\frac{1}{n} - 1}cn + \\Theta(1) \\\\ &=& cn(\\log_{1 - \\alpha}\\frac{1}{n}) + \\Theta(1) \\\\ &=& cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} + \\Theta(1) \\\\ &<& cn\\frac{\\lg{n}}{\\lg{\\frac{1}{1 - \\alpha}}} + n\\lg{n}\\\\ &=& O(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to veify that our guess was correct. We want to show that $T(n) \\leq dn\\lg{n}$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\leq& d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ &=& d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ &=& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ &\\leq& dn\\lg{n} + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\, (0 < \\alpha \\leq \\frac{1}{2}) \\\\ &=& dn\\lg{n} + (c + 2d(1 - \\alpha)\\lg{(1 - \\alpha)})n \\\\ &\\leq& dn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $d \\geq \\frac{c}{2(1 - \\alpha)\\lg{\\frac{1}{1 - \\alpha}}}$. So we proved $T(n) = \\Omega(n\\lg{n})$ and $T(n) = O(n\\lg{n})$. So $T(n) = \\Theta(n\\lg{n})$ when $0 < \\alpha \\leq \\frac{1}{2}$. Then let's check another case, when $\\frac{1}{2} < \\alpha < 1$. In this case, the right most branch reaches at the bottom first. Let's assume other branches also reach at the bottom, so the total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{1 - \\alpha}\\frac{1}{n}$, is cn. The cost at the bottom is cn. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\geq& \\sum_{i = 0}^{\\log_{1 - \\alpha}\\frac{1}{n} - 1}cn + cn \\\\ &=& cn(\\log_{1 - \\alpha}\\frac{1}{n}) + cn \\\\ &=& cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} + cn \\\\ &>& cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{1 - \\alpha}}} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Omega(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq dn\\lg{n}$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\geq& d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ &=& d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ &=& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ &>& dn\\lg{n} + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\, (\\frac{1}{2} < \\alpha < 1) \\\\ &=& dn\\lg{n} + (c + 2d(1 - \\alpha)\\lg{(1 - \\alpha)})n \\\\ &\\geq& dn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $d \\leq \\frac{c}{2(1 - \\alpha)\\lg{\\frac{1}{1 - \\alpha}}}$. Now let's check the left most branch. When it reaches at the bottom, other branches have already reached at the bottom, if the tree depth is k, let's assume other branches become T(1) at depth k - 1. Each level has two times more nodes than the level above, so the number of nodes at depth i is $2^i$. And total cost over all nodes at depth i, for $i = 0, 1, 2, ... , \\log_{\\alpha}\\frac{1}{n} - 1$, is cn. The cost at the bottom is T(1). So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &<& \\sum_{i = 0}^{\\log_{\\alpha}\\frac{1}{n} - 1}cn + \\Theta(1) \\\\ &=& cn(\\log_{\\alpha}\\frac{1}{n}) + \\Theta(1) \\\\ &=& cn\\frac{\\lg{n}}{\\lg_{\\frac{1}{\\alpha}}} + \\Theta(1) \\\\ &<& cn\\frac{\\lg{n}}{\\lg{\\frac{1}{\\alpha}}} + n\\lg{n}\\\\ &=& O(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n\\lg{n})$ for our original recurrence. Now let's use the substitution method to veify that our guess was correct. We want to show that $T(n) \\leq dn\\lg{n}$ for some constant $d > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\alpha{n}) + T((1 - \\alpha)n) + cn \\\\ &\\leq& d\\alpha{n}\\lg{(\\alpha{n})} + d(1 - \\alpha)n\\lg{((1 - \\alpha)n)} + cn \\\\ &=& d\\alpha{n}(\\lg{\\alpha} + n) + d(1 - \\alpha)n(\\lg{(1 - \\alpha)} + \\lg{n}) + cn \\\\ &=& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d(1 - \\alpha)\\lg{(1 - \\alpha)}n + cn \\\\ &<& dn\\lg{n} + d\\alpha\\lg{\\alpha}n + d\\alpha\\lg{\\alpha}n + cn \\, (\\frac{1}{2} < \\alpha < 1) \\\\ &=& dn\\lg{n} + (c + 2d\\alpha\\lg{\\alpha})n \\\\ &\\leq& dn\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $d \\geq \\frac{c}{2\\alpha\\lg{\\frac{1}{\\alpha}}}$. So we proved $T(n) = \\Omega(n\\lg{n})$ and $T(n) = O(n\\lg{n})$. So $T(n) = \\Theta(n\\lg{n})$ when $\\frac{1}{2} < \\alpha < 1$. Thus $T(n) = \\Theta(n\\lg{n})$ for all $0 < \\alpha < 1$.","title":"4.4-9"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/","text":"4.5 The master method for solving recurrences 4.5-1 a Here, we have a = 2, b = 4, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $n^{\\frac{1}{2}}$ is polynomially larger than f(n) (that is, $f(n) = O(n^{\\frac{1}{2} - \\epsilon})$ for $\\epsilon \\leq \\frac{1}{2}$), case 1 applies, and $T(n) = \\Theta(n^{\\frac{1}{2}})$. b Here, we have a = 2, b = 4, and $f(n) = \\Theta(\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Case 2 applies, so $T(n) = \\Theta(\\sqrt{n}\\lg{n})$. c Here, we have a = 2, b = 4, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $f(n) = \\Omega(n^{\\frac{1}{2} + \\epsilon})$, for $\\epsilon \\leq \\frac{1}{2}$. Case 3 applies if we can show that the regularity condition holds for f(n). For sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{4}) = 2\\frac{n}{4} = \\frac{n}{2} \\leq cf(n)$ for $c = \\frac{2}{3}$. So $T(n) = \\Theta(n)$. d Here, we have a = 2, b = 4, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $f(n) = \\Omega(n^{\\frac{1}{2} + \\epsilon})$, for $\\epsilon \\leq \\frac{3}{2}$. Case 3 applies if we can show that the regularity condition holds for f(n). For sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{4}) = 2\\frac{n}{4} = \\frac{n}{2} \\leq cf(n)$ for $c = \\frac{1}{2}$. So $T(n) = \\Theta(n^2)$. 4.5-2 In Strassen's algorithm, $T(n) = \\Theta(n^{\\lg7})$. In Professor Caesar's algorithm, $T(n) = aT(\\frac{n}{4}) + \\Theta(n^2)$. In order to beat Strassen's algorithm, then Professor Caesar's algorithm cannot apply case 3, since $f(n) = \\Theta(n^2)$ which is polynomially larger than $\\Theta(n^{\\lg7})$. We have that $n^{\\log_ba} = n^{\\log_4a} = n^{\\frac{\\lg{a}}{2}}$. If case 2 applies, then $f(n) = \\Theta(n^{\\frac{\\lg{a}}{2}})$, so $n^{\\frac{\\lg{a}}{2}} = n^2$, a = 16. And $T(n) = \\Theta(n^{\\frac{\\lg{a}}{2}}\\lg{n}) = \\Theta(n^2\\lg{n})$ which cannot beat Strassen's algorithm. So case 1 applies. And $T(n) = \\Theta(n^{\\frac{\\lg{a}}{2}})$, and we have $\\frac{\\lg{a}}{2} < \\lg7$, thus $a < 49$, so the largest integer value of a is 48. 4.5-3 Here, we have a = 1, b = 2, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. Case 2 applies, so $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(\\lg{n})$. 4.5-4 Here, we have a = 4, b = 2, and $f(n) = \\Theta(n^2\\lg{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_2{4}} = n^2$. Since f(n) is not polymonially larger than $n^2$, so we cannot use master method to solve the recurrence. According to exercise 4.6-2, $f(n) = \\Theta(n^2\\lg{n}) = \\Theta(n^{\\log_b{a}}\\lg^k{n})$, where k = 1. So $T(n) = \\Theta(n^{\\log_b{a}}\\lg^{k + 1}{n}) = \\Theta(n^2\\lg^2{n}) = \\Theta((n\\lg{n})^2)$. 4.5-5","title":"4.5 The master method for solving recurrences"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-the-master-method-for-solving-recurrences","text":"","title":"4.5 The master method for solving recurrences"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-1","text":"","title":"4.5-1"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#a","text":"Here, we have a = 2, b = 4, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $n^{\\frac{1}{2}}$ is polynomially larger than f(n) (that is, $f(n) = O(n^{\\frac{1}{2} - \\epsilon})$ for $\\epsilon \\leq \\frac{1}{2}$), case 1 applies, and $T(n) = \\Theta(n^{\\frac{1}{2}})$.","title":"a"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#b","text":"Here, we have a = 2, b = 4, and $f(n) = \\Theta(\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Case 2 applies, so $T(n) = \\Theta(\\sqrt{n}\\lg{n})$.","title":"b"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#c","text":"Here, we have a = 2, b = 4, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $f(n) = \\Omega(n^{\\frac{1}{2} + \\epsilon})$, for $\\epsilon \\leq \\frac{1}{2}$. Case 3 applies if we can show that the regularity condition holds for f(n). For sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{4}) = 2\\frac{n}{4} = \\frac{n}{2} \\leq cf(n)$ for $c = \\frac{2}{3}$. So $T(n) = \\Theta(n)$.","title":"c"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#d","text":"Here, we have a = 2, b = 4, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = n^{\\frac{1}{2}}$. Since $f(n) = \\Omega(n^{\\frac{1}{2} + \\epsilon})$, for $\\epsilon \\leq \\frac{3}{2}$. Case 3 applies if we can show that the regularity condition holds for f(n). For sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{4}) = 2\\frac{n}{4} = \\frac{n}{2} \\leq cf(n)$ for $c = \\frac{1}{2}$. So $T(n) = \\Theta(n^2)$.","title":"d"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-2","text":"In Strassen's algorithm, $T(n) = \\Theta(n^{\\lg7})$. In Professor Caesar's algorithm, $T(n) = aT(\\frac{n}{4}) + \\Theta(n^2)$. In order to beat Strassen's algorithm, then Professor Caesar's algorithm cannot apply case 3, since $f(n) = \\Theta(n^2)$ which is polynomially larger than $\\Theta(n^{\\lg7})$. We have that $n^{\\log_ba} = n^{\\log_4a} = n^{\\frac{\\lg{a}}{2}}$. If case 2 applies, then $f(n) = \\Theta(n^{\\frac{\\lg{a}}{2}})$, so $n^{\\frac{\\lg{a}}{2}} = n^2$, a = 16. And $T(n) = \\Theta(n^{\\frac{\\lg{a}}{2}}\\lg{n}) = \\Theta(n^2\\lg{n})$ which cannot beat Strassen's algorithm. So case 1 applies. And $T(n) = \\Theta(n^{\\frac{\\lg{a}}{2}})$, and we have $\\frac{\\lg{a}}{2} < \\lg7$, thus $a < 49$, so the largest integer value of a is 48.","title":"4.5-2"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-3","text":"Here, we have a = 1, b = 2, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. Case 2 applies, so $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(\\lg{n})$.","title":"4.5-3"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-4","text":"Here, we have a = 4, b = 2, and $f(n) = \\Theta(n^2\\lg{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_2{4}} = n^2$. Since f(n) is not polymonially larger than $n^2$, so we cannot use master method to solve the recurrence. According to exercise 4.6-2, $f(n) = \\Theta(n^2\\lg{n}) = \\Theta(n^{\\log_b{a}}\\lg^k{n})$, where k = 1. So $T(n) = \\Theta(n^{\\log_b{a}}\\lg^{k + 1}{n}) = \\Theta(n^2\\lg^2{n}) = \\Theta((n\\lg{n})^2)$.","title":"4.5-4"},{"location":"04-Divide-and-Conquer/4.5-The-master-method-for-solving-recurrences/#45-5","text":"","title":"4.5-5"},{"location":"04-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/","text":"4.6 Proof of the master theorem 4.6-1 Let's prove that if b is a positive integer, then $\\lceil \\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\rceil = \\lceil \\frac{n}{b^2} \\rceil$. Since $\\lceil \\frac{n}{b} \\rceil \\geq \\frac{n}{b}$, so $\\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\geq \\frac{n}{b}\\frac{1}{b} = \\frac{n}{b^2}$, thus $\\lceil \\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\rceil \\geq \\lceil \\frac{n}{b^2} \\rceil$. And we have $\\frac{n}{b}\\frac{1}{b} \\leq \\lceil\\frac{n}{b}\\frac{1}{b} \\rceil = \\lceil\\frac{n}{b^2} \\rceil$, hence $\\frac{n}{b} \\leq b\\lceil \\frac{n}{b^2} \\rceil$. Because both b and $\\lceil \\frac{n}{b^2} \\rceil$ are integers, so $b\\lceil \\frac{n}{b^2} \\rceil$ is also a integer. So $\\lceil \\frac{n}{b} \\rceil \\leq b\\lceil \\frac{n}{b^2} \\rceil$. So $\\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\leq \\lceil \\frac{n}{b^2} \\rceil$, so $\\lceil \\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\rceil \\leq \\lceil \\frac{n}{b^2} \\rceil$. Thus $\\lceil \\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\rceil = \\lceil \\frac{n}{b^2} \\rceil$, so $n_j = \\lceil \\frac{n}{b^{j - 1}} \\rceil$ if j > 0. If j = 0, $n_j = n$, so we can get a more simpler expression, $n_j = \\lceil \\frac{n}{b^j} \\rceil$. You can get some useful lemmas here . 4.6-2 From lemma 4.2 we know $T(n) = \\Theta(n^{\\log_b{a}}) + \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j})$. $$ \\begin{eqnarray} \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j}) &=& \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j}) \\\\ &=& \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta((\\frac{n}{b^j})^{\\log_b{a}}\\lg^k{\\frac{n}{b^j}}) \\\\ &=& \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta(\\frac{n^{\\log_b{a}}}{(b^{\\log_b{a}})^j}\\lg^k{\\frac{n}{b^j}}) \\\\ &=& \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta(\\frac{n^{\\log_b{a}}}{a^j}\\lg^k{\\frac{n}{b^j}}) \\\\ &=& \\sum_{j = 0}^{\\log_b{n - 1}}\\Theta(n^{\\log_b{a}}\\lg^k{\\frac{n}{b^j}}) \\\\ &=& n^{\\log_b{a}}\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta(\\lg^k{\\frac{n}{b^j}}) \\\\ &=& n^{\\log_b{a}}\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k) \\end{eqnarray} $$ We have: $$ \\begin{eqnarray} \\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k) &\\leq& \\sum_{j = 0}^{\\log_b{n - 1}}\\lg^k{n} \\\\ &=& \\log_b{n}\\lg^k{n} \\\\ &=& \\frac{\\lg^{k + 1}{n}}{\\lg{b}} \\\\ &=& O(\\lg^{k + 1}{n}) \\end{eqnarray} $$ But I don't know how to prove $\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k) = \\Omega(\\lg^{k + 1}{n})$. 4.6-3 We have $af(\\frac{n}{b}) \\leq cf(n)$, so $f(n) \\geq \\frac{a}{c}f(\\frac{n}{b})$, and: $$ \\begin{eqnarray} f(n) &\\geq& \\frac{a}{c}f(\\frac{n}{b}) \\\\ &\\geq& (\\frac{a}{c})^2f(\\frac{n}{b^2}) \\\\ &\\geq& \\ldots \\\\ &\\geq& (\\frac{a}{c})^{\\log_b{n}}f(1) \\\\ &=& n^{\\log_b{\\frac{a}{c}}}f(1) \\\\ &=& n^{\\log_b{a + \\epsilon}}f(1) \\text{ (c < 1)} \\\\ &=& \\Omega(n^{\\log_b{a + \\epsilon}}) \\end{eqnarray} $$ So $f(n) = \\Omega(n^{\\log_b{a + \\epsilon}})$.","title":"4.6 Proof of the master theorem"},{"location":"04-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/#46-proof-of-the-master-theorem","text":"","title":"4.6 Proof of the master theorem"},{"location":"04-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/#46-1","text":"Let's prove that if b is a positive integer, then $\\lceil \\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\rceil = \\lceil \\frac{n}{b^2} \\rceil$. Since $\\lceil \\frac{n}{b} \\rceil \\geq \\frac{n}{b}$, so $\\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\geq \\frac{n}{b}\\frac{1}{b} = \\frac{n}{b^2}$, thus $\\lceil \\lceil \\frac{n}{b} \\rceil \\frac{1}{b} \\rceil \\geq \\lceil \\frac{n}{b^2} \\rceil$. And we have $\\frac{n}{b}\\frac{1}{b} \\leq \\lceil\\frac{n}{b}\\frac{1}{b} \\rceil = \\lceil\\frac{n}{b^2} \\rceil$, hence $\\frac{n}{b} \\leq b\\lceil \\frac{n}{b^2} \\rceil$. Because both b and $\\lceil \\frac{n}{b^2} \\rceil$ are integers, so $b\\lceil \\frac{n}{b^2} \\rceil$ is also a integer. So $\\lceil \\frac{n}{b} \\rceil \\leq b\\lceil \\frac{n}{b^2} \\rceil$. So $\\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\leq \\lceil \\frac{n}{b^2} \\rceil$, so $\\lceil \\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\rceil \\leq \\lceil \\frac{n}{b^2} \\rceil$. Thus $\\lceil \\lceil \\frac{n}{b} \\rceil\\frac{1}{b} \\rceil = \\lceil \\frac{n}{b^2} \\rceil$, so $n_j = \\lceil \\frac{n}{b^{j - 1}} \\rceil$ if j > 0. If j = 0, $n_j = n$, so we can get a more simpler expression, $n_j = \\lceil \\frac{n}{b^j} \\rceil$. You can get some useful lemmas here .","title":"4.6-1"},{"location":"04-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/#46-2","text":"From lemma 4.2 we know $T(n) = \\Theta(n^{\\log_b{a}}) + \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j})$. $$ \\begin{eqnarray} \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j}) &=& \\sum_{j = 0}^{\\log_b{n - 1}} a^jf(\\frac{n}{b^j}) \\\\ &=& \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta((\\frac{n}{b^j})^{\\log_b{a}}\\lg^k{\\frac{n}{b^j}}) \\\\ &=& \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta(\\frac{n^{\\log_b{a}}}{(b^{\\log_b{a}})^j}\\lg^k{\\frac{n}{b^j}}) \\\\ &=& \\sum_{j = 0}^{\\log_b{n - 1}} a^j\\Theta(\\frac{n^{\\log_b{a}}}{a^j}\\lg^k{\\frac{n}{b^j}}) \\\\ &=& \\sum_{j = 0}^{\\log_b{n - 1}}\\Theta(n^{\\log_b{a}}\\lg^k{\\frac{n}{b^j}}) \\\\ &=& n^{\\log_b{a}}\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta(\\lg^k{\\frac{n}{b^j}}) \\\\ &=& n^{\\log_b{a}}\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k) \\end{eqnarray} $$ We have: $$ \\begin{eqnarray} \\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k) &\\leq& \\sum_{j = 0}^{\\log_b{n - 1}}\\lg^k{n} \\\\ &=& \\log_b{n}\\lg^k{n} \\\\ &=& \\frac{\\lg^{k + 1}{n}}{\\lg{b}} \\\\ &=& O(\\lg^{k + 1}{n}) \\end{eqnarray} $$ But I don't know how to prove $\\sum_{j = 0}^{\\log_b{n - 1}}\\Theta((\\lg{n} - j\\lg{b})^k) = \\Omega(\\lg^{k + 1}{n})$.","title":"4.6-2"},{"location":"04-Divide-and-Conquer/4.6-Proof-of-the-master-theorem/#46-3","text":"We have $af(\\frac{n}{b}) \\leq cf(n)$, so $f(n) \\geq \\frac{a}{c}f(\\frac{n}{b})$, and: $$ \\begin{eqnarray} f(n) &\\geq& \\frac{a}{c}f(\\frac{n}{b}) \\\\ &\\geq& (\\frac{a}{c})^2f(\\frac{n}{b^2}) \\\\ &\\geq& \\ldots \\\\ &\\geq& (\\frac{a}{c})^{\\log_b{n}}f(1) \\\\ &=& n^{\\log_b{\\frac{a}{c}}}f(1) \\\\ &=& n^{\\log_b{a + \\epsilon}}f(1) \\text{ (c < 1)} \\\\ &=& \\Omega(n^{\\log_b{a + \\epsilon}}) \\end{eqnarray} $$ So $f(n) = \\Omega(n^{\\log_b{a + \\epsilon}})$.","title":"4.6-3"},{"location":"04-Divide-and-Conquer/Problems/","text":"Problems 4-1 a Here, we have a = 2, b = 2, and $f(n) = \\Theta(n^4)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{2}} = n$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 3$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{2}) = \\frac{n^4}{8} = cf(n)$, for $c = \\frac{1}{8}$. So, the solution to the recurrence is $T(n) = \\Theta(n^4)$. b Here, we have a = 1, $b = \\frac{10}{7}$, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_{\\frac{10}{7}}{1}} = 1$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 1$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = f(\\frac{7n}{10}) = \\frac{7n}{10} = cf(n)$, for $c = \\frac{7}{10}$. So, the solution to the recurrence is $T(n) = \\Theta(n)$. c Here, we have a = 16, b = 4, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{16}} = n^2$. So case 2 applies, so the solution to the recurrence is $T(n) = \\Theta(n^2\\lg{n})$. d Here, we have a = 7, b = 3, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{7}}$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 2 - \\log_3{7}$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 7f(\\frac{n}{3}) = \\frac{7n^2}{9} = cf(n)$, for $c = \\frac{7}{9}$. So, the solution to the recurrence is $T(n) = \\Theta(n^2)$. e Here, we have a = 7, b = 2, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{7}} = n^{\\lg{7}}$. Since $f(n) = O(n^{\\log_b{a} - \\epsilon})$, where $\\epsilon \\leq \\lg{\\frac{7}{4}}$, case 1 applies. So, the solution to the recurrence is $T(n) = \\Theta(n^{\\lg7})$. f Here, we have a = 2, b = 4, and $f(n) = \\Theta(\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = \\sqrt{n}$, case 2 applies. So, the solution to the recurrence is $T(n) = \\Theta(\\sqrt{n}\\lg{n})$. g First let's create a recursion tree for the recurrence $T(n) = T(n - 2) + n^2$. The number of nodes at depth i is 1. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\frac{n - 1}{2} - 1$, has a cost of $(n - 2i)^2$. So the total cost over all nodes at depth i, is $(n - 2i)^2$. The bottom level, at depth $\\frac{n - 1}{2}$, has 1 node, which contributing cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}(n - 2i)^2 + \\Theta(1) \\\\ &=& \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}(n^2 - 2ni + 4i^2) + \\Theta(1) \\\\ &=& \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}n^2 - \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}2ni + \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}4i^2 + \\Theta(1) \\\\ &=& \\frac{n - 1}{2}n^2 - 2n\\frac{(\\frac{n - 1}{2} - 1)(1 + \\frac{n - 1}{2} - 1)}{2} + 4\\frac{(\\frac{n - 1}{2} - 1)(\\frac{n - 1}{2} - 1 + 1)(2(\\frac{n - 1}{2} - 1) + 1)}{6} + \\Theta(1) \\\\ &=& \\frac{n^2(n - 1)}{2} - \\frac{n(n - 1)(n - 3)}{4} + \\frac{(n - 1)(n - 2)(n - 3)}{6} + \\Theta(1) \\\\ &=& \\frac{5n^3 - 6n^2 + 13n -12}{6} + \\Theta(1) \\\\ &=& \\Theta(n^3) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Theta(n^3)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n^3$ and $T(n) \\leq c_2n^3$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(n - 2) + n^2 \\\\ &\\geq& c_1(n - 2)^3 + n^2 \\\\ &=& c_1n^3 + (1 - 6c_1)n^2 + 4c_1(3n - 2) \\\\ &>& c_1n^3 \\end{eqnarray} $$ where the last step holds as long as $c_1 \\leq \\frac{1}{6}$. So $T(n) = \\Omega(n^3)$. And: $$ \\begin{eqnarray} T(n) &=& T(n - 2) + n^2 \\\\ &\\leq& c_2(n - 2)^3 + n^2 \\\\ &=& c_2n^3 + n((1 - 6c_2)n + 12c_2) - 8c_2 \\\\ &\\leq& c_2n^3 \\end{eqnarray} $$ where the last step holds as long as $c_2 > \\frac{1}{6}$ and $n \\geq \\frac{12c_2}{6c_2 - 1}$. So $T(n) = O(n^3)$, thus $T(n) = \\Theta(n^3)$. 4-2 a Here is the pseudocode of recursive binary search algorithm: RECURSIVE-BINARY-SEARCH(A, v, low, high) if low <= high middle = (low + high) / 2 if A[middle] < v return RECURSIVE-BINARY-SEARCH(A, v, middle + 1, high) else if A[middle] > v return RECURSIVE-BINARY-SEARCH(A, v, low, middle - 1) else return middle return NIL An array is passed by pointer Before it halves the problem size, it needs to do some operations like comparing low and high , calculating middle . But they are constant operations, we can let it be $\\Theta(1)$. So $T(n) = T(\\frac{n}{2}) + \\Theta(1)$. Here, we have a = 1, b = 2, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. So case 2 applies, thus $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(\\lg{n}) = \\Theta(\\lg{N})$. An array is passed by copying Each time it halves the problem size, it needs additional $\\Theta(N)$ operation to copy the array. So: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + \\Theta(N) + \\Theta(1) \\\\ &=& T(\\frac{n}{2}) + \\Theta(N) \\\\ &=& T(\\frac{n}{4}) + \\Theta(N) + \\Theta(N) \\\\ &=& \\ldots = T(1) + \\lg{n}\\Theta(N) \\\\ &=& \\Theta(N\\lg{N}) \\end{eqnarray} $$. An array is passed by range Each time it halves the problem size, it needs additional $\\Theta(n)$ operation to copy the array. So $T(n) = T(\\frac{n}{2}) + \\Theta(n) + \\Theta(1) = T(\\frac{n}{2}) + \\Theta(n)$. Here, we have a = 1, b = 2, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 1$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = f(\\frac{n}{2}) = \\Theta(\\frac{n}{2}) = c\\Theta(n)$, for $c = \\frac{1}{2}$. So, the solution to the recurrence is $T(n) = \\Theta(n) = \\Theta(N)$. b Here is the pseudocode of merge sor algorithm: MERGE-SORT(A, p, r) if p < r q = (p + r) / 2 MERGE-SORT(A, p ,q) MERGE-SORT(A, q + 1, r) MERGE(A, p, q, r) An array is passed by pointer We already know the solution that $T(n) = \\Theta(n\\lg{n}) = \\Theta(N\\lg{N})$. An array is passed by copying From the pseudocode we know it needs to pass the array 3 times to subroutine. So we have $T(n) = 2T(\\frac{n}{2}) + \\Theta(n) + 3\\Theta(N) = 2T(\\frac{n}{2}) + \\Theta(n) + \\Theta(N)$. Let's use the iterative method to solve it: $$ \\begin{eqnarray} T(n) &=& 2T(\\frac{n}{2}) + \\Theta(n) + \\Theta(N) \\\\ &=& 2(2T(\\frac{n}{4}) + \\Theta(\\frac{n}{2}) + \\Theta(N)) + \\Theta(n) + \\Theta(N) \\\\ &=& 4T(\\frac{n}{4}) + 2\\Theta(\\frac{n}{2}) + 2\\Theta(N) + \\Theta(n) + \\Theta(N) \\\\ &=& 4T(\\frac{n}{4}) + 2\\Theta(n) + 3\\Theta(N) \\\\ &=& \\ldots \\\\ &=& 2^iT(\\frac{n}{2^i}) + i\\Theta(n) + (1 + 2 + \\ldots + 2^{i - 1})\\Theta(N) \\text{ for } i = 1, 2, \\ldots, \\lg{n} \\\\ &=& 2^{\\lg{n}}T(1) + \\lg{n}\\Theta(n) + \\Theta(N)\\frac{1 - 2^{\\lg{n}}}{1 - 2} \\\\ &=& nT(1) + \\Theta(n\\lg{n}) + \\Theta(Nn) \\\\ &=& \\Theta(N^2) \\end{eqnarray} $$ An array is passed by range It needs additional $3\\Theta(n)$ to copy the array. So $T(n) = 2T(\\frac{n}{2}) + \\Theta(n) + 3\\Theta(n) = 2T(\\frac{n}{2}) + \\Theta(n) = \\Theta(n\\lg{n}) = \\Theta(N\\lg{N})$. 4-3 a Here, we have a = 4, b = 3, and $f(n) = \\Theta(n\\lg{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_3{4}}$. In problems 3-2-a, we proved that $\\lg^k{n} = O(n^\\epsilon)$ for $k \\geq 1$ and $\\epsilon > 0$, so $\\lg{n} = O(n^{\\epsilon})$. Since $\\log_3{4} \\approx 1.2618595071429148$, so $f(n) = O(n^{\\log_ba - \\epsilon})$ for $\\epsilon \\leq 0.26$. So case 1 applies, the solution to the recurrence is $T(n) = \\Theta(n^{\\log_34})$. b Here, we have a = 3, b = 3, and $f(n) = \\Theta(\\frac{n}{\\lg{n}})$, and thus we have that $n^{\\log_ba} = n^{\\log_3{3}} = n$. So $f(n) = O(n^{\\log_ba})$. But it's not that easy to prove that $f(n) = O(n^{\\log_ba - \\epsilon})$. Let's solve it by iterative method: $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &=& 3(3T(\\frac{n}{9}) + \\frac{\\frac{n}{3}}{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& 9T(\\frac{n}{9}) + \\frac{n}{\\lg{n} - \\lg3} + \\frac{n}{\\lg{n}} \\\\ &=& \\ldots \\\\ &=& 3^iT(\\frac{n}{3^i}) + \\sum_{i = 1}^{\\log_3{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg3} \\\\ &=& nT(1) + \\sum_{i = 1}^{\\log_3{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg3} \\\\ &=& nT(1) + \\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\lg{n} - i\\lg3} \\\\ &=& nT(1) + \\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\frac{\\log_3{n}}{\\log_3{2}} - i\\frac{\\log_3{3}}{\\log_3{2}}} \\\\ &=& nT(1) + \\log_3{2}\\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\log_3{n} - i} \\\\ &=& nT(1) + \\log_3{2}\\sum_{i = 1}^{\\log_3{n}}\\frac{n}{i} \\\\ &=& nT(1) + \\log_3{2}(n(\\ln{\\log_3{n}} + O(1))) \\text{ by equation (A.7)} \\\\ &=& \\Theta(n\\lg{\\lg{n}}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Theta(n\\lg{\\lg{n}})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{\\lg{n}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}}$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &\\leq& 3(c_2\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_2n\\lg{(\\lg{n} - \\lg3)} + \\frac{n}{\\lg{n}} \\end{eqnarray} $$ It's not that easy to prove that $T(n) \\leq c_2n\\lg{\\lg{n}}$, let's guess $T(n) \\leq c_2n\\lg{\\lg{n}} - \\frac{n}{\\lg{3n}}$. $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &\\leq& 3(c_2\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}} - \\frac{\\frac{n}{3}}{\\lg{3\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_2n\\lg{(\\lg{n} - \\lg3)} - \\frac{n}{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\ &<& c_2n\\lg{\\lg{n}} \\end{eqnarray} $$ So $T(n) = O(n\\lg{\\lg{n}})$. And: $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &\\geq& 3(c_1\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_1n\\lg{(\\lg{n} - \\lg3)} + \\frac{n}{\\lg{n}} \\end{eqnarray} $$ Let's reguess $T(n) \\geq c_1n\\lg{\\lg{3n}}$. So: $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &\\geq& 3(c_1\\frac{n}{3}\\lg{\\lg{3\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_1n\\lg{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\ &>& c_1n\\lg{\\lg{n}} \\end{eqnarray} $$ So $T(n) = \\Omega(n\\lg{\\lg{n}})$. Thus = $T(n) \\ Theta(n\\lg{\\lg{n}})$. c Here, we have a = 4, b = 2, and $f(n) = \\Theta(n^2\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = \\sqrt{n}$. So $f(n) = \\Omega(n^{\\log_ba + \\epsilon})$ for $\\epsilon \\leq 2$. case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 4f(\\frac{n}{2}) = \\frac{\\sqrt{2}}{2}n^2\\sqrt{n} = cf(n)$, for $c = \\frac{\\sqrt{2}}{2}$. So, the solution to the recurrence is $T(n) = \\Theta(n^2\\sqrt{n})$. d If n is large enough, then we can ignore the -2 . Then we can solve it by the master method. So we have a = 3, b = 3, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{3}} = n$. Case 2 applies, thus the solution to the recurrence is $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(n\\lg{n})$. e Let's try to solve a general form of the recurrence $T(n) = aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}}, a > 1$. $$ \\begin{eqnarray} T(n) &=& aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\ &=& a(aT(\\frac{n}{a^2}) + \\frac{\\frac{n}{a}}{\\lg{\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\ &=& a^2T(\\frac{n}{a^2}) + \\frac{n}{\\lg{n} - \\lg{a}} + \\frac{n}{\\lg{n}} \\\\ &=& \\ldots \\\\ &=& a^iT(\\frac{n}{a^i}) + \\sum_{i = 1}^{\\log_a{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg{a}} \\\\ &=& nT(1) + \\sum_{i = 1}^{\\log_a{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg{a}} \\\\ &=& nT(1) + \\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\lg{n} - i\\lg{a}} \\\\ &=& nT(1) + \\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\frac{\\log_a{n}}{\\log_a{2}} - i\\frac{\\log_a{a}}{\\log_a{2}}} \\\\ &=& nT(1) + \\log_a{2}\\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\log_a{n} - i} \\\\ &=& nT(1) + \\log_a{2}\\sum_{i = 1}^{\\log_a{n}}\\frac{n}{i} \\\\ &=& nT(1) + \\log_a{2}(n(\\ln{\\log_a{n}} + O(1))) \\text{ by equation (A.7)} \\\\ &=& \\Theta(n\\lg{\\lg{n}}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Theta(n\\lg{\\lg{n}})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{\\lg{n}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}}$ for some constants $c_1 > 0$ and $c_2 > 0$. Like the problem b, let's guess $T(n) \\geq c_1n\\lg{\\lg{an}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}} - \\frac{n}{\\lg{an}}$. $$ \\begin{eqnarray} T(n) &=& aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\ &\\leq& a(c_2\\frac{n}{a}\\lg{\\lg{\\frac{n}{a}}} - \\frac{\\frac{n}{a}}{\\lg{a\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_2n\\lg{(\\lg{n} - \\lg{a})} - \\frac{n}{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\ &<& c_2n\\lg{\\lg{n}} \\end{eqnarray} $$ So $T(n) = O(n\\lg{\\lg{n}})$. And: $$ \\begin{eqnarray} T(n) &=& aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\ &\\geq& a(c_1\\frac{n}{a}\\lg{\\lg{a\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_1n\\lg{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\ &>& c_1n\\lg{\\lg{n}} \\end{eqnarray} $$ So $T(n) = \\Omega(n\\lg{\\lg{n}})$. Thus = $T(n) = \\Theta(n\\lg{\\lg{n}})$. The solution to recurrence $T(n) = 2T(\\frac{n}{2}) + \\frac{n}{\\lg{n}}$ is $\\Theta(n\\lg{\\lg{n}})$. f First let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n$ and assume that n is an exact power of 8. We've solved similar problems before. Not all branch reaches at the bottom at the same time. The right most branch reaches at the bottom first. The left most branch is the last one that reaches at the bottom. And we can see the total cost over all nodes at depth i is $(\\frac{7}{8})^in$. If the left most branch reaches at the bottom, and assume the current depth is k, then we assume other branches reach at depth k - 1 at the same time. So we have: $$ \\begin{eqnarray} T(n) &\\leq& \\sum_{i = 0}^{\\lg{n} - 1}(\\frac{7}{8})^in + T(1) \\\\ &=& n\\frac{1 - (\\frac{7}{8})^{\\lg{n}}}{1 - \\frac{7}{8}} + T(1) \\\\ &=& 8n(1 - (\\frac{7}{8})^{\\lg{n}}) + T(1) \\\\ &<& 8n + T(1) \\\\ &\\leq& 9n \\\\ &=& O(n) \\end{eqnarray} $$ where the last step holds as long as $n \\geq T(1)$. Then if the right most branch reaches at the bottom, other branches have not reach at the bottom. But we assume they also reach at the bottom. So we have: $$ \\begin{eqnarray} T(n) &\\geq& \\sum_{i = 0}^{\\log_8{n} - 1}(\\frac{7}{8})^in + (\\frac{7}{8})^{\\log_8{n}}n \\\\ &=& n\\frac{1 - (\\frac{7}{8})^{\\log_8{n} + 1}}{1 - \\frac{7}{8}} \\\\ &=& 8n(1 - (\\frac{7}{8})^{\\log_8{n} + 1}) \\\\ &\\geq& 8n(1 - (\\frac{7}{8})^{\\log_8{1} + 1}) \\\\ &=& n \\\\ &=& \\Omega(n) \\end{eqnarray} $$ Thus we devird a guess of $T(n) = \\Theta(n)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n$ and $T(n) \\leq c_2n$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n \\\\ &\\geq& c_1\\frac{n}{2} + c_1\\frac{n}{4} + c_1\\frac{n}{8} + n \\\\ &=& (1 + \\frac{7}{8}c_1)n \\\\ &\\geq& c_1n \\\\ &=& \\Omega(n) \\end{eqnarray} $$ where the last step holds as long as $c_1 \\leq 8$. And: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n \\\\ &\\leq& c_2\\frac{n}{2} + c_2\\frac{n}{4} + c_2\\frac{n}{8} + n \\\\ &=& (1 + \\frac{7}{8}c_2)n \\\\ &\\leq& c_2n \\\\ &=& O(n) \\end{eqnarray} $$ where the last step holds as long as $c_2 \\geq 8$. Thus $T(n) = \\Theta(n)$. g First let's create a recursion tree for the recurrence $T(n) = T(n - 1) + \\frac{1}{n}$. The number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, n - 2$, has a cost of $\\frac{1}{n - i}$. The bottom level, at depth n - 1, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{n - 2}\\frac{1}{n - i} + \\Theta(1) \\\\ &=& \\sum_{i = 2}^{n}\\frac{1}{i} + \\Theta(1) \\\\ &=& \\ln{n} - 1 + \\Theta(1) \\\\ &=& \\Theta(n) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = T(n - 1) + \\frac{1}{n}$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1\\lg{n}$ and $T(n) \\leq c_2\\lg{n}$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &\\geq& c_1\\lg{(n - 1)} + \\frac{1}{n} \\end{eqnarray} $$ It's not obvious to see that $T(n) \\geq c_1\\lg{n}$. Let's try to guess $T(n) \\geq c_1\\lg{(n + 1)}$. So: $$ \\begin{eqnarray} T(n) &\\geq& c_1\\lg{(n - 1 + 1)} + \\frac{1}{n} \\\\ &=& c_1\\lg{n} + \\frac{1}{n} \\\\ &>& c_1\\lg{n} \\\\ &=& \\Omega(\\lg{n}) \\end{eqnarray} $$ So $T(n) = \\Omega(\\lg{n})$. And: $$ \\begin{eqnarray} T(n) &\\leq& c_2\\lg{(n - 1)} + \\frac{1}{n} \\end{eqnarray} $$ Let's try to reguess $T(n) \\leq c_2\\lg{n} - \\frac{1}{n + 1}$. So: $$ \\begin{eqnarray} T(n) &\\leq& c_2\\lg{(n - 1)} - \\frac{1}{n - 1 + 1} + \\frac{1}{n} \\\\ &=& c_2\\lg{(n - 1)} \\\\ &<& c_2\\lg{n} \\\\ &=& O(\\lg{n}) \\end{eqnarray} $$ So $T(n) = O(\\lg{n})$, thus $T(n) = \\Theta(\\lg{n})$. h First let's create a recursion tree for the recurrence $T(n) = T(n - 1) + \\lg{n}$. The number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, n - 2$, has a cost of $\\lg({n - i})$. The bottom level, at depth n - 1, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{n - 2}\\lg{(n - i)} + \\Theta(1) \\\\ &=& \\sum_{i = 2}^{n}\\lg{n} + \\Theta(1) \\\\ &=& \\lg{(n!)} + \\Theta(1) \\\\ &=& \\Theta(n\\lg{n}) \\text{ (we have proved it in 3.2-3)} \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = T(n - 1) + \\lg{n}$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{n}$ and $T(n) \\leq c_2n\\lg{n}$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &\\geq& c_1(n - 1)\\lg{(n - 1)} + \\lg{n} \\end{eqnarray} $$ Let's try to reguess $T(n) \\geq c_1(n + 1)\\lg{(n + 1)}$. So: $$ \\begin{eqnarray} T(n) &\\geq& c_1(n - 1 + 1)\\lg{(n - 1 + 1)} + \\lg{n} \\\\ &=& c_1n\\lg{n} + \\lg{n} \\\\ &>& c_1n\\lg{n} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ So $T(n) = \\Omega(n\\lg{n})$. And: $$ \\begin{eqnarray} T(n) &\\leq& c_2(n - 1)\\lg{(n - 1)} + \\lg{n} \\\\ &<& c_2(n - 1)\\lg{n} + \\lg{n} \\\\ &=& \\lg{n}(c_2(n - 1) + 1) \\\\ &\\leq& c_2n\\lg{n} \\\\ &=& O(n\\lg{n}) \\end{eqnarray} $$ where the last step holds as long as $c_2 \\geq 1$. So $T(n) = O(n\\lg{n})$. Thus $T(n) = \\Theta(n\\lg{n})$. i First let's create a recursion tree for the recurrence $T(n) = T(n - 2) + \\frac{1}{\\lg{n}}$. The number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, \\lg{(n - 1)} - 1$, has a cost of $\\frac{1}{\\lg{(n - 2^i)}}$. The bottom level, at depth $\\lg{(n - 1)}$, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{(n - 1)} - 1}\\frac{1}{\\lg{(n - 2^i)}} + \\Theta(1) \\end{eqnarray} $$ But I don't know how to compute the sum. j Let's solve it by the iterative method and assume n is an exact power of 2, and n is also a perfect square. $$ \\begin{eqnarray} T(n) &=& \\sqrt{n}T(\\sqrt{n}) + n \\\\ &=& n^{\\frac{1}{2}}T(n^{\\frac{1}{2}}) + n \\\\ &=& n^{\\frac{1}{2}}((n^{\\frac{1}{2}})^{\\frac{1}{2}}T((n^{\\frac{1}{2}})^{\\frac{1}{2}}) + n^{\\frac{1}{2}}) + n \\\\ &=& n^{\\frac{1}{2}}(n^{\\frac{1}{4}}T(n^{\\frac{1}{4}}) + n^{\\frac{1}{2}}) + n \\\\ &=& n^{\\frac{1}{2} + \\frac{1}{4}}T(n^{\\frac{1}{4}}) + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\ &=& n^{\\frac{1}{2} + \\frac{1}{4}}(n^{\\frac{1}{8}}T(n^{\\frac{1}{8}}) + n^{\\frac{1}{4}}) + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\ &=& n^{\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8}}T(n^{\\frac{1}{8}}) + n^{\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{4}} + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\ &=& \\ldots \\\\ &=& n^{\\sum_{i = 1}^{\\lg{\\sqrt{n}}}\\frac{1}{2^i}}T(2) + \\lg{\\sqrt{n}} * n \\\\ &<& n^{\\sum_{i = 1}^{\\infty}\\frac{1}{2^i}}T(2) + \\lg{\\sqrt{n}} * n \\\\ &=& n^{\\frac{\\frac{1}{2}}{1 - \\frac{1}{2}}}T(2) + \\frac{1}{2}n\\lg{n} \\\\ &=& nT(2) + \\frac{1}{2}n\\lg{n} \\\\ &=& O(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n\\lg{n})$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn\\lg{n}$ for some constant c > 0. So: $$ \\begin{eqnarray} T(n) &=& \\sqrt{n}T(\\sqrt{n}) + n \\\\ &\\leq& \\sqrt{n}c\\sqrt{n}\\lg{\\sqrt{n}} + n \\\\ &=& cn\\lg{\\sqrt{n}} + n \\\\ &=& \\frac{1}{2}cn\\lg{n} + n \\end{eqnarray} $$ Because $\\frac{1}{2}cn\\lg{n} + n - cn\\lg{n} = n(1 - \\frac{1}{2}c\\lg{n}) \\leq n(1 - \\frac{1}{2}c)$ when $n \\geq 2$. So $n(1 - \\frac{1}{2}c) \\leq 0$ when $c \\geq 2$. So $T(n) \\leq cn\\lg{n}$ when $c \\geq 2$ and $n \\geq 2$. Thus, $T(n) = O(n\\lg{n})$. But I don't know how to get the lower bound. 4-4 a $$ \\begin{eqnarray} z + z\\mathcal{F}(z) + z^2\\mathcal{F}(Z) &=& z + z\\sum_{i = 0}^{\\infty}F_iz^i + z^2\\sum_{i = 0}^{\\infty}F_iz^i \\\\ &=& z + (0 + z^2 + z^3 + 2z^4 + 3z^5 + \\ldots) + (0 + z^3 + z^4 + 2z^5 + \\ldots) \\\\ &=& 0 + z + z^2 + 2z^3 + 3z^4 + 5z^5 + \\ldots \\\\ &=& \\mathcal{F}(z) \\end{eqnarray} $$ b Because $\\mathcal{F}(z) = z + z\\mathcal{F}(z) + z^2\\mathcal{F}(z)$, so $\\mathcal{F}(z) - z\\mathcal{F}(z) - z^2\\mathcal{F}(z) = z$, thus $\\mathcal{F}(z)(1 - z - z^2) = z$, so $\\mathcal{F}(z) = \\frac{z}{1 - z - z^2}$. And: $$ \\begin{eqnarray} \\frac{z}{(1 - \\phi{z})(1 - \\hat\\phi{z})} &=& \\frac{z}{1 - (\\phi + \\hat\\phi)z + \\phi\\hat\\phi{z^2}} \\\\ &=& \\frac{z}{1 - (\\frac{1 - \\sqrt{5}}{2} + \\frac{1 + \\sqrt{5}}{2})z + \\frac{1 - \\sqrt{5}}{2}\\frac{1 + \\sqrt{5}}{2}z^2} \\\\ &=& \\frac{z}{1 - z - z^2} \\end{eqnarray} $$ And: $$ \\begin{eqnarray} \\frac{1}{\\sqrt{5}}(\\frac{1}{1 - \\phi{z}} - \\frac{1}{1 - \\hat\\phi{z}}) &=& \\frac{1}{\\sqrt{5}}\\frac{1 - \\hat\\phi{z} - 1 + \\phi{z}}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ &=& \\frac{1}{\\sqrt{5}}\\frac{z(\\phi - \\hat\\phi)}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ &=& \\frac{1}{\\sqrt{5}}\\frac{z(\\frac{1 + \\sqrt{5}}{2} - \\frac{1 - \\sqrt{5}}{2})}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ &=& \\frac{1}{\\sqrt{5}}\\frac{z\\sqrt{5}}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ &=& \\frac{z}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\end{eqnarray} $$ c $$ \\begin{eqnarray} \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)z^i &=& \\frac{1}{\\sqrt{5}}\\sum_{i = 0}^{\\infty}(\\phi^iz^i - \\hat\\phi^iz^i) \\\\ &=& \\frac{1}{\\sqrt{5}}(\\sum_{i = 0}^{\\infty}\\phi^iz^i - \\sum_{i = 0}^{\\infty}\\hat\\phi^iz^i) \\\\ &=& \\frac{1}{\\sqrt{5}}(\\frac{1}{1 - \\phi{z}} - \\frac{1}{1 - \\hat\\phi{z}}) \\\\ &=& \\mathcal{F}(z) \\end{eqnarray} $$ d Because $\\mathcal{F}(z) = \\sum_{i = 0}^{\\infty}F_iz^i$ and $\\mathcal{F}(z) = \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)z^i$, so we have $F_i = \\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)$. Since $|\\hat\\phi| < 1$, we have $\\frac{|\\hat\\phi^i|}{\\sqrt{5}} < \\frac{1}{\\sqrt{5}} < \\frac{1}{2}$, which implies that $F_i = \\lfloor \\frac{\\phi^i}{\\sqrt{5}} + \\frac{1}{2} \\rfloor$, which is to say that the ith Fibonacci number $F_i$ is equal to $\\frac{\\phi^i}{\\sqrt{5}}$ rounded to the nearest integer. 4-5 a 4-6 a The \"only if\" part is easy. If an array is Monge, then for all i, j, k and l such that $l \\leq i < k \\leq m$ and $l \\leq j < l \\leq n$ we have $A[i, j] + A[k, l] \\leq A[i, l] + A[k, j]$. So we let k = i + 1, l = j + 1, then we have $A[i, j] + A[i + 1, j + 1] \\leq A[i, j + 1] + A[i + 1, j]$. So we proved the \"only if\" part. Then let's prove the \"if\" part, we'll use induction separately on rows and columns. Let's get on columns first. The base case is already given, let's assume $A[i, j] + A[i + 1, j + k] \\leq A[i, j + k] + A[i + 1, j]$ for $k \\geq 1$ and $j + k \\leq n$. Then we need to prove $A[i, j] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j]$. According to the definition, we have $A[i, j + k] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j + k]$, thus: $$\\begin{aligned} A[i, j] + A[i + 1, j + k] + A[i, j + k] + A[i + 1, j + k + 1] \\leq & A[i, j + k] + A[i + 1, j] + \\\\ & A[i, j + k + 1] + A[i + 1, j + k] \\end{aligned}$$ So we get $A[i, j] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j]$. So the induction is correct, and similarly we can prove: $$ \\begin{eqnarray} A[i, j + 1] + A[i + 1, j + k] &\\leq& A[i, j + k] + A[i + 1, j + 1] \\, (j + k \\leq n), \\ldots, A[i, j + p] + A[i + 1, j + k] \\\\ &\\leq& A[i, j + k] + A[i + 1, j + p] \\, (j + p \\leq j + k - 1, j + k \\leq n) \\end{eqnarray} $$ thus we can see that all adjacent rows are Monge arrays. Similarly, we can use the induction on rows, so all adjacent columns are Monge arrays. For any $m_1 x n_1$ array, if $m_1 = 2$ or $n_1 = 2$, then the subarray is a Monge array. And we need to prove the subarray is also a Monge array if $m_1 \\geq 3$ and $n_1 \\geq 3$. Let's assume the subarray starts at row $A[i_1, j_1]$ and ends at $A[i_2, j_2]$. Since any adjacent columns are Monge arrays, so we have: $$A[i_1, j_1] + A[i_2, j_1 + 1] \\leq A[i_1, j_1 + 1] + A[i_2, j_1]$$ $$A[i_1, j_1 + 1] + A[i_2, j_1 + 2] \\leq A[i_1, j_1 + 2] + A[i_2, j_1 + 1]$$ $$\\ldots$$ $$A[i_1, j_2 - 1] + A[i_2, j_2] \\leq A[i_1, j_2] + A[i_2, j_2 - 1]$$ Then we sum all inequations together, so we have: $$A[i_1, j_1] + A[i_2, j_2] \\leq A[i_1, j_2] + A[i_2, j_1]$$, which means the subarray $A[i_1, j_1]$ to $A[i_2, j_2]$ is a Monge array when $i_2 - i_1 \\geq 3$ and $j_2 - j_1 \\geq 3$. So for all subarrays in A, they are Monge arrays. So we proved the \"if\" part. b \\begin{matrix} 37 & 23 & 22 & 32 \\\\ 21 & 6 & 5 & 10 \\\\ 53 & 34 & 30 & 31 \\\\ 32 & 13 & 9 & 6 \\\\ 43 & 21 & 15 & 8 \\\\ \\end{matrix} c Let's consider row i and row i + 1. Let $f(i) = j_1$ and $f(i + 1) = j_2$ and assume $f(i) > f(i + 1)$, so we have $j_1 > j_2$. Since A is a Monge array, so we have $A[i, j_2] + A[i + 1, j_1] \\leq A[i, j_1] + A[i + 1, j_2]$. But according to the definition of f(i), we have $A[i, j_2] > A[i, j_1]$ and $A[i + 1, j_2] < A[i + 1, j_1]$, combine them together: $A[i, j_2] + A[i + 1, j_1] > A[i, j_1] + A[i + 1, j_2]$. Thus the assumption is wrong. So $f(i) \\leq f(i + 1)$. So $f(1) \\leq f(2) \\leq \\ldots \\leq f(m)$ for any m x n Monge array. d Let's assume $m = 2k, k = 1, 2, \\ldots$, from the previous question we have $f(2k - 2) \\leq f(2k - 1) \\leq f(2k)$. We want to get f(2k - 1) when f(2k - 2) and f(2k) are already known. We only need to compare the numbers from A[2k - 1][f(2k - 2)] to A[2k - 1][f(2k)], which costs at most $O(f(2k) - f(2k - 2) + 1)$. Thus: $$ \\begin{eqnarray} T &=& \\sum_{k = 1}^{\\frac{m}{2}}O(f(2k) - f(2k - 2) + 1) \\\\ &=& (f(2) - f(0) + 1) + (f(4) - f(2) + 1) + \\ldots + (f(m) - f(m - 2) + 1) \\\\ &=& f(m) - f(0) + \\frac{m}{2} \\\\ &\\leq& n - 0 + \\frac{m}{2} \\\\ &<& n + m \\\\ &=& O(m + n) \\end{eqnarray} $$ e $$ \\begin{eqnarray} T(m) &=& T(\\frac{m}{2}) + O(m + n) \\\\ &=& T(\\frac{m}{4}) + O(\\frac{m}{2} + n) + O(m + n) \\\\ &=& T(\\frac{m}{8}) + O(\\frac{m}{4} + n) + O(\\frac{m}{2} + n) + O(m + n) \\\\ &=& \\ldots \\\\ &=& T(1) + \\sum_{i = 0}^{\\lg{m} - 1}O(\\frac{m}{2^i}) + \\sum_{i = 0}^{\\lg{m} - 1}O(n) \\\\ &<& T(1) + \\sum_{i = 0}^{\\infty}O(\\frac{m}{2^i}) + \\sum_{i = 0}^{\\lg{m} - 1}O(n) \\\\ &=& T(1) + O(m\\frac{1}{1 - \\frac{1}{2}}) + O(n\\lg{m}) \\\\ &=& O(1) + O(2m) + O(n\\lg{m}) \\\\ &=& O(m + n\\lg{m}) \\end{eqnarray} $$ def leftmost_min_element_in_each_row_of_monge_array(matrix): # Column index of leftmost min element in each row leftmost_min_element_in_each_row = [0] * len(matrix) leftmost_min_element_divide_and_conquer( matrix, leftmost_min_element_in_each_row, 0, len(matrix) - 1, 1) return leftmost_min_element_in_each_row def leftmost_min_element_divide_and_conquer( matrix, leftmost_min_element_in_each_row, row_start, row_end, step): if row_start == row_end: leftmost_min_element_in_each_row[row_start] = \\ find_leftmost_min_element_in_row( matrix, row_start, 0, len(matrix[row_start]) - 1) else: # Construct a submatrix consisting of the even-numbered rows sub_matrix_row_start = row_start + step sub_matrix_row_end = 0 if row_end % 2 == 0 or ((row_end - row_start) // step) % 2 == 0: sub_matrix_row_end = row_end - step elif ((row_end - row_start) // step) % 2 == 1: sub_matrix_row_end = row_end leftmost_min_element_divide_and_conquer( matrix, leftmost_min_element_in_each_row, sub_matrix_row_start, sub_matrix_row_end, step * 2) leftmost_min_element_in_odd_numbered_rows( matrix, leftmost_min_element_in_each_row, row_start, row_end, step) def find_leftmost_min_element_in_row(matrix, row, column_start, column_end): min_column_index = 0 for column in range(column_start, column_end + 1): if matrix[row][column] < matrix[row][min_column_index]: min_column_index = column return min_column_index def leftmost_min_element_in_odd_numbered_rows( matrix, leftmost_min_element_in_each_row, row_start, row_end, step): for odd_numbered_row in range(row_start, row_end + 1, step * 2): prev_even_numbered_row = odd_numbered_row - step next_even_numbered_row = odd_numbered_row + step column_start = -1 column_end = -1 if prev_even_numbered_row >= row_start and \\ next_even_numbered_row <= row_end: column_start = \\ leftmost_min_element_in_each_row[prev_even_numbered_row] column_end = \\ leftmost_min_element_in_each_row[next_even_numbered_row] elif prev_even_numbered_row >= row_start: column_start = \\ leftmost_min_element_in_each_row[prev_even_numbered_row] column_end = len(matrix[0]) - 1 elif next_even_numbered_row <= row_end: column_start = 0 column_end = \\ leftmost_min_element_in_each_row[next_even_numbered_row] leftmost_min_element_in_each_row[odd_numbered_row] = \\ find_leftmost_min_element_in_row( matrix, odd_numbered_row, column_start, column_end)","title":"Problems"},{"location":"04-Divide-and-Conquer/Problems/#problems","text":"","title":"Problems"},{"location":"04-Divide-and-Conquer/Problems/#4-1","text":"","title":"4-1"},{"location":"04-Divide-and-Conquer/Problems/#a","text":"Here, we have a = 2, b = 2, and $f(n) = \\Theta(n^4)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{2}} = n$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 3$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 2f(\\frac{n}{2}) = \\frac{n^4}{8} = cf(n)$, for $c = \\frac{1}{8}$. So, the solution to the recurrence is $T(n) = \\Theta(n^4)$.","title":"a"},{"location":"04-Divide-and-Conquer/Problems/#b","text":"Here, we have a = 1, $b = \\frac{10}{7}$, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_{\\frac{10}{7}}{1}} = 1$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 1$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = f(\\frac{7n}{10}) = \\frac{7n}{10} = cf(n)$, for $c = \\frac{7}{10}$. So, the solution to the recurrence is $T(n) = \\Theta(n)$.","title":"b"},{"location":"04-Divide-and-Conquer/Problems/#c","text":"Here, we have a = 16, b = 4, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_4{16}} = n^2$. So case 2 applies, so the solution to the recurrence is $T(n) = \\Theta(n^2\\lg{n})$.","title":"c"},{"location":"04-Divide-and-Conquer/Problems/#d","text":"Here, we have a = 7, b = 3, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{7}}$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 2 - \\log_3{7}$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 7f(\\frac{n}{3}) = \\frac{7n^2}{9} = cf(n)$, for $c = \\frac{7}{9}$. So, the solution to the recurrence is $T(n) = \\Theta(n^2)$.","title":"d"},{"location":"04-Divide-and-Conquer/Problems/#e","text":"Here, we have a = 7, b = 2, and $f(n) = \\Theta(n^2)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{7}} = n^{\\lg{7}}$. Since $f(n) = O(n^{\\log_b{a} - \\epsilon})$, where $\\epsilon \\leq \\lg{\\frac{7}{4}}$, case 1 applies. So, the solution to the recurrence is $T(n) = \\Theta(n^{\\lg7})$.","title":"e"},{"location":"04-Divide-and-Conquer/Problems/#f","text":"Here, we have a = 2, b = 4, and $f(n) = \\Theta(\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = \\sqrt{n}$, case 2 applies. So, the solution to the recurrence is $T(n) = \\Theta(\\sqrt{n}\\lg{n})$.","title":"f"},{"location":"04-Divide-and-Conquer/Problems/#g","text":"First let's create a recursion tree for the recurrence $T(n) = T(n - 2) + n^2$. The number of nodes at depth i is 1. And each node at depth i, for $i = 0, 1, 2, \\ldots, \\frac{n - 1}{2} - 1$, has a cost of $(n - 2i)^2$. So the total cost over all nodes at depth i, is $(n - 2i)^2$. The bottom level, at depth $\\frac{n - 1}{2}$, has 1 node, which contributing cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}(n - 2i)^2 + \\Theta(1) \\\\ &=& \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}(n^2 - 2ni + 4i^2) + \\Theta(1) \\\\ &=& \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}n^2 - \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}2ni + \\sum_{i = 0}^{\\frac{n - 1}{2} - 1}4i^2 + \\Theta(1) \\\\ &=& \\frac{n - 1}{2}n^2 - 2n\\frac{(\\frac{n - 1}{2} - 1)(1 + \\frac{n - 1}{2} - 1)}{2} + 4\\frac{(\\frac{n - 1}{2} - 1)(\\frac{n - 1}{2} - 1 + 1)(2(\\frac{n - 1}{2} - 1) + 1)}{6} + \\Theta(1) \\\\ &=& \\frac{n^2(n - 1)}{2} - \\frac{n(n - 1)(n - 3)}{4} + \\frac{(n - 1)(n - 2)(n - 3)}{6} + \\Theta(1) \\\\ &=& \\frac{5n^3 - 6n^2 + 13n -12}{6} + \\Theta(1) \\\\ &=& \\Theta(n^3) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Theta(n^3)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n^3$ and $T(n) \\leq c_2n^3$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(n - 2) + n^2 \\\\ &\\geq& c_1(n - 2)^3 + n^2 \\\\ &=& c_1n^3 + (1 - 6c_1)n^2 + 4c_1(3n - 2) \\\\ &>& c_1n^3 \\end{eqnarray} $$ where the last step holds as long as $c_1 \\leq \\frac{1}{6}$. So $T(n) = \\Omega(n^3)$. And: $$ \\begin{eqnarray} T(n) &=& T(n - 2) + n^2 \\\\ &\\leq& c_2(n - 2)^3 + n^2 \\\\ &=& c_2n^3 + n((1 - 6c_2)n + 12c_2) - 8c_2 \\\\ &\\leq& c_2n^3 \\end{eqnarray} $$ where the last step holds as long as $c_2 > \\frac{1}{6}$ and $n \\geq \\frac{12c_2}{6c_2 - 1}$. So $T(n) = O(n^3)$, thus $T(n) = \\Theta(n^3)$.","title":"g"},{"location":"04-Divide-and-Conquer/Problems/#4-2","text":"","title":"4-2"},{"location":"04-Divide-and-Conquer/Problems/#a_1","text":"Here is the pseudocode of recursive binary search algorithm: RECURSIVE-BINARY-SEARCH(A, v, low, high) if low <= high middle = (low + high) / 2 if A[middle] < v return RECURSIVE-BINARY-SEARCH(A, v, middle + 1, high) else if A[middle] > v return RECURSIVE-BINARY-SEARCH(A, v, low, middle - 1) else return middle return NIL","title":"a"},{"location":"04-Divide-and-Conquer/Problems/#an-array-is-passed-by-pointer","text":"Before it halves the problem size, it needs to do some operations like comparing low and high , calculating middle . But they are constant operations, we can let it be $\\Theta(1)$. So $T(n) = T(\\frac{n}{2}) + \\Theta(1)$. Here, we have a = 1, b = 2, and $f(n) = \\Theta(1)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. So case 2 applies, thus $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(\\lg{n}) = \\Theta(\\lg{N})$.","title":"An array is passed by pointer"},{"location":"04-Divide-and-Conquer/Problems/#an-array-is-passed-by-copying","text":"Each time it halves the problem size, it needs additional $\\Theta(N)$ operation to copy the array. So: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + \\Theta(N) + \\Theta(1) \\\\ &=& T(\\frac{n}{2}) + \\Theta(N) \\\\ &=& T(\\frac{n}{4}) + \\Theta(N) + \\Theta(N) \\\\ &=& \\ldots = T(1) + \\lg{n}\\Theta(N) \\\\ &=& \\Theta(N\\lg{N}) \\end{eqnarray} $$.","title":"An array is passed by copying"},{"location":"04-Divide-and-Conquer/Problems/#an-array-is-passed-by-range","text":"Each time it halves the problem size, it needs additional $\\Theta(n)$ operation to copy the array. So $T(n) = T(\\frac{n}{2}) + \\Theta(n) + \\Theta(1) = T(\\frac{n}{2}) + \\Theta(n)$. Here, we have a = 1, b = 2, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_2{1}} = 1$. Since $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$, where $\\epsilon \\leq 1$, case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = f(\\frac{n}{2}) = \\Theta(\\frac{n}{2}) = c\\Theta(n)$, for $c = \\frac{1}{2}$. So, the solution to the recurrence is $T(n) = \\Theta(n) = \\Theta(N)$.","title":"An array is passed by range"},{"location":"04-Divide-and-Conquer/Problems/#b_1","text":"Here is the pseudocode of merge sor algorithm: MERGE-SORT(A, p, r) if p < r q = (p + r) / 2 MERGE-SORT(A, p ,q) MERGE-SORT(A, q + 1, r) MERGE(A, p, q, r)","title":"b"},{"location":"04-Divide-and-Conquer/Problems/#an-array-is-passed-by-pointer_1","text":"We already know the solution that $T(n) = \\Theta(n\\lg{n}) = \\Theta(N\\lg{N})$.","title":"An array is passed by pointer"},{"location":"04-Divide-and-Conquer/Problems/#an-array-is-passed-by-copying_1","text":"From the pseudocode we know it needs to pass the array 3 times to subroutine. So we have $T(n) = 2T(\\frac{n}{2}) + \\Theta(n) + 3\\Theta(N) = 2T(\\frac{n}{2}) + \\Theta(n) + \\Theta(N)$. Let's use the iterative method to solve it: $$ \\begin{eqnarray} T(n) &=& 2T(\\frac{n}{2}) + \\Theta(n) + \\Theta(N) \\\\ &=& 2(2T(\\frac{n}{4}) + \\Theta(\\frac{n}{2}) + \\Theta(N)) + \\Theta(n) + \\Theta(N) \\\\ &=& 4T(\\frac{n}{4}) + 2\\Theta(\\frac{n}{2}) + 2\\Theta(N) + \\Theta(n) + \\Theta(N) \\\\ &=& 4T(\\frac{n}{4}) + 2\\Theta(n) + 3\\Theta(N) \\\\ &=& \\ldots \\\\ &=& 2^iT(\\frac{n}{2^i}) + i\\Theta(n) + (1 + 2 + \\ldots + 2^{i - 1})\\Theta(N) \\text{ for } i = 1, 2, \\ldots, \\lg{n} \\\\ &=& 2^{\\lg{n}}T(1) + \\lg{n}\\Theta(n) + \\Theta(N)\\frac{1 - 2^{\\lg{n}}}{1 - 2} \\\\ &=& nT(1) + \\Theta(n\\lg{n}) + \\Theta(Nn) \\\\ &=& \\Theta(N^2) \\end{eqnarray} $$","title":"An array is passed by copying"},{"location":"04-Divide-and-Conquer/Problems/#an-array-is-passed-by-range_1","text":"It needs additional $3\\Theta(n)$ to copy the array. So $T(n) = 2T(\\frac{n}{2}) + \\Theta(n) + 3\\Theta(n) = 2T(\\frac{n}{2}) + \\Theta(n) = \\Theta(n\\lg{n}) = \\Theta(N\\lg{N})$.","title":"An array is passed by range"},{"location":"04-Divide-and-Conquer/Problems/#4-3","text":"","title":"4-3"},{"location":"04-Divide-and-Conquer/Problems/#a_2","text":"Here, we have a = 4, b = 3, and $f(n) = \\Theta(n\\lg{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_3{4}}$. In problems 3-2-a, we proved that $\\lg^k{n} = O(n^\\epsilon)$ for $k \\geq 1$ and $\\epsilon > 0$, so $\\lg{n} = O(n^{\\epsilon})$. Since $\\log_3{4} \\approx 1.2618595071429148$, so $f(n) = O(n^{\\log_ba - \\epsilon})$ for $\\epsilon \\leq 0.26$. So case 1 applies, the solution to the recurrence is $T(n) = \\Theta(n^{\\log_34})$.","title":"a"},{"location":"04-Divide-and-Conquer/Problems/#b_2","text":"Here, we have a = 3, b = 3, and $f(n) = \\Theta(\\frac{n}{\\lg{n}})$, and thus we have that $n^{\\log_ba} = n^{\\log_3{3}} = n$. So $f(n) = O(n^{\\log_ba})$. But it's not that easy to prove that $f(n) = O(n^{\\log_ba - \\epsilon})$. Let's solve it by iterative method: $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &=& 3(3T(\\frac{n}{9}) + \\frac{\\frac{n}{3}}{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& 9T(\\frac{n}{9}) + \\frac{n}{\\lg{n} - \\lg3} + \\frac{n}{\\lg{n}} \\\\ &=& \\ldots \\\\ &=& 3^iT(\\frac{n}{3^i}) + \\sum_{i = 1}^{\\log_3{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg3} \\\\ &=& nT(1) + \\sum_{i = 1}^{\\log_3{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg3} \\\\ &=& nT(1) + \\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\lg{n} - i\\lg3} \\\\ &=& nT(1) + \\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\frac{\\log_3{n}}{\\log_3{2}} - i\\frac{\\log_3{3}}{\\log_3{2}}} \\\\ &=& nT(1) + \\log_3{2}\\sum_{i = 0}^{\\log_3{n} - 1}\\frac{n}{\\log_3{n} - i} \\\\ &=& nT(1) + \\log_3{2}\\sum_{i = 1}^{\\log_3{n}}\\frac{n}{i} \\\\ &=& nT(1) + \\log_3{2}(n(\\ln{\\log_3{n}} + O(1))) \\text{ by equation (A.7)} \\\\ &=& \\Theta(n\\lg{\\lg{n}}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Theta(n\\lg{\\lg{n}})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{\\lg{n}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}}$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &\\leq& 3(c_2\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_2n\\lg{(\\lg{n} - \\lg3)} + \\frac{n}{\\lg{n}} \\end{eqnarray} $$ It's not that easy to prove that $T(n) \\leq c_2n\\lg{\\lg{n}}$, let's guess $T(n) \\leq c_2n\\lg{\\lg{n}} - \\frac{n}{\\lg{3n}}$. $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &\\leq& 3(c_2\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}} - \\frac{\\frac{n}{3}}{\\lg{3\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_2n\\lg{(\\lg{n} - \\lg3)} - \\frac{n}{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\ &<& c_2n\\lg{\\lg{n}} \\end{eqnarray} $$ So $T(n) = O(n\\lg{\\lg{n}})$. And: $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &\\geq& 3(c_1\\frac{n}{3}\\lg{\\lg{\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_1n\\lg{(\\lg{n} - \\lg3)} + \\frac{n}{\\lg{n}} \\end{eqnarray} $$ Let's reguess $T(n) \\geq c_1n\\lg{\\lg{3n}}$. So: $$ \\begin{eqnarray} T(n) &=& 3T(\\frac{n}{3}) + \\frac{n}{\\lg{n}} \\\\ &\\geq& 3(c_1\\frac{n}{3}\\lg{\\lg{3\\frac{n}{3}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_1n\\lg{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\ &>& c_1n\\lg{\\lg{n}} \\end{eqnarray} $$ So $T(n) = \\Omega(n\\lg{\\lg{n}})$. Thus = $T(n) \\ Theta(n\\lg{\\lg{n}})$.","title":"b"},{"location":"04-Divide-and-Conquer/Problems/#c_1","text":"Here, we have a = 4, b = 2, and $f(n) = \\Theta(n^2\\sqrt{n})$, and thus we have that $n^{\\log_ba} = n^{\\log_4{2}} = \\sqrt{n}$. So $f(n) = \\Omega(n^{\\log_ba + \\epsilon})$ for $\\epsilon \\leq 2$. case 3 applies. And for sufficiently large n, we have that $af(\\frac{n}{b}) = 4f(\\frac{n}{2}) = \\frac{\\sqrt{2}}{2}n^2\\sqrt{n} = cf(n)$, for $c = \\frac{\\sqrt{2}}{2}$. So, the solution to the recurrence is $T(n) = \\Theta(n^2\\sqrt{n})$.","title":"c"},{"location":"04-Divide-and-Conquer/Problems/#d_1","text":"If n is large enough, then we can ignore the -2 . Then we can solve it by the master method. So we have a = 3, b = 3, and $f(n) = \\Theta(n)$, and thus we have that $n^{\\log_ba} = n^{\\log_3{3}} = n$. Case 2 applies, thus the solution to the recurrence is $T(n) = \\Theta(n^{\\log_ba}\\lg{n}) = \\Theta(n\\lg{n})$.","title":"d"},{"location":"04-Divide-and-Conquer/Problems/#e_1","text":"Let's try to solve a general form of the recurrence $T(n) = aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}}, a > 1$. $$ \\begin{eqnarray} T(n) &=& aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\ &=& a(aT(\\frac{n}{a^2}) + \\frac{\\frac{n}{a}}{\\lg{\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\ &=& a^2T(\\frac{n}{a^2}) + \\frac{n}{\\lg{n} - \\lg{a}} + \\frac{n}{\\lg{n}} \\\\ &=& \\ldots \\\\ &=& a^iT(\\frac{n}{a^i}) + \\sum_{i = 1}^{\\log_a{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg{a}} \\\\ &=& nT(1) + \\sum_{i = 1}^{\\log_a{n}}\\frac{n}{\\lg{n} - (i - 1)\\lg{a}} \\\\ &=& nT(1) + \\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\lg{n} - i\\lg{a}} \\\\ &=& nT(1) + \\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\frac{\\log_a{n}}{\\log_a{2}} - i\\frac{\\log_a{a}}{\\log_a{2}}} \\\\ &=& nT(1) + \\log_a{2}\\sum_{i = 0}^{\\log_a{n} - 1}\\frac{n}{\\log_a{n} - i} \\\\ &=& nT(1) + \\log_a{2}\\sum_{i = 1}^{\\log_a{n}}\\frac{n}{i} \\\\ &=& nT(1) + \\log_a{2}(n(\\ln{\\log_a{n}} + O(1))) \\text{ by equation (A.7)} \\\\ &=& \\Theta(n\\lg{\\lg{n}}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = \\Theta(n\\lg{\\lg{n}})$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{\\lg{n}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}}$ for some constants $c_1 > 0$ and $c_2 > 0$. Like the problem b, let's guess $T(n) \\geq c_1n\\lg{\\lg{an}}$ and $T(n) \\leq c_2n\\lg{\\lg{n}} - \\frac{n}{\\lg{an}}$. $$ \\begin{eqnarray} T(n) &=& aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\ &\\leq& a(c_2\\frac{n}{a}\\lg{\\lg{\\frac{n}{a}}} - \\frac{\\frac{n}{a}}{\\lg{a\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_2n\\lg{(\\lg{n} - \\lg{a})} - \\frac{n}{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\ &<& c_2n\\lg{\\lg{n}} \\end{eqnarray} $$ So $T(n) = O(n\\lg{\\lg{n}})$. And: $$ \\begin{eqnarray} T(n) &=& aT(\\frac{n}{a}) + \\frac{n}{\\lg{n}} \\\\ &\\geq& a(c_1\\frac{n}{a}\\lg{\\lg{a\\frac{n}{a}}}) + \\frac{n}{\\lg{n}} \\\\ &=& c_1n\\lg{\\lg{n}} + \\frac{n}{\\lg{n}} \\\\ &>& c_1n\\lg{\\lg{n}} \\end{eqnarray} $$ So $T(n) = \\Omega(n\\lg{\\lg{n}})$. Thus = $T(n) = \\Theta(n\\lg{\\lg{n}})$. The solution to recurrence $T(n) = 2T(\\frac{n}{2}) + \\frac{n}{\\lg{n}}$ is $\\Theta(n\\lg{\\lg{n}})$.","title":"e"},{"location":"04-Divide-and-Conquer/Problems/#f_1","text":"First let's create a recursion tree for the recurrence $T(n) = T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n$ and assume that n is an exact power of 8. We've solved similar problems before. Not all branch reaches at the bottom at the same time. The right most branch reaches at the bottom first. The left most branch is the last one that reaches at the bottom. And we can see the total cost over all nodes at depth i is $(\\frac{7}{8})^in$. If the left most branch reaches at the bottom, and assume the current depth is k, then we assume other branches reach at depth k - 1 at the same time. So we have: $$ \\begin{eqnarray} T(n) &\\leq& \\sum_{i = 0}^{\\lg{n} - 1}(\\frac{7}{8})^in + T(1) \\\\ &=& n\\frac{1 - (\\frac{7}{8})^{\\lg{n}}}{1 - \\frac{7}{8}} + T(1) \\\\ &=& 8n(1 - (\\frac{7}{8})^{\\lg{n}}) + T(1) \\\\ &<& 8n + T(1) \\\\ &\\leq& 9n \\\\ &=& O(n) \\end{eqnarray} $$ where the last step holds as long as $n \\geq T(1)$. Then if the right most branch reaches at the bottom, other branches have not reach at the bottom. But we assume they also reach at the bottom. So we have: $$ \\begin{eqnarray} T(n) &\\geq& \\sum_{i = 0}^{\\log_8{n} - 1}(\\frac{7}{8})^in + (\\frac{7}{8})^{\\log_8{n}}n \\\\ &=& n\\frac{1 - (\\frac{7}{8})^{\\log_8{n} + 1}}{1 - \\frac{7}{8}} \\\\ &=& 8n(1 - (\\frac{7}{8})^{\\log_8{n} + 1}) \\\\ &\\geq& 8n(1 - (\\frac{7}{8})^{\\log_8{1} + 1}) \\\\ &=& n \\\\ &=& \\Omega(n) \\end{eqnarray} $$ Thus we devird a guess of $T(n) = \\Theta(n)$ for our original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n$ and $T(n) \\leq c_2n$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n \\\\ &\\geq& c_1\\frac{n}{2} + c_1\\frac{n}{4} + c_1\\frac{n}{8} + n \\\\ &=& (1 + \\frac{7}{8}c_1)n \\\\ &\\geq& c_1n \\\\ &=& \\Omega(n) \\end{eqnarray} $$ where the last step holds as long as $c_1 \\leq 8$. And: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + T(\\frac{n}{4}) + T(\\frac{n}{8}) + n \\\\ &\\leq& c_2\\frac{n}{2} + c_2\\frac{n}{4} + c_2\\frac{n}{8} + n \\\\ &=& (1 + \\frac{7}{8}c_2)n \\\\ &\\leq& c_2n \\\\ &=& O(n) \\end{eqnarray} $$ where the last step holds as long as $c_2 \\geq 8$. Thus $T(n) = \\Theta(n)$.","title":"f"},{"location":"04-Divide-and-Conquer/Problems/#g_1","text":"First let's create a recursion tree for the recurrence $T(n) = T(n - 1) + \\frac{1}{n}$. The number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, n - 2$, has a cost of $\\frac{1}{n - i}$. The bottom level, at depth n - 1, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{n - 2}\\frac{1}{n - i} + \\Theta(1) \\\\ &=& \\sum_{i = 2}^{n}\\frac{1}{i} + \\Theta(1) \\\\ &=& \\ln{n} - 1 + \\Theta(1) \\\\ &=& \\Theta(n) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = T(n - 1) + \\frac{1}{n}$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1\\lg{n}$ and $T(n) \\leq c_2\\lg{n}$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &\\geq& c_1\\lg{(n - 1)} + \\frac{1}{n} \\end{eqnarray} $$ It's not obvious to see that $T(n) \\geq c_1\\lg{n}$. Let's try to guess $T(n) \\geq c_1\\lg{(n + 1)}$. So: $$ \\begin{eqnarray} T(n) &\\geq& c_1\\lg{(n - 1 + 1)} + \\frac{1}{n} \\\\ &=& c_1\\lg{n} + \\frac{1}{n} \\\\ &>& c_1\\lg{n} \\\\ &=& \\Omega(\\lg{n}) \\end{eqnarray} $$ So $T(n) = \\Omega(\\lg{n})$. And: $$ \\begin{eqnarray} T(n) &\\leq& c_2\\lg{(n - 1)} + \\frac{1}{n} \\end{eqnarray} $$ Let's try to reguess $T(n) \\leq c_2\\lg{n} - \\frac{1}{n + 1}$. So: $$ \\begin{eqnarray} T(n) &\\leq& c_2\\lg{(n - 1)} - \\frac{1}{n - 1 + 1} + \\frac{1}{n} \\\\ &=& c_2\\lg{(n - 1)} \\\\ &<& c_2\\lg{n} \\\\ &=& O(\\lg{n}) \\end{eqnarray} $$ So $T(n) = O(\\lg{n})$, thus $T(n) = \\Theta(\\lg{n})$.","title":"g"},{"location":"04-Divide-and-Conquer/Problems/#h","text":"First let's create a recursion tree for the recurrence $T(n) = T(n - 1) + \\lg{n}$. The number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, n - 2$, has a cost of $\\lg({n - i})$. The bottom level, at depth n - 1, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{n - 2}\\lg{(n - i)} + \\Theta(1) \\\\ &=& \\sum_{i = 2}^{n}\\lg{n} + \\Theta(1) \\\\ &=& \\lg{(n!)} + \\Theta(1) \\\\ &=& \\Theta(n\\lg{n}) \\text{ (we have proved it in 3.2-3)} \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = T(n - 1) + \\lg{n}$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\geq c_1n\\lg{n}$ and $T(n) \\leq c_2n\\lg{n}$ for some constants $c_1 > 0$ and $c_2 > 0$. So: $$ \\begin{eqnarray} T(n) &\\geq& c_1(n - 1)\\lg{(n - 1)} + \\lg{n} \\end{eqnarray} $$ Let's try to reguess $T(n) \\geq c_1(n + 1)\\lg{(n + 1)}$. So: $$ \\begin{eqnarray} T(n) &\\geq& c_1(n - 1 + 1)\\lg{(n - 1 + 1)} + \\lg{n} \\\\ &=& c_1n\\lg{n} + \\lg{n} \\\\ &>& c_1n\\lg{n} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ So $T(n) = \\Omega(n\\lg{n})$. And: $$ \\begin{eqnarray} T(n) &\\leq& c_2(n - 1)\\lg{(n - 1)} + \\lg{n} \\\\ &<& c_2(n - 1)\\lg{n} + \\lg{n} \\\\ &=& \\lg{n}(c_2(n - 1) + 1) \\\\ &\\leq& c_2n\\lg{n} \\\\ &=& O(n\\lg{n}) \\end{eqnarray} $$ where the last step holds as long as $c_2 \\geq 1$. So $T(n) = O(n\\lg{n})$. Thus $T(n) = \\Theta(n\\lg{n})$.","title":"h"},{"location":"04-Divide-and-Conquer/Problems/#i","text":"First let's create a recursion tree for the recurrence $T(n) = T(n - 2) + \\frac{1}{\\lg{n}}$. The number of nodes at depth i is 1, for $i = 0, 1, 2, \\ldots, \\lg{(n - 1)} - 1$, has a cost of $\\frac{1}{\\lg{(n - 2^i)}}$. The bottom level, at depth $\\lg{(n - 1)}$, has 1 node, which contribution cost T(1), for a total cost of T(1), which is $\\Theta(1)$. So: $$ \\begin{eqnarray} T(n) &=& \\sum_{i = 0}^{\\lg{(n - 1)} - 1}\\frac{1}{\\lg{(n - 2^i)}} + \\Theta(1) \\end{eqnarray} $$ But I don't know how to compute the sum.","title":"i"},{"location":"04-Divide-and-Conquer/Problems/#j","text":"Let's solve it by the iterative method and assume n is an exact power of 2, and n is also a perfect square. $$ \\begin{eqnarray} T(n) &=& \\sqrt{n}T(\\sqrt{n}) + n \\\\ &=& n^{\\frac{1}{2}}T(n^{\\frac{1}{2}}) + n \\\\ &=& n^{\\frac{1}{2}}((n^{\\frac{1}{2}})^{\\frac{1}{2}}T((n^{\\frac{1}{2}})^{\\frac{1}{2}}) + n^{\\frac{1}{2}}) + n \\\\ &=& n^{\\frac{1}{2}}(n^{\\frac{1}{4}}T(n^{\\frac{1}{4}}) + n^{\\frac{1}{2}}) + n \\\\ &=& n^{\\frac{1}{2} + \\frac{1}{4}}T(n^{\\frac{1}{4}}) + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\ &=& n^{\\frac{1}{2} + \\frac{1}{4}}(n^{\\frac{1}{8}}T(n^{\\frac{1}{8}}) + n^{\\frac{1}{4}}) + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\ &=& n^{\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8}}T(n^{\\frac{1}{8}}) + n^{\\frac{1}{2} + \\frac{1}{4} + \\frac{1}{4}} + n^{\\frac{1}{2} + \\frac{1}{2}} + n \\\\ &=& \\ldots \\\\ &=& n^{\\sum_{i = 1}^{\\lg{\\sqrt{n}}}\\frac{1}{2^i}}T(2) + \\lg{\\sqrt{n}} * n \\\\ &<& n^{\\sum_{i = 1}^{\\infty}\\frac{1}{2^i}}T(2) + \\lg{\\sqrt{n}} * n \\\\ &=& n^{\\frac{\\frac{1}{2}}{1 - \\frac{1}{2}}}T(2) + \\frac{1}{2}n\\lg{n} \\\\ &=& nT(2) + \\frac{1}{2}n\\lg{n} \\\\ &=& O(n\\lg{n}) \\end{eqnarray} $$ Thus, we have derived a guess of $T(n) = O(n\\lg{n})$ for original recurrence. Now let's use the substitution method to verify that our guess was correct. We want to show that $T(n) \\leq cn\\lg{n}$ for some constant c > 0. So: $$ \\begin{eqnarray} T(n) &=& \\sqrt{n}T(\\sqrt{n}) + n \\\\ &\\leq& \\sqrt{n}c\\sqrt{n}\\lg{\\sqrt{n}} + n \\\\ &=& cn\\lg{\\sqrt{n}} + n \\\\ &=& \\frac{1}{2}cn\\lg{n} + n \\end{eqnarray} $$ Because $\\frac{1}{2}cn\\lg{n} + n - cn\\lg{n} = n(1 - \\frac{1}{2}c\\lg{n}) \\leq n(1 - \\frac{1}{2}c)$ when $n \\geq 2$. So $n(1 - \\frac{1}{2}c) \\leq 0$ when $c \\geq 2$. So $T(n) \\leq cn\\lg{n}$ when $c \\geq 2$ and $n \\geq 2$. Thus, $T(n) = O(n\\lg{n})$. But I don't know how to get the lower bound.","title":"j"},{"location":"04-Divide-and-Conquer/Problems/#4-4","text":"","title":"4-4"},{"location":"04-Divide-and-Conquer/Problems/#a_3","text":"$$ \\begin{eqnarray} z + z\\mathcal{F}(z) + z^2\\mathcal{F}(Z) &=& z + z\\sum_{i = 0}^{\\infty}F_iz^i + z^2\\sum_{i = 0}^{\\infty}F_iz^i \\\\ &=& z + (0 + z^2 + z^3 + 2z^4 + 3z^5 + \\ldots) + (0 + z^3 + z^4 + 2z^5 + \\ldots) \\\\ &=& 0 + z + z^2 + 2z^3 + 3z^4 + 5z^5 + \\ldots \\\\ &=& \\mathcal{F}(z) \\end{eqnarray} $$","title":"a"},{"location":"04-Divide-and-Conquer/Problems/#b_3","text":"Because $\\mathcal{F}(z) = z + z\\mathcal{F}(z) + z^2\\mathcal{F}(z)$, so $\\mathcal{F}(z) - z\\mathcal{F}(z) - z^2\\mathcal{F}(z) = z$, thus $\\mathcal{F}(z)(1 - z - z^2) = z$, so $\\mathcal{F}(z) = \\frac{z}{1 - z - z^2}$. And: $$ \\begin{eqnarray} \\frac{z}{(1 - \\phi{z})(1 - \\hat\\phi{z})} &=& \\frac{z}{1 - (\\phi + \\hat\\phi)z + \\phi\\hat\\phi{z^2}} \\\\ &=& \\frac{z}{1 - (\\frac{1 - \\sqrt{5}}{2} + \\frac{1 + \\sqrt{5}}{2})z + \\frac{1 - \\sqrt{5}}{2}\\frac{1 + \\sqrt{5}}{2}z^2} \\\\ &=& \\frac{z}{1 - z - z^2} \\end{eqnarray} $$ And: $$ \\begin{eqnarray} \\frac{1}{\\sqrt{5}}(\\frac{1}{1 - \\phi{z}} - \\frac{1}{1 - \\hat\\phi{z}}) &=& \\frac{1}{\\sqrt{5}}\\frac{1 - \\hat\\phi{z} - 1 + \\phi{z}}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ &=& \\frac{1}{\\sqrt{5}}\\frac{z(\\phi - \\hat\\phi)}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ &=& \\frac{1}{\\sqrt{5}}\\frac{z(\\frac{1 + \\sqrt{5}}{2} - \\frac{1 - \\sqrt{5}}{2})}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ &=& \\frac{1}{\\sqrt{5}}\\frac{z\\sqrt{5}}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\\\ &=& \\frac{z}{(1 - \\phi{z})(1 - \\hat\\phi{z})} \\end{eqnarray} $$","title":"b"},{"location":"04-Divide-and-Conquer/Problems/#c_2","text":"$$ \\begin{eqnarray} \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)z^i &=& \\frac{1}{\\sqrt{5}}\\sum_{i = 0}^{\\infty}(\\phi^iz^i - \\hat\\phi^iz^i) \\\\ &=& \\frac{1}{\\sqrt{5}}(\\sum_{i = 0}^{\\infty}\\phi^iz^i - \\sum_{i = 0}^{\\infty}\\hat\\phi^iz^i) \\\\ &=& \\frac{1}{\\sqrt{5}}(\\frac{1}{1 - \\phi{z}} - \\frac{1}{1 - \\hat\\phi{z}}) \\\\ &=& \\mathcal{F}(z) \\end{eqnarray} $$","title":"c"},{"location":"04-Divide-and-Conquer/Problems/#d_2","text":"Because $\\mathcal{F}(z) = \\sum_{i = 0}^{\\infty}F_iz^i$ and $\\mathcal{F}(z) = \\sum_{i = 0}^{\\infty}\\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)z^i$, so we have $F_i = \\frac{1}{\\sqrt{5}}(\\phi^i - \\hat\\phi^i)$. Since $|\\hat\\phi| < 1$, we have $\\frac{|\\hat\\phi^i|}{\\sqrt{5}} < \\frac{1}{\\sqrt{5}} < \\frac{1}{2}$, which implies that $F_i = \\lfloor \\frac{\\phi^i}{\\sqrt{5}} + \\frac{1}{2} \\rfloor$, which is to say that the ith Fibonacci number $F_i$ is equal to $\\frac{\\phi^i}{\\sqrt{5}}$ rounded to the nearest integer.","title":"d"},{"location":"04-Divide-and-Conquer/Problems/#4-5","text":"","title":"4-5"},{"location":"04-Divide-and-Conquer/Problems/#a_4","text":"","title":"a"},{"location":"04-Divide-and-Conquer/Problems/#4-6","text":"","title":"4-6"},{"location":"04-Divide-and-Conquer/Problems/#a_5","text":"The \"only if\" part is easy. If an array is Monge, then for all i, j, k and l such that $l \\leq i < k \\leq m$ and $l \\leq j < l \\leq n$ we have $A[i, j] + A[k, l] \\leq A[i, l] + A[k, j]$. So we let k = i + 1, l = j + 1, then we have $A[i, j] + A[i + 1, j + 1] \\leq A[i, j + 1] + A[i + 1, j]$. So we proved the \"only if\" part. Then let's prove the \"if\" part, we'll use induction separately on rows and columns. Let's get on columns first. The base case is already given, let's assume $A[i, j] + A[i + 1, j + k] \\leq A[i, j + k] + A[i + 1, j]$ for $k \\geq 1$ and $j + k \\leq n$. Then we need to prove $A[i, j] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j]$. According to the definition, we have $A[i, j + k] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j + k]$, thus: $$\\begin{aligned} A[i, j] + A[i + 1, j + k] + A[i, j + k] + A[i + 1, j + k + 1] \\leq & A[i, j + k] + A[i + 1, j] + \\\\ & A[i, j + k + 1] + A[i + 1, j + k] \\end{aligned}$$ So we get $A[i, j] + A[i + 1, j + k + 1] \\leq A[i, j + k + 1] + A[i + 1, j]$. So the induction is correct, and similarly we can prove: $$ \\begin{eqnarray} A[i, j + 1] + A[i + 1, j + k] &\\leq& A[i, j + k] + A[i + 1, j + 1] \\, (j + k \\leq n), \\ldots, A[i, j + p] + A[i + 1, j + k] \\\\ &\\leq& A[i, j + k] + A[i + 1, j + p] \\, (j + p \\leq j + k - 1, j + k \\leq n) \\end{eqnarray} $$ thus we can see that all adjacent rows are Monge arrays. Similarly, we can use the induction on rows, so all adjacent columns are Monge arrays. For any $m_1 x n_1$ array, if $m_1 = 2$ or $n_1 = 2$, then the subarray is a Monge array. And we need to prove the subarray is also a Monge array if $m_1 \\geq 3$ and $n_1 \\geq 3$. Let's assume the subarray starts at row $A[i_1, j_1]$ and ends at $A[i_2, j_2]$. Since any adjacent columns are Monge arrays, so we have: $$A[i_1, j_1] + A[i_2, j_1 + 1] \\leq A[i_1, j_1 + 1] + A[i_2, j_1]$$ $$A[i_1, j_1 + 1] + A[i_2, j_1 + 2] \\leq A[i_1, j_1 + 2] + A[i_2, j_1 + 1]$$ $$\\ldots$$ $$A[i_1, j_2 - 1] + A[i_2, j_2] \\leq A[i_1, j_2] + A[i_2, j_2 - 1]$$ Then we sum all inequations together, so we have: $$A[i_1, j_1] + A[i_2, j_2] \\leq A[i_1, j_2] + A[i_2, j_1]$$, which means the subarray $A[i_1, j_1]$ to $A[i_2, j_2]$ is a Monge array when $i_2 - i_1 \\geq 3$ and $j_2 - j_1 \\geq 3$. So for all subarrays in A, they are Monge arrays. So we proved the \"if\" part.","title":"a"},{"location":"04-Divide-and-Conquer/Problems/#b_4","text":"\\begin{matrix} 37 & 23 & 22 & 32 \\\\ 21 & 6 & 5 & 10 \\\\ 53 & 34 & 30 & 31 \\\\ 32 & 13 & 9 & 6 \\\\ 43 & 21 & 15 & 8 \\\\ \\end{matrix}","title":"b"},{"location":"04-Divide-and-Conquer/Problems/#c_3","text":"Let's consider row i and row i + 1. Let $f(i) = j_1$ and $f(i + 1) = j_2$ and assume $f(i) > f(i + 1)$, so we have $j_1 > j_2$. Since A is a Monge array, so we have $A[i, j_2] + A[i + 1, j_1] \\leq A[i, j_1] + A[i + 1, j_2]$. But according to the definition of f(i), we have $A[i, j_2] > A[i, j_1]$ and $A[i + 1, j_2] < A[i + 1, j_1]$, combine them together: $A[i, j_2] + A[i + 1, j_1] > A[i, j_1] + A[i + 1, j_2]$. Thus the assumption is wrong. So $f(i) \\leq f(i + 1)$. So $f(1) \\leq f(2) \\leq \\ldots \\leq f(m)$ for any m x n Monge array.","title":"c"},{"location":"04-Divide-and-Conquer/Problems/#d_3","text":"Let's assume $m = 2k, k = 1, 2, \\ldots$, from the previous question we have $f(2k - 2) \\leq f(2k - 1) \\leq f(2k)$. We want to get f(2k - 1) when f(2k - 2) and f(2k) are already known. We only need to compare the numbers from A[2k - 1][f(2k - 2)] to A[2k - 1][f(2k)], which costs at most $O(f(2k) - f(2k - 2) + 1)$. Thus: $$ \\begin{eqnarray} T &=& \\sum_{k = 1}^{\\frac{m}{2}}O(f(2k) - f(2k - 2) + 1) \\\\ &=& (f(2) - f(0) + 1) + (f(4) - f(2) + 1) + \\ldots + (f(m) - f(m - 2) + 1) \\\\ &=& f(m) - f(0) + \\frac{m}{2} \\\\ &\\leq& n - 0 + \\frac{m}{2} \\\\ &<& n + m \\\\ &=& O(m + n) \\end{eqnarray} $$","title":"d"},{"location":"04-Divide-and-Conquer/Problems/#e_2","text":"$$ \\begin{eqnarray} T(m) &=& T(\\frac{m}{2}) + O(m + n) \\\\ &=& T(\\frac{m}{4}) + O(\\frac{m}{2} + n) + O(m + n) \\\\ &=& T(\\frac{m}{8}) + O(\\frac{m}{4} + n) + O(\\frac{m}{2} + n) + O(m + n) \\\\ &=& \\ldots \\\\ &=& T(1) + \\sum_{i = 0}^{\\lg{m} - 1}O(\\frac{m}{2^i}) + \\sum_{i = 0}^{\\lg{m} - 1}O(n) \\\\ &<& T(1) + \\sum_{i = 0}^{\\infty}O(\\frac{m}{2^i}) + \\sum_{i = 0}^{\\lg{m} - 1}O(n) \\\\ &=& T(1) + O(m\\frac{1}{1 - \\frac{1}{2}}) + O(n\\lg{m}) \\\\ &=& O(1) + O(2m) + O(n\\lg{m}) \\\\ &=& O(m + n\\lg{m}) \\end{eqnarray} $$ def leftmost_min_element_in_each_row_of_monge_array(matrix): # Column index of leftmost min element in each row leftmost_min_element_in_each_row = [0] * len(matrix) leftmost_min_element_divide_and_conquer( matrix, leftmost_min_element_in_each_row, 0, len(matrix) - 1, 1) return leftmost_min_element_in_each_row def leftmost_min_element_divide_and_conquer( matrix, leftmost_min_element_in_each_row, row_start, row_end, step): if row_start == row_end: leftmost_min_element_in_each_row[row_start] = \\ find_leftmost_min_element_in_row( matrix, row_start, 0, len(matrix[row_start]) - 1) else: # Construct a submatrix consisting of the even-numbered rows sub_matrix_row_start = row_start + step sub_matrix_row_end = 0 if row_end % 2 == 0 or ((row_end - row_start) // step) % 2 == 0: sub_matrix_row_end = row_end - step elif ((row_end - row_start) // step) % 2 == 1: sub_matrix_row_end = row_end leftmost_min_element_divide_and_conquer( matrix, leftmost_min_element_in_each_row, sub_matrix_row_start, sub_matrix_row_end, step * 2) leftmost_min_element_in_odd_numbered_rows( matrix, leftmost_min_element_in_each_row, row_start, row_end, step) def find_leftmost_min_element_in_row(matrix, row, column_start, column_end): min_column_index = 0 for column in range(column_start, column_end + 1): if matrix[row][column] < matrix[row][min_column_index]: min_column_index = column return min_column_index def leftmost_min_element_in_odd_numbered_rows( matrix, leftmost_min_element_in_each_row, row_start, row_end, step): for odd_numbered_row in range(row_start, row_end + 1, step * 2): prev_even_numbered_row = odd_numbered_row - step next_even_numbered_row = odd_numbered_row + step column_start = -1 column_end = -1 if prev_even_numbered_row >= row_start and \\ next_even_numbered_row <= row_end: column_start = \\ leftmost_min_element_in_each_row[prev_even_numbered_row] column_end = \\ leftmost_min_element_in_each_row[next_even_numbered_row] elif prev_even_numbered_row >= row_start: column_start = \\ leftmost_min_element_in_each_row[prev_even_numbered_row] column_end = len(matrix[0]) - 1 elif next_even_numbered_row <= row_end: column_start = 0 column_end = \\ leftmost_min_element_in_each_row[next_even_numbered_row] leftmost_min_element_in_each_row[odd_numbered_row] = \\ find_leftmost_min_element_in_row( matrix, odd_numbered_row, column_start, column_end)","title":"e"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/","text":"5.1 The hiring problem 5.1-1 Because we are always able to determine which candidate is best, that means we can compare any two candidates and know which is better. Thus we are able to sort the candidates based on this knowledge, which implies that we know a total order on the ranks of the candidates. 5.1-2 First we find the smallest number p , such that 2^p > b - a . Then call RANDOM(0, 1) to generate p bits, thus we have a random number r . If r + a <= b , then that's what we want, otherwise we regenerate r . ( Link 1 , link 2 ) import math import random def random_number(a, b): bits = math.ceil(math.log2(b - a + 1)) while True: number = random_binay(bits) if a + number <= b: return a + number def random_binay(bits): number = 0 for i in range(bits): number = number * 2 + random.randint(0, 1) return number The running time of function random_binay is $O(\\lg{(b - a)})$. And it's possible that we need to call random_binay multiple times if a + number > b , but that doesn't affect the expected running time, since $cO(\\lg{(b - a)})$ is still $O((\\lg{(b - a)}))$. Thus the expected running time is $O(\\lg{(b - a)})$. 5.1-3 We can call BIASED-RANDOM twice to get two numbers x and y . The results would be 0, 0 , '1, 0', '0, 1', '1, 1', with probability (1 - p)(1 - p) , p(1 - p) , (1 - p)p , pp respectively. Since p(1 - p) = (1 - p)p , we can treat one as 0 and the other as 1 . So we can generate 0 with probability 1/2 and 1 with probability 1/2. The probability to generate 0, 1 and 1, 0 is $2p(1 - p)$, so we have to try $\\frac{1}{2p(1 - p)}$ times. Thus the expected running time is $O(\\frac{1}{p(1 - p)})$. def unbiased_random(): while True: x = BIASED-RANDOM() y = BIASED-RANDOM() if x != y: return x","title":"5.1 The hiring problem"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/#51-the-hiring-problem","text":"","title":"5.1 The hiring problem"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/#51-1","text":"Because we are always able to determine which candidate is best, that means we can compare any two candidates and know which is better. Thus we are able to sort the candidates based on this knowledge, which implies that we know a total order on the ranks of the candidates.","title":"5.1-1"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/#51-2","text":"First we find the smallest number p , such that 2^p > b - a . Then call RANDOM(0, 1) to generate p bits, thus we have a random number r . If r + a <= b , then that's what we want, otherwise we regenerate r . ( Link 1 , link 2 ) import math import random def random_number(a, b): bits = math.ceil(math.log2(b - a + 1)) while True: number = random_binay(bits) if a + number <= b: return a + number def random_binay(bits): number = 0 for i in range(bits): number = number * 2 + random.randint(0, 1) return number The running time of function random_binay is $O(\\lg{(b - a)})$. And it's possible that we need to call random_binay multiple times if a + number > b , but that doesn't affect the expected running time, since $cO(\\lg{(b - a)})$ is still $O((\\lg{(b - a)}))$. Thus the expected running time is $O(\\lg{(b - a)})$.","title":"5.1-2"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.1-The-hiring-problem/#51-3","text":"We can call BIASED-RANDOM twice to get two numbers x and y . The results would be 0, 0 , '1, 0', '0, 1', '1, 1', with probability (1 - p)(1 - p) , p(1 - p) , (1 - p)p , pp respectively. Since p(1 - p) = (1 - p)p , we can treat one as 0 and the other as 1 . So we can generate 0 with probability 1/2 and 1 with probability 1/2. The probability to generate 0, 1 and 1, 0 is $2p(1 - p)$, so we have to try $\\frac{1}{2p(1 - p)}$ times. Thus the expected running time is $O(\\frac{1}{p(1 - p)})$. def unbiased_random(): while True: x = BIASED-RANDOM() y = BIASED-RANDOM() if x != y: return x","title":"5.1-3"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/","text":"5.2 Indicator random variables 5.2-1 We hire exactly one time when the first candidate is the best. So the first candidate has a probability of $\\frac{1}{n}$ of being better qualified than other n - 1 candidates. Thus the probability of hiring exactly one time is $\\frac{1}{n}$. We hire n times when each candidate is better than the previous candidate. In each turn, the candidate i has a probability of $\\frac{1}{i}$ of being better than the previous i - 1 candidates. Thus the probability is $1 * \\frac{1}{2} * \\frac{1}{3} * \\ldots * \\frac{1}{n} = \\frac{1}{n!}$. 5.2-2 We hire exactly twice when the first candidate is not the best candidate, and the best candidate comes before all the candidates who are better than the first candidate. Let's assume the first candidate has a rank k among the candidates (k < n, bigger is better). So there are n - k candidates that are better than the first candidate, including the best candidate. So the best candidate could only appear through position 2 to k + 1, which are k + 1 - 2 + 1 = k positions. And the best candidate has a probability of $\\frac{1}{k}$ to apppear in those k positions. Because each candidate has a probability of $\\frac{1}{n}$ to appear in the first position, thus the probability of hiring twice is $\\frac{1}{n}\\frac{1}{1} + \\frac{1}{n}\\frac{1}{2} + \\frac{1}{n}\\frac{1}{3} \\ldots \\frac{1}{n}\\frac{1}{n - 1} = \\frac{1}{n}\\sum_{k = 1}^{n - 1}\\frac{1}{k} = \\frac{\\ln{(n - 1)} + O(1)}{n}$. 5.2-3 Let's define $X_i$ be the indicator random variable indicates the value of ith dice. Thus, $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{the number of dice is k}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if k = 1} \\\\ 2, & \\text{if k = 2} \\\\ 3, & \\text{if k = 3} \\\\ 4, & \\text{if k = 4} \\\\ 5, & \\text{if k = 5} \\\\ 6, & \\text{if k = 6} \\\\ \\end{cases} \\end{eqnarray} $$ and $X = X_1 + X_2 + \\ldots + X_n$. Thus the expected value in one dice is: $$ \\begin{eqnarray} E[X_i] &=& E[I\\lbrace \\text{the number of dice is k} \\rbrace] \\\\ &=& \\sum_{k = 1}^6 kPr\\lbrace X_i = k \\rbrace \\\\ &=& \\frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) \\\\ &=& 3.5 \\end{eqnarray} $$ Thus, the expected value of the sum of n dice is: $$ \\begin{eqnarray} E[X] &=& E\\big[\\sum_{i = 1}^n X_i\\big] \\\\ &=& \\sum_{i = 1}^nE\\big[X_i\\big] \\\\ &=& \\sum_{i = 1}^n 3.5 \\\\ &=& 3.5n \\end{eqnarray} $$ 5.2-4 Each person gets his own hat with probability $\\frac{1}{n}$. Let's define $X_i$ be the indicator random variable associated with the event in which the ith customer gets his own hat. Thus, $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{customer i gets his own hat}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if customer i gets his own hat} \\\\ 0, & \\text{if customer i doesn't get his own hat} \\end{cases} \\end{eqnarray} $$ and $X = X_1 + X_2 + \\ldots + X_n$. Thus the expected number of customers who get back their own hat is: $$ \\begin{eqnarray} E[X] &=& E\\big[\\sum_{i = 1}^n X_i\\big] \\\\ &=& \\sum_{i = 1}^nE\\big[X_i\\big] \\\\ &=& \\sum_{i = 1}^n \\frac{1}{n} * 1 \\\\ &=& 1 \\end{eqnarray} $$ 5.2-5 Let's define $X_{ij}$ be the the indicator random variable associated with the event that if i < j, then A[i] > A[j]. Thus, $$ \\begin{eqnarray} X_{ij} &=& I\\lbrace A[i] > A[j]\\rbrace \\\\ &=& \\begin{cases} 1, & A[i] > A[j] \\\\ 0, & A[i] \\leq A[j] \\end{cases} \\end{eqnarray} $$ and $Pr\\lbrace A[i] > A[j] \\rbrace = Pr\\lbrace A[i] \\leq A[j] \\rbrace = \\frac{1}{2}$. Thus the expected number of inversions is: $$ \\begin{eqnarray} E[X] &=& E\\big[\\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n X_{ij}\\big] \\\\ &=& \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n E\\big[X_{ij}\\big] \\\\ &=& \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n (\\frac{1}{2} * 1 + 0) \\\\ &=& \\frac{1}{2}\\sum_{i = 1}^{n - 1} (n - i) \\\\ &=& \\frac{1}{2}\\sum_{i = 1}^{n - 1} i \\\\ &=& \\frac{1}{2}\\frac{(1 + n - 1)(n - 1)}{2} \\\\ &=& \\frac{n(n - 1)}{4} \\end{eqnarray} $$","title":"5.2 Indicator random variables"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-indicator-random-variables","text":"","title":"5.2 Indicator random variables"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-1","text":"We hire exactly one time when the first candidate is the best. So the first candidate has a probability of $\\frac{1}{n}$ of being better qualified than other n - 1 candidates. Thus the probability of hiring exactly one time is $\\frac{1}{n}$. We hire n times when each candidate is better than the previous candidate. In each turn, the candidate i has a probability of $\\frac{1}{i}$ of being better than the previous i - 1 candidates. Thus the probability is $1 * \\frac{1}{2} * \\frac{1}{3} * \\ldots * \\frac{1}{n} = \\frac{1}{n!}$.","title":"5.2-1"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-2","text":"We hire exactly twice when the first candidate is not the best candidate, and the best candidate comes before all the candidates who are better than the first candidate. Let's assume the first candidate has a rank k among the candidates (k < n, bigger is better). So there are n - k candidates that are better than the first candidate, including the best candidate. So the best candidate could only appear through position 2 to k + 1, which are k + 1 - 2 + 1 = k positions. And the best candidate has a probability of $\\frac{1}{k}$ to apppear in those k positions. Because each candidate has a probability of $\\frac{1}{n}$ to appear in the first position, thus the probability of hiring twice is $\\frac{1}{n}\\frac{1}{1} + \\frac{1}{n}\\frac{1}{2} + \\frac{1}{n}\\frac{1}{3} \\ldots \\frac{1}{n}\\frac{1}{n - 1} = \\frac{1}{n}\\sum_{k = 1}^{n - 1}\\frac{1}{k} = \\frac{\\ln{(n - 1)} + O(1)}{n}$.","title":"5.2-2"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-3","text":"Let's define $X_i$ be the indicator random variable indicates the value of ith dice. Thus, $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{the number of dice is k}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if k = 1} \\\\ 2, & \\text{if k = 2} \\\\ 3, & \\text{if k = 3} \\\\ 4, & \\text{if k = 4} \\\\ 5, & \\text{if k = 5} \\\\ 6, & \\text{if k = 6} \\\\ \\end{cases} \\end{eqnarray} $$ and $X = X_1 + X_2 + \\ldots + X_n$. Thus the expected value in one dice is: $$ \\begin{eqnarray} E[X_i] &=& E[I\\lbrace \\text{the number of dice is k} \\rbrace] \\\\ &=& \\sum_{k = 1}^6 kPr\\lbrace X_i = k \\rbrace \\\\ &=& \\frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) \\\\ &=& 3.5 \\end{eqnarray} $$ Thus, the expected value of the sum of n dice is: $$ \\begin{eqnarray} E[X] &=& E\\big[\\sum_{i = 1}^n X_i\\big] \\\\ &=& \\sum_{i = 1}^nE\\big[X_i\\big] \\\\ &=& \\sum_{i = 1}^n 3.5 \\\\ &=& 3.5n \\end{eqnarray} $$","title":"5.2-3"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-4","text":"Each person gets his own hat with probability $\\frac{1}{n}$. Let's define $X_i$ be the indicator random variable associated with the event in which the ith customer gets his own hat. Thus, $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{customer i gets his own hat}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if customer i gets his own hat} \\\\ 0, & \\text{if customer i doesn't get his own hat} \\end{cases} \\end{eqnarray} $$ and $X = X_1 + X_2 + \\ldots + X_n$. Thus the expected number of customers who get back their own hat is: $$ \\begin{eqnarray} E[X] &=& E\\big[\\sum_{i = 1}^n X_i\\big] \\\\ &=& \\sum_{i = 1}^nE\\big[X_i\\big] \\\\ &=& \\sum_{i = 1}^n \\frac{1}{n} * 1 \\\\ &=& 1 \\end{eqnarray} $$","title":"5.2-4"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.2-Indicator-random-variables/#52-5","text":"Let's define $X_{ij}$ be the the indicator random variable associated with the event that if i < j, then A[i] > A[j]. Thus, $$ \\begin{eqnarray} X_{ij} &=& I\\lbrace A[i] > A[j]\\rbrace \\\\ &=& \\begin{cases} 1, & A[i] > A[j] \\\\ 0, & A[i] \\leq A[j] \\end{cases} \\end{eqnarray} $$ and $Pr\\lbrace A[i] > A[j] \\rbrace = Pr\\lbrace A[i] \\leq A[j] \\rbrace = \\frac{1}{2}$. Thus the expected number of inversions is: $$ \\begin{eqnarray} E[X] &=& E\\big[\\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n X_{ij}\\big] \\\\ &=& \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n E\\big[X_{ij}\\big] \\\\ &=& \\sum_{i = 1}^{n - 1} \\sum_{j = i + 1}^n (\\frac{1}{2} * 1 + 0) \\\\ &=& \\frac{1}{2}\\sum_{i = 1}^{n - 1} (n - i) \\\\ &=& \\frac{1}{2}\\sum_{i = 1}^{n - 1} i \\\\ &=& \\frac{1}{2}\\frac{(1 + n - 1)(n - 1)}{2} \\\\ &=& \\frac{n(n - 1)}{4} \\end{eqnarray} $$","title":"5.2-5"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/","text":"5.3 Randomized algorithms 5.3-1 We can randomly swap the first element first, then start the for loop from the second element. RANDOMIZE-IN-PLACE(A) n = A.length swap A[1] with A[RANDOM(1, n)] for i = 2 to n swap A[i] with A[RANDOM(i, n)] And here is the initialization step: before the first loop iteration, i = 2. The loop invariant says that for each possible 1-permutation, the subarray A[1..1] contains this 1-permutation with probability (n - i + 1)!/n! = (n - 1)!/n! = 1/n. The subarray A[1..1] contains only one element, contains 1-permutation with probability 1/n, so the loop invariant holds prior to the first iteration. The maintenance and termination steps remain the same. 5.3-2 No, it cannot get any permutation, for example, the identity permutation. 5.3-3 This method can produce $n^n$ permutations, but there are only at most n! permutations. So some permutations have duplicates. If each permutation has same number of duplicates, which means $n^n$ is divisible by n!, then the code can produce a uniform random permutation, otherwise, it can not. Let's consider n - 1, it's obvious that n - 1 is a divisor of n!. Then lt's prove n - 1 is not a divisor of $n^n$ for n > 2. To make it simple, let's compare $(n + 1)^{n + 1}$ and n for n > 1. We can write $(n + 1)^{n + 1}$ as $a_{n + 1}n^{n + 1} + a_nn^n + \\ldots + a_1n + 1$. Thus $\\frac{(n + 1)^{n + 1}}{n} = a_{n + 1}n^n + a_nn^{n - 1} + \\ldots + a_1 + \\frac{1}{n}$, which is not an integer when n > 1. So $(n + 1)^{n + 1}$ is not divisible by n when n > 1, thus, $n^n$ is not divisible by n - 1 when n > 2. So $n^n$ is not divisible by n!, otherwise, n - 1 must also be a divisor of $n^n$. Thus, the code cannot produce a uniform random permutation. 5.3-4 The code shifts each element in A by k positions by cyclic, k ranges from 1 to n, thus each element A[i] has a $\\frac{1}{n}$ probability of winding up in any particular position in B. This is not a uniformly random because it cannot generate all possible permutations, it can only generate n permutations. 5.3-5 There are $(n^3)^n$ permutations, and there are $A_{n^3}^{n}$ permutations that all elements are unique. Thus, the probability that all elements are unique is: $$ \\begin{eqnarray} P &=& \\frac{A_{n^3}^{n}}{(n^3)^n} \\\\ &=& \\frac{n^3(n^3 - 1)(n^3 - 2)\\ldots(n^3 - (n - 1))}{(n^3)^n} \\\\ &=& 1\\frac{n^3 - 1}{n^3}\\frac{n^3 - 2}{n^3}\\ldots\\frac{n^3 - (n - 1)}{n^3} \\\\ &=& (1 - \\frac{1}{n^3})(1 - \\frac{2}{n^3})\\ldots(1 - \\frac{n - 1}{n^3}) \\\\ &\\geq& (1 - \\frac{n}{n^3})(1 - \\frac{n}{n^3})\\ldots(1 - \\frac{n}{n^3}) \\\\ &=& (1 - \\frac{n}{n^3})^{n - 1} \\\\ &=& (1 - \\frac{1}{n^2})^{n - 1} \\\\ &=& 1 + (n - 1)(-\\frac{1}{n^2}) + \\frac{(n - 1)(n - 2)}{2}(-\\frac{1}{n^2})^2 + \\frac{(n - 1)(n - 2)(n - 3)}{3!}(-\\frac{1}{n^2})^3 + \\ldots \\\\ &=& 1 - \\frac{n - 1}{n^2} + O(\\frac{1}{n^2}) - O(\\frac{1}{n^3}) + \\ldots \\\\ &>& 1 - \\frac{n - 1}{n^2} \\\\ &>& 1 - \\frac{n}{n^2} \\\\ &=& 1 - \\frac{1}{n} \\end{eqnarray} $$ 5.3-6 Regenerate the priorities array, until all elements are unique. 5.3-7","title":"5.3 Randomized algorithms"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-randomized-algorithms","text":"","title":"5.3 Randomized algorithms"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-1","text":"We can randomly swap the first element first, then start the for loop from the second element. RANDOMIZE-IN-PLACE(A) n = A.length swap A[1] with A[RANDOM(1, n)] for i = 2 to n swap A[i] with A[RANDOM(i, n)] And here is the initialization step: before the first loop iteration, i = 2. The loop invariant says that for each possible 1-permutation, the subarray A[1..1] contains this 1-permutation with probability (n - i + 1)!/n! = (n - 1)!/n! = 1/n. The subarray A[1..1] contains only one element, contains 1-permutation with probability 1/n, so the loop invariant holds prior to the first iteration. The maintenance and termination steps remain the same.","title":"5.3-1"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-2","text":"No, it cannot get any permutation, for example, the identity permutation.","title":"5.3-2"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-3","text":"This method can produce $n^n$ permutations, but there are only at most n! permutations. So some permutations have duplicates. If each permutation has same number of duplicates, which means $n^n$ is divisible by n!, then the code can produce a uniform random permutation, otherwise, it can not. Let's consider n - 1, it's obvious that n - 1 is a divisor of n!. Then lt's prove n - 1 is not a divisor of $n^n$ for n > 2. To make it simple, let's compare $(n + 1)^{n + 1}$ and n for n > 1. We can write $(n + 1)^{n + 1}$ as $a_{n + 1}n^{n + 1} + a_nn^n + \\ldots + a_1n + 1$. Thus $\\frac{(n + 1)^{n + 1}}{n} = a_{n + 1}n^n + a_nn^{n - 1} + \\ldots + a_1 + \\frac{1}{n}$, which is not an integer when n > 1. So $(n + 1)^{n + 1}$ is not divisible by n when n > 1, thus, $n^n$ is not divisible by n - 1 when n > 2. So $n^n$ is not divisible by n!, otherwise, n - 1 must also be a divisor of $n^n$. Thus, the code cannot produce a uniform random permutation.","title":"5.3-3"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-4","text":"The code shifts each element in A by k positions by cyclic, k ranges from 1 to n, thus each element A[i] has a $\\frac{1}{n}$ probability of winding up in any particular position in B. This is not a uniformly random because it cannot generate all possible permutations, it can only generate n permutations.","title":"5.3-4"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-5","text":"There are $(n^3)^n$ permutations, and there are $A_{n^3}^{n}$ permutations that all elements are unique. Thus, the probability that all elements are unique is: $$ \\begin{eqnarray} P &=& \\frac{A_{n^3}^{n}}{(n^3)^n} \\\\ &=& \\frac{n^3(n^3 - 1)(n^3 - 2)\\ldots(n^3 - (n - 1))}{(n^3)^n} \\\\ &=& 1\\frac{n^3 - 1}{n^3}\\frac{n^3 - 2}{n^3}\\ldots\\frac{n^3 - (n - 1)}{n^3} \\\\ &=& (1 - \\frac{1}{n^3})(1 - \\frac{2}{n^3})\\ldots(1 - \\frac{n - 1}{n^3}) \\\\ &\\geq& (1 - \\frac{n}{n^3})(1 - \\frac{n}{n^3})\\ldots(1 - \\frac{n}{n^3}) \\\\ &=& (1 - \\frac{n}{n^3})^{n - 1} \\\\ &=& (1 - \\frac{1}{n^2})^{n - 1} \\\\ &=& 1 + (n - 1)(-\\frac{1}{n^2}) + \\frac{(n - 1)(n - 2)}{2}(-\\frac{1}{n^2})^2 + \\frac{(n - 1)(n - 2)(n - 3)}{3!}(-\\frac{1}{n^2})^3 + \\ldots \\\\ &=& 1 - \\frac{n - 1}{n^2} + O(\\frac{1}{n^2}) - O(\\frac{1}{n^3}) + \\ldots \\\\ &>& 1 - \\frac{n - 1}{n^2} \\\\ &>& 1 - \\frac{n}{n^2} \\\\ &=& 1 - \\frac{1}{n} \\end{eqnarray} $$","title":"5.3-5"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-6","text":"Regenerate the priorities array, until all elements are unique.","title":"5.3-6"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.3-Randomized-algorithms/#53-7","text":"","title":"5.3-7"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/","text":"5.4 Probabilistic analysis and further uses of indicator random variables 5.4-1 For each people, the probability that he doesn't have the same birthday as me is $\\frac{n - 1}{n}$, for k people that all don't have the same birthday as me is $(\\frac{n - 1}{n})^k$, thus the probability that at least one people that has the same birthday as me is $1 - (\\frac{n - 1}{n})^k$. Let $1 - (\\frac{n - 1}{n})^k \\geq \\frac{1}{2}$, and we have $k \\geq 253$. The probability that only one people has a birthday on July 4 is $k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1}$, and the probability that no one has a birthday on July 4 is $(\\frac{n - 1}{n})^k$, thus the probability that at least two people have a birthday on July 4 is $1 - k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1} - (\\frac{n - 1}{n})^k$, let $1 - k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1} - (\\frac{n - 1}{n})^k \\geq \\frac{1}{2}$, we have $k \\geq 613$. 5.4-2 The expected number of ball tosses is $1 + \\sum_{k = 1}^{b} \\frac{b!}{(b - k)!b^k}$, it's a birthday problem . 5.4-3 The equation shows that $E[X] = \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}E[X_{ij}]$, so pairwise indenpendence is sufficient. 5.4-4 For each pair (i, j, r) of the k people in the room, we define the indicator random variable $X_{ijr}$, for $1 \\leq i < j < r \\leq k$, by: $$ \\begin{eqnarray} X_{ijr} &=& I\\lbrace\\text{person i, person j and person r have the same birthday}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if person i, person j and person r have the same birthday} \\\\ 0, & \\text{otherwise} \\end{cases} \\end{eqnarray} $$ The probability that i's birthday, j's birthday and r's birthday all fall on day s is $Pr\\lbrace b_i = s \\text{ and } b_j = s \\text{ and } b_r = s \\rbrace = Pr\\lbrace b_i = s \\rbrace Pr\\lbrace b_j = s \\rbrace Pr\\lbrace b_r = s \\rbrace = \\frac{1}{n^3}$. Thus, the probability that they all fall on the same day is: $$ \\begin{eqnarray} Pr\\lbrace b_i = b_j = b_r \\rbrace &=& \\sum_{s = 1}^{n}Pr\\lbrace b_i = s \\text{ and } b_j = s \\text{ and } b_r = s \\rbrace \\\\ &=& \\sum_{s = 1}^{n}\\frac{1}{n^3} \\\\ &=& \\frac{1}{n^2} \\end{eqnarray} $$ Thus, $E[X_{ijr}] = Pr\\lbrace\\text{person i, person j and person r have the same birthday}\\rbrace = \\frac{1}{n^2}$. Letting X be the random variable that counts the number of pairs of inviduals having the same birthday, we have $X = \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}X_{ijr}$. Taking expectations of both sides and applying linearity of expectation, we obtain: $$ \\begin{eqnarray} E[X] &=& E[\\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}X_{ijr}] \\\\ &=& \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}E[X_{ijr}] \\\\ &=& \\binom{k}{3}\\frac{1}{n^2} \\\\ &=& \\frac{k(k - 1)(k - 2)}{6n^2} \\end{eqnarray} $$ Let $\\frac{k(k - 1)(k - 2)}{6n^2} \\geq 1$, we have $k \\geq 94$. 5.4-5 $P = \\frac{A_n^k}{n^k}$. It can be described by the birthday paradox that k people in the romm have unique birthday. 5.4-6 Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by: $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{bin i is empty}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if bin i is empty} \\\\ 0, & \\text{otherwise} \\end{cases} \\end{eqnarray} $$ We have $E[X_i] = Pr\\lbrace \\text{bin i is empty} \\rbrace = (\\frac{n - 1}{n})^n$ (toss n balls into n - 1 bins). Let X be the random variable that counts the number of bins that are empty, we have $X = \\sum_{i = 1}^{n}X_i$. So: $$ \\begin{eqnarray} E[X] &=& E[\\sum_{i = 1}^{n}X_i] \\\\ &=& \\sum_{i = 1}^{n}E[X_i] \\\\ &=& \\sum_{i = 1}^{n}(\\frac{n - 1}{n})^n \\\\ &=& n(\\frac{n - 1}{n})^n \\\\ &=& n(1 - \\frac{1}{n})^n \\\\ \\end{eqnarray} $$ Then let's check another question. Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by: $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{bin i has exactly one ball}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if bin i is has exactly one ball} \\\\ 0, & \\text{otherwise} \\end{cases} \\end{eqnarray} $$ We have $E[X_i] = Pr\\lbrace \\text{bin i has exactly one ball} \\rbrace = n\\frac{1}{n}(\\frac{n - 1}{n})^{n - 1} = (1 - \\frac{1}{n})^{n - 1}$ (toss one ball into one bin, then toss n - 1 balls into n - 1 bins). Let X be the random variable that counts the number of bins that are empty, we have $X = \\sum_{i = 1}^{n}X_i$. So: $$ \\begin{eqnarray} E[X] &=& E[\\sum_{i = 1}^{n}X_i] \\\\ &=& \\sum_{i = 1}^{n}E[X_i] \\\\ &=& \\sum_{i = 1}^{n}(1 - \\frac{1}{n})^{n - 1} \\\\ &=& n(1 - \\frac{1}{n})^{n - 1} \\end{eqnarray} $$ 5.4-7","title":"5.4 Probabilistic analysis and further uses of indicator random variables"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-probabilistic-analysis-and-further-uses-of-indicator-random-variables","text":"","title":"5.4 Probabilistic analysis and further uses of indicator random variables"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-1","text":"For each people, the probability that he doesn't have the same birthday as me is $\\frac{n - 1}{n}$, for k people that all don't have the same birthday as me is $(\\frac{n - 1}{n})^k$, thus the probability that at least one people that has the same birthday as me is $1 - (\\frac{n - 1}{n})^k$. Let $1 - (\\frac{n - 1}{n})^k \\geq \\frac{1}{2}$, and we have $k \\geq 253$. The probability that only one people has a birthday on July 4 is $k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1}$, and the probability that no one has a birthday on July 4 is $(\\frac{n - 1}{n})^k$, thus the probability that at least two people have a birthday on July 4 is $1 - k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1} - (\\frac{n - 1}{n})^k$, let $1 - k\\frac{1}{n}(\\frac{n - 1}{n})^{k - 1} - (\\frac{n - 1}{n})^k \\geq \\frac{1}{2}$, we have $k \\geq 613$.","title":"5.4-1"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-2","text":"The expected number of ball tosses is $1 + \\sum_{k = 1}^{b} \\frac{b!}{(b - k)!b^k}$, it's a birthday problem .","title":"5.4-2"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-3","text":"The equation shows that $E[X] = \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}E[X_{ij}]$, so pairwise indenpendence is sufficient.","title":"5.4-3"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-4","text":"For each pair (i, j, r) of the k people in the room, we define the indicator random variable $X_{ijr}$, for $1 \\leq i < j < r \\leq k$, by: $$ \\begin{eqnarray} X_{ijr} &=& I\\lbrace\\text{person i, person j and person r have the same birthday}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if person i, person j and person r have the same birthday} \\\\ 0, & \\text{otherwise} \\end{cases} \\end{eqnarray} $$ The probability that i's birthday, j's birthday and r's birthday all fall on day s is $Pr\\lbrace b_i = s \\text{ and } b_j = s \\text{ and } b_r = s \\rbrace = Pr\\lbrace b_i = s \\rbrace Pr\\lbrace b_j = s \\rbrace Pr\\lbrace b_r = s \\rbrace = \\frac{1}{n^3}$. Thus, the probability that they all fall on the same day is: $$ \\begin{eqnarray} Pr\\lbrace b_i = b_j = b_r \\rbrace &=& \\sum_{s = 1}^{n}Pr\\lbrace b_i = s \\text{ and } b_j = s \\text{ and } b_r = s \\rbrace \\\\ &=& \\sum_{s = 1}^{n}\\frac{1}{n^3} \\\\ &=& \\frac{1}{n^2} \\end{eqnarray} $$ Thus, $E[X_{ijr}] = Pr\\lbrace\\text{person i, person j and person r have the same birthday}\\rbrace = \\frac{1}{n^2}$. Letting X be the random variable that counts the number of pairs of inviduals having the same birthday, we have $X = \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}X_{ijr}$. Taking expectations of both sides and applying linearity of expectation, we obtain: $$ \\begin{eqnarray} E[X] &=& E[\\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}X_{ijr}] \\\\ &=& \\sum_{i = 1}^{k}\\sum_{j = i + 1}^{k}\\sum_{r = j + 1}^{k}E[X_{ijr}] \\\\ &=& \\binom{k}{3}\\frac{1}{n^2} \\\\ &=& \\frac{k(k - 1)(k - 2)}{6n^2} \\end{eqnarray} $$ Let $\\frac{k(k - 1)(k - 2)}{6n^2} \\geq 1$, we have $k \\geq 94$.","title":"5.4-4"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-5","text":"$P = \\frac{A_n^k}{n^k}$. It can be described by the birthday paradox that k people in the romm have unique birthday.","title":"5.4-5"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-6","text":"Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by: $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{bin i is empty}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if bin i is empty} \\\\ 0, & \\text{otherwise} \\end{cases} \\end{eqnarray} $$ We have $E[X_i] = Pr\\lbrace \\text{bin i is empty} \\rbrace = (\\frac{n - 1}{n})^n$ (toss n balls into n - 1 bins). Let X be the random variable that counts the number of bins that are empty, we have $X = \\sum_{i = 1}^{n}X_i$. So: $$ \\begin{eqnarray} E[X] &=& E[\\sum_{i = 1}^{n}X_i] \\\\ &=& \\sum_{i = 1}^{n}E[X_i] \\\\ &=& \\sum_{i = 1}^{n}(\\frac{n - 1}{n})^n \\\\ &=& n(\\frac{n - 1}{n})^n \\\\ &=& n(1 - \\frac{1}{n})^n \\\\ \\end{eqnarray} $$ Then let's check another question. Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by: $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{bin i has exactly one ball}\\rbrace \\\\ &=& \\begin{cases} 1, & \\text{if bin i is has exactly one ball} \\\\ 0, & \\text{otherwise} \\end{cases} \\end{eqnarray} $$ We have $E[X_i] = Pr\\lbrace \\text{bin i has exactly one ball} \\rbrace = n\\frac{1}{n}(\\frac{n - 1}{n})^{n - 1} = (1 - \\frac{1}{n})^{n - 1}$ (toss one ball into one bin, then toss n - 1 balls into n - 1 bins). Let X be the random variable that counts the number of bins that are empty, we have $X = \\sum_{i = 1}^{n}X_i$. So: $$ \\begin{eqnarray} E[X] &=& E[\\sum_{i = 1}^{n}X_i] \\\\ &=& \\sum_{i = 1}^{n}E[X_i] \\\\ &=& \\sum_{i = 1}^{n}(1 - \\frac{1}{n})^{n - 1} \\\\ &=& n(1 - \\frac{1}{n})^{n - 1} \\end{eqnarray} $$","title":"5.4-6"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/5.4-Probabilistic-analysis-and-further-uses-of-indicator-random-variables/#54-7","text":"","title":"5.4-7"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/","text":"Problems 5-1 a Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by: $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{the INCREMENT operation increases the counter}\\rbrace \\\\ &=& \\begin{cases} n_{i + 1} - n_i, & \\text{the INCREMENT operation increases the counter} \\\\ 0, & \\text{otherwise} \\end{cases} \\end{eqnarray} $$ We have: $$ \\begin{eqnarray} E[X_i] &=& 0 * Pr\\lbrace \\text{the INCREMENT operation doesn't increase the counter} \\rbrace + \\\\ && (n_{i + 1} - n_i) * Pr\\lbrace \\text{the INCREMENT operation increases the counter} \\rbrace \\\\ &=& 0 * (1 - \\frac{1}{n_{i + 1} - n_i}) + (n_{i + 1} - n_i) * \\frac{1}{n_{i + 1} - n_i} \\\\ &=& 1 \\end{eqnarray} $$ Let X be the random variable as the expected value represented by the counter, we have $X = \\sum_{i = 1}^{n}X_i$. So: $$ \\begin{eqnarray} E[X] &=& E[\\sum_{i = 1}^{n}X_i] \\\\ &=& \\sum_{i = 1}^{n}E[X_i] \\\\ &=& \\sum_{i = 1}^{n}1 \\\\ &=& n \\end{eqnarray} $$ b $$ \\begin{eqnarray} Var(X_i) &=& E[X_i^2] - (E[X_i])^2 \\\\ &=& 0^2 * (1 - \\frac{1}{n_{i + 1} - n_i}) + (n_{i + 1} - n_i)^2 * \\frac{1}{n_{i + 1} - n_i} - 1^2 \\\\ &=& 100^2 * \\frac{1}{100} - 1^2 \\\\ &=& 99 \\end{eqnarray} $$ Because the random variables $X_i$ are uncorrelated, so $Var(X) = Var(\\sum_{i = 1}^{n}X_i) = \\sum_{i = 1}^{n}Var(X_i) = \\sum_{i = 1}^{n}99 = 99n$. 5-2 a RANDOM-SEARCH(A, x) n = A.length let B[1..n] be new array while B is not full i = RANDOM(1, n) B[i] = i if A[i] = x return i return -1 b This is similar like the question \"How many balls must we toss, an the average, until a given bin contains a ball?\", the answer is n. c Each time when we pick an index, we have the probability $\\frac{k}{n}$ such that we find x in A. So according to C.32, it takes $\\frac{1}{\\frac{k}{n}} = \\frac{n}{k}$ trials before we find x. d This is similar like the question \"How many balls must we toss until every bin contains at least one ball?\", the answer is $n(\\ln{n} + O(1))$. e The best-case running time is 1, and the worst-case running time is n, thus the average-case running time is $\\frac{n + 1}{2}$. f The best-case running time is 1, and the worst-case running time is n - k + 1. The average-case running time is $\\frac{n + 1}{k + 1}$, here is the discussion . g If there are no indices i such that A[i] = x, then it always scans all elements in A, thus the average-case and worst-case running time are both n. h The worst-case running time is the same as DETERMINISTIC-SEARCH, the expected-case running time is the average-case running time in DETERMINISTIC-SEARCH. i The DETERMINISTIC-SEARCH. It has better average-case running time, and the SCRAMBLE-SEARCH takes additional time to permute the array.","title":"Problems"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#problems","text":"","title":"Problems"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#5-1","text":"","title":"5-1"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#a","text":"Let's define the indicator random variable $X_i$, for $1 \\leq i \\leq n$, by: $$ \\begin{eqnarray} X_i &=& I\\lbrace\\text{the INCREMENT operation increases the counter}\\rbrace \\\\ &=& \\begin{cases} n_{i + 1} - n_i, & \\text{the INCREMENT operation increases the counter} \\\\ 0, & \\text{otherwise} \\end{cases} \\end{eqnarray} $$ We have: $$ \\begin{eqnarray} E[X_i] &=& 0 * Pr\\lbrace \\text{the INCREMENT operation doesn't increase the counter} \\rbrace + \\\\ && (n_{i + 1} - n_i) * Pr\\lbrace \\text{the INCREMENT operation increases the counter} \\rbrace \\\\ &=& 0 * (1 - \\frac{1}{n_{i + 1} - n_i}) + (n_{i + 1} - n_i) * \\frac{1}{n_{i + 1} - n_i} \\\\ &=& 1 \\end{eqnarray} $$ Let X be the random variable as the expected value represented by the counter, we have $X = \\sum_{i = 1}^{n}X_i$. So: $$ \\begin{eqnarray} E[X] &=& E[\\sum_{i = 1}^{n}X_i] \\\\ &=& \\sum_{i = 1}^{n}E[X_i] \\\\ &=& \\sum_{i = 1}^{n}1 \\\\ &=& n \\end{eqnarray} $$","title":"a"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#b","text":"$$ \\begin{eqnarray} Var(X_i) &=& E[X_i^2] - (E[X_i])^2 \\\\ &=& 0^2 * (1 - \\frac{1}{n_{i + 1} - n_i}) + (n_{i + 1} - n_i)^2 * \\frac{1}{n_{i + 1} - n_i} - 1^2 \\\\ &=& 100^2 * \\frac{1}{100} - 1^2 \\\\ &=& 99 \\end{eqnarray} $$ Because the random variables $X_i$ are uncorrelated, so $Var(X) = Var(\\sum_{i = 1}^{n}X_i) = \\sum_{i = 1}^{n}Var(X_i) = \\sum_{i = 1}^{n}99 = 99n$.","title":"b"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#5-2","text":"","title":"5-2"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#a_1","text":"RANDOM-SEARCH(A, x) n = A.length let B[1..n] be new array while B is not full i = RANDOM(1, n) B[i] = i if A[i] = x return i return -1","title":"a"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#b_1","text":"This is similar like the question \"How many balls must we toss, an the average, until a given bin contains a ball?\", the answer is n.","title":"b"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#c","text":"Each time when we pick an index, we have the probability $\\frac{k}{n}$ such that we find x in A. So according to C.32, it takes $\\frac{1}{\\frac{k}{n}} = \\frac{n}{k}$ trials before we find x.","title":"c"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#d","text":"This is similar like the question \"How many balls must we toss until every bin contains at least one ball?\", the answer is $n(\\ln{n} + O(1))$.","title":"d"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#e","text":"The best-case running time is 1, and the worst-case running time is n, thus the average-case running time is $\\frac{n + 1}{2}$.","title":"e"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#f","text":"The best-case running time is 1, and the worst-case running time is n - k + 1. The average-case running time is $\\frac{n + 1}{k + 1}$, here is the discussion .","title":"f"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#g","text":"If there are no indices i such that A[i] = x, then it always scans all elements in A, thus the average-case and worst-case running time are both n.","title":"g"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#h","text":"The worst-case running time is the same as DETERMINISTIC-SEARCH, the expected-case running time is the average-case running time in DETERMINISTIC-SEARCH.","title":"h"},{"location":"05-Probabilistic-Analysis-and-Randomized-Algorithms/Problems/#i","text":"The DETERMINISTIC-SEARCH. It has better average-case running time, and the SCRAMBLE-SEARCH takes additional time to permute the array.","title":"i"},{"location":"06-Heapsort/6.1-Heaps/","text":"6.1 Heaps 6.1-1 The height of a heap is defined to be the height of its root. The heap has minimum number of elements when height h contains only one element, and the heap has maximum number of elements when height contains $2^h$ elements. So the minimum number of elements is $1 + 2^1 + 2^2 + \\ldots + 2^{h - 1} + 1 = 2^h$. The maximum number of elements below root is $1 + 2^1 + 2^2 + \\ldots + 2^h = 2^{h + 1} - 1$. 6.1-2 In the previous question we have proved if a heap has height h, then the number of elements n satisfies $2^h \\leq n \\leq 2^{h + 1} - 1$. So $2^h \\leq n < 2^{h + 1}$, thus $h \\leq \\lg{n} < h + 1$, so $h = \\lfloor \\lg{n} \\rfloor$. 6.1-3 In a max-heap, for every node i other than the root, $A[PARENT(i)] \\geq A[i]$. So the root of a subtree contains the value that is greater than its left and right children. Since the left and right children are also the root of their own subtree, their values are greater than their own children's values. Thus, the root of the subtree contains the largest value occurring anywhere in that subtree. 6.1-4 It cannot be a root of subtree, it must be a leaf. 6.1-5 If the array is in increasing order, then for any index $i \\leq j$, we have $A[i] \\leq A[j]$, thus for any root index i of a subtree in heap, the left child index is 2i, and the right child index is 2i + 1, it's obvious that i < 2i and i < 2i + 1, thus $A[i] \\leq A[2i]$ and $A[i] \\leq A[2i + 1]$, so it's a min-heap. But if the array is in descending order, it's not a min-heap. 6.1-6 The subtree root index of 4 has right child with index 9, but A[4] < A[9]. So it's not a max-heap. 6.1-7 If a root of subtree has index i, then its left child index is 2i, and its right child index is 2i + 1. So it must satisfy $2i \\leq n$, $i \\leq \\lfloor \\frac{n}{2} \\rfloor$. So the maximum index of subtree root is $\\lfloor \\frac{n}{2} \\rfloor$, so the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\lfloor \\frac{n}{2} \\rfloor + 2, \\ldots, n$.","title":"6.1 Heaps"},{"location":"06-Heapsort/6.1-Heaps/#61-heaps","text":"","title":"6.1 Heaps"},{"location":"06-Heapsort/6.1-Heaps/#61-1","text":"The height of a heap is defined to be the height of its root. The heap has minimum number of elements when height h contains only one element, and the heap has maximum number of elements when height contains $2^h$ elements. So the minimum number of elements is $1 + 2^1 + 2^2 + \\ldots + 2^{h - 1} + 1 = 2^h$. The maximum number of elements below root is $1 + 2^1 + 2^2 + \\ldots + 2^h = 2^{h + 1} - 1$.","title":"6.1-1"},{"location":"06-Heapsort/6.1-Heaps/#61-2","text":"In the previous question we have proved if a heap has height h, then the number of elements n satisfies $2^h \\leq n \\leq 2^{h + 1} - 1$. So $2^h \\leq n < 2^{h + 1}$, thus $h \\leq \\lg{n} < h + 1$, so $h = \\lfloor \\lg{n} \\rfloor$.","title":"6.1-2"},{"location":"06-Heapsort/6.1-Heaps/#61-3","text":"In a max-heap, for every node i other than the root, $A[PARENT(i)] \\geq A[i]$. So the root of a subtree contains the value that is greater than its left and right children. Since the left and right children are also the root of their own subtree, their values are greater than their own children's values. Thus, the root of the subtree contains the largest value occurring anywhere in that subtree.","title":"6.1-3"},{"location":"06-Heapsort/6.1-Heaps/#61-4","text":"It cannot be a root of subtree, it must be a leaf.","title":"6.1-4"},{"location":"06-Heapsort/6.1-Heaps/#61-5","text":"If the array is in increasing order, then for any index $i \\leq j$, we have $A[i] \\leq A[j]$, thus for any root index i of a subtree in heap, the left child index is 2i, and the right child index is 2i + 1, it's obvious that i < 2i and i < 2i + 1, thus $A[i] \\leq A[2i]$ and $A[i] \\leq A[2i + 1]$, so it's a min-heap. But if the array is in descending order, it's not a min-heap.","title":"6.1-5"},{"location":"06-Heapsort/6.1-Heaps/#61-6","text":"The subtree root index of 4 has right child with index 9, but A[4] < A[9]. So it's not a max-heap.","title":"6.1-6"},{"location":"06-Heapsort/6.1-Heaps/#61-7","text":"If a root of subtree has index i, then its left child index is 2i, and its right child index is 2i + 1. So it must satisfy $2i \\leq n$, $i \\leq \\lfloor \\frac{n}{2} \\rfloor$. So the maximum index of subtree root is $\\lfloor \\frac{n}{2} \\rfloor$, so the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\lfloor \\frac{n}{2} \\rfloor + 2, \\ldots, n$.","title":"6.1-7"},{"location":"06-Heapsort/6.2-Maintaining-the-heap-property/","text":"6.2 Maintaining the heap property 6.2-1 6.2-2 MIN-HEAPIFY(A, i) l = LEFT(i) r = RIGHT(i) if l <= A.heap-size and A[l] < A[i] min = l else min = i if r <= A.heap-size and A[r] < A[min] min = r if min != i exchange A[i] with A[min] MIN-HEAPIFY(A, min) The running time is still $O(\\lg{n})$. 6.2-3 The procedure terminates and the running time is O(1). 6.2-4 In exercise 6.1-7 we know the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\ldots$, thus if i > A.heap-size / 2, then the node at index i is a leaf, so both l <= A.heap-size and r <= A.heap-size fail, the procedure terminates. 6.2-5 MAX-HEAPIFY(A, i) largest = -1 root = i while largest != root l = LEFT(root) r = RIGHT(root) if l <= A.heap-size and A[l] > A[root] largest = l else largest = root if r <= A.heap-size and A[r] > A[largest] largest = r if largest != root: exchange A[root] with A[largest] root = largest largest = -1 6.2-6 The worst-case happens when it checks every node from the root down to a leaf. And the height h is $\\lfloor \\lg{n} \\rfloor \\geq \\lg{n}$, so the worst-case running time is $\\Omega(\\lg{n})$.","title":"6.2 Maintaining the heap property"},{"location":"06-Heapsort/6.2-Maintaining-the-heap-property/#62-maintaining-the-heap-property","text":"","title":"6.2 Maintaining the heap property"},{"location":"06-Heapsort/6.2-Maintaining-the-heap-property/#62-1","text":"","title":"6.2-1"},{"location":"06-Heapsort/6.2-Maintaining-the-heap-property/#62-2","text":"MIN-HEAPIFY(A, i) l = LEFT(i) r = RIGHT(i) if l <= A.heap-size and A[l] < A[i] min = l else min = i if r <= A.heap-size and A[r] < A[min] min = r if min != i exchange A[i] with A[min] MIN-HEAPIFY(A, min) The running time is still $O(\\lg{n})$.","title":"6.2-2"},{"location":"06-Heapsort/6.2-Maintaining-the-heap-property/#62-3","text":"The procedure terminates and the running time is O(1).","title":"6.2-3"},{"location":"06-Heapsort/6.2-Maintaining-the-heap-property/#62-4","text":"In exercise 6.1-7 we know the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\ldots$, thus if i > A.heap-size / 2, then the node at index i is a leaf, so both l <= A.heap-size and r <= A.heap-size fail, the procedure terminates.","title":"6.2-4"},{"location":"06-Heapsort/6.2-Maintaining-the-heap-property/#62-5","text":"MAX-HEAPIFY(A, i) largest = -1 root = i while largest != root l = LEFT(root) r = RIGHT(root) if l <= A.heap-size and A[l] > A[root] largest = l else largest = root if r <= A.heap-size and A[r] > A[largest] largest = r if largest != root: exchange A[root] with A[largest] root = largest largest = -1","title":"6.2-5"},{"location":"06-Heapsort/6.2-Maintaining-the-heap-property/#62-6","text":"The worst-case happens when it checks every node from the root down to a leaf. And the height h is $\\lfloor \\lg{n} \\rfloor \\geq \\lg{n}$, so the worst-case running time is $\\Omega(\\lg{n})$.","title":"6.2-6"},{"location":"06-Heapsort/6.3-Building-a-heap/","text":"6.3 Building a heap 6.3-1 6.3-2 If we loop the index in a increasing order, when we call MAX-HEAPIFY(A, i), it's possible that the left subtree at root 2i and the right subtree at root 2i + 1 are not max-heap. 6.3-3 Remember that the height of a node is the distance from the node to a leaf, such that the height of a leaf is 0 (and the height of the root is the height of the tree). In exercise 6.1-7, we proved that for a heap has n elements, the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\lfloor \\frac{n}{2} \\rfloor + 2, \\ldots, n$. So there are $\\lceil \\frac{n}{2} \\rceil$ leaves. And the height of leaves is 0, so when h = 0, $\\lceil \\frac{n}{2^{h + 1}} \\rceil = \\lceil \\frac{n}{2^{0 + 1}} \\rceil = \\lceil \\frac{n}{2} \\rceil$. The base case holds. Now let's consider the inductive step. Suppose there are at most $\\lceil \\frac{n}{2^h} \\rceil$ nodes for height h - 1. And let's cut the leaves for the tree with height h. So it has $n - \\lceil \\frac{n}{2} \\rceil = \\lfloor \\frac{n}{2} \\rfloor$ elements. And it's height becomes h - 1. By induction, the nodes at height h - 1 is $\\lceil \\frac{\\lfloor \\frac{n}{2} \\rfloor}{2^h} \\rceil \\leq \\lceil \\frac{\\frac{n}{2}}{2^h} \\rceil = \\lceil \\frac{n}{2^{h + 1}} \\rceil$, which is also the number of nodes at height h in the original tree.","title":"6.3 Building a heap"},{"location":"06-Heapsort/6.3-Building-a-heap/#63-building-a-heap","text":"","title":"6.3 Building a heap"},{"location":"06-Heapsort/6.3-Building-a-heap/#63-1","text":"","title":"6.3-1"},{"location":"06-Heapsort/6.3-Building-a-heap/#63-2","text":"If we loop the index in a increasing order, when we call MAX-HEAPIFY(A, i), it's possible that the left subtree at root 2i and the right subtree at root 2i + 1 are not max-heap.","title":"6.3-2"},{"location":"06-Heapsort/6.3-Building-a-heap/#63-3","text":"Remember that the height of a node is the distance from the node to a leaf, such that the height of a leaf is 0 (and the height of the root is the height of the tree). In exercise 6.1-7, we proved that for a heap has n elements, the leaves are the nodes indexed by $\\lfloor \\frac{n}{2} \\rfloor + 1, \\lfloor \\frac{n}{2} \\rfloor + 2, \\ldots, n$. So there are $\\lceil \\frac{n}{2} \\rceil$ leaves. And the height of leaves is 0, so when h = 0, $\\lceil \\frac{n}{2^{h + 1}} \\rceil = \\lceil \\frac{n}{2^{0 + 1}} \\rceil = \\lceil \\frac{n}{2} \\rceil$. The base case holds. Now let's consider the inductive step. Suppose there are at most $\\lceil \\frac{n}{2^h} \\rceil$ nodes for height h - 1. And let's cut the leaves for the tree with height h. So it has $n - \\lceil \\frac{n}{2} \\rceil = \\lfloor \\frac{n}{2} \\rfloor$ elements. And it's height becomes h - 1. By induction, the nodes at height h - 1 is $\\lceil \\frac{\\lfloor \\frac{n}{2} \\rfloor}{2^h} \\rceil \\leq \\lceil \\frac{\\frac{n}{2}}{2^h} \\rceil = \\lceil \\frac{n}{2^{h + 1}} \\rceil$, which is also the number of nodes at height h in the original tree.","title":"6.3-3"},{"location":"06-Heapsort/6.4-The-heapsort-algorithm/","text":"6.4 The heapsort algorithm 6.4-1 6.4-2 Initialization : Prior to the first iteration of the loop, i = n, and we called BUILD-MAX-HEAP on A, thus the subarray A[1..n] is a max-heap containing the n smallest elements of A[1..n] , and the subarray A[n + 1..n] is an empty array, thus it contains n - n = 0 largest elements of A[1..n] , sorted. Maintenance : Let's assume prior to the ith iteration of the loop, the subarray A[1..i] is a max-heap containing the i smallest elements of A[1..n] , and the subarray A[i + 1,..n] contains the n - i largest elements of A[1..n] , sorted. In the ith loop, A[1] is the biggest number in the subarray A[1..i] , in line 3, it exchanges A[1] and A[i] , making the subarray A[i..n] contains the n - i + 1 largest elements of A[1..n] , sorted. In line 4-5, it calls MAX-HEAPIFY on A, so it makes the subarray A[1..i - 1] to a max-heap contains the i - 1 smallest elements of A[1..n] . Termination : At termination, i = 1. By the loop invariant, the subarray A[1..1] is a max-heap containing the 1 smallest elements of A[1..n] . And the subarray A[2..n] contains the n - 1 largest elements of A[1..n] , sorted. So the array A[1..n] is sorted. 6.4-3 If the array is in increasing order, then BUILD-MAX-HEAP takes O(n), and it changes A to a max-heap. Then it calls MAX-HEAPIFY(A, 1) n - 1 times, at each iteration, there are i - 1 elements in the current max-heap, so the running time of for loop is $\\sum_{i = n - 1}^{1}\\lg{i} = \\lg{((n - 1)!)} = \\Theta(n\\lg{n})$ (exercise 3.2-3). So the total running time is $O(n) + \\Theta(n\\lg{n}) = \\Theta(n\\lg{n})$. If the array is in decreasing order, it takes O(n) to call BUILD-MAX-HEAP , but it won't change the array. But it still needs $\\Theta(n\\lg{n})$ running time to call MAX-HEAPIFY(A, 1) . The total running time is also $\\Theta(n\\lg{n})$. 6.4-4 The worst-case happens when we need to call MAX-HEAPIFY(A, 1) in each iteration with the cost $\\lg{(i - 1)}$. And the answer is given in the above question. 6.4-5 You can find the answer here .","title":"6.4 The heapsort algorithm"},{"location":"06-Heapsort/6.4-The-heapsort-algorithm/#64-the-heapsort-algorithm","text":"","title":"6.4 The heapsort algorithm"},{"location":"06-Heapsort/6.4-The-heapsort-algorithm/#64-1","text":"","title":"6.4-1"},{"location":"06-Heapsort/6.4-The-heapsort-algorithm/#64-2","text":"Initialization : Prior to the first iteration of the loop, i = n, and we called BUILD-MAX-HEAP on A, thus the subarray A[1..n] is a max-heap containing the n smallest elements of A[1..n] , and the subarray A[n + 1..n] is an empty array, thus it contains n - n = 0 largest elements of A[1..n] , sorted. Maintenance : Let's assume prior to the ith iteration of the loop, the subarray A[1..i] is a max-heap containing the i smallest elements of A[1..n] , and the subarray A[i + 1,..n] contains the n - i largest elements of A[1..n] , sorted. In the ith loop, A[1] is the biggest number in the subarray A[1..i] , in line 3, it exchanges A[1] and A[i] , making the subarray A[i..n] contains the n - i + 1 largest elements of A[1..n] , sorted. In line 4-5, it calls MAX-HEAPIFY on A, so it makes the subarray A[1..i - 1] to a max-heap contains the i - 1 smallest elements of A[1..n] . Termination : At termination, i = 1. By the loop invariant, the subarray A[1..1] is a max-heap containing the 1 smallest elements of A[1..n] . And the subarray A[2..n] contains the n - 1 largest elements of A[1..n] , sorted. So the array A[1..n] is sorted.","title":"6.4-2"},{"location":"06-Heapsort/6.4-The-heapsort-algorithm/#64-3","text":"If the array is in increasing order, then BUILD-MAX-HEAP takes O(n), and it changes A to a max-heap. Then it calls MAX-HEAPIFY(A, 1) n - 1 times, at each iteration, there are i - 1 elements in the current max-heap, so the running time of for loop is $\\sum_{i = n - 1}^{1}\\lg{i} = \\lg{((n - 1)!)} = \\Theta(n\\lg{n})$ (exercise 3.2-3). So the total running time is $O(n) + \\Theta(n\\lg{n}) = \\Theta(n\\lg{n})$. If the array is in decreasing order, it takes O(n) to call BUILD-MAX-HEAP , but it won't change the array. But it still needs $\\Theta(n\\lg{n})$ running time to call MAX-HEAPIFY(A, 1) . The total running time is also $\\Theta(n\\lg{n})$.","title":"6.4-3"},{"location":"06-Heapsort/6.4-The-heapsort-algorithm/#64-4","text":"The worst-case happens when we need to call MAX-HEAPIFY(A, 1) in each iteration with the cost $\\lg{(i - 1)}$. And the answer is given in the above question.","title":"6.4-4"},{"location":"06-Heapsort/6.4-The-heapsort-algorithm/#64-5","text":"You can find the answer here .","title":"6.4-5"},{"location":"06-Heapsort/6.5-Priority-queues/","text":"6.5 Priority queues 6.5-1 6.5-2 6.5-3 HEAP-MINIMUM(A) return A[1] HEAP-EXTRACT-MIN(A) if A.heap-size < 1 error \"heap underflow\" min = A[1] A[1] = A[A.heap-size] A.heap-size = A.heap-size - 1 MIN-HEAPIFY(A, 1) return max HEAP-DECREASE-KEY(A, i, key) if key > A[i] error \"new key is bigger than current key\" A[i] = key while i > 1 and A[PARENT(i)] > A[i] exchange A[i] with A[PARENT(i)] i = PARENT(i) MIN-HEAP-INSERT(A, key) A.heap-size = A.heap-size + 1 A[A.heap-size] = +\u221e HEAP-DECREASE-KEY(A, A.heap-size, key) 6.5-4 Setting the key of the inserted node to negative infinity to make sure it can pass the truth check in line 1 of HEAP-INCREASE-KEY(A, i, key) . 6.5-5 Initialization : Prior to the first iteration of the loop, the subarray A[1..A.heap-size] is satifies the max-heap before we set A[i] = key , after we set A[i] = key , it's possible that A[i] would be greater than its parent. And we cannot set a smaller key to A[i] , so there may be only one violation. Maintenance : In the for loop, we are exchanging A[i] and A[PARENT(i)] , before the exchange, the subtree rooted at A[i] is a max-heap, but it may be greater than its parent, so there may be on violation. Termination : When it's terminated, i maybe 1 or A[i] is smaller thant its parent, either makes A a max-heap. 6.5-6 HEAP-INCREASE-KEY(A, i, key) if key < A[i] error \"new key is smaller than current key\" while i > 1 and A[PARENT(i)] < key A[i] = A[PARENT(i)] i = PARENT(i) A[i] = key 6.5-7 Implement a first-in, first-out queue with a min priority queue. Call MIN-HEAP-INSERT method to enqueue with a priority, for example, use timestamp as priority. Call HEAP-MINIMUM to dequeue. Implement a stack with a max priority queue. Call MAX-HEAP-INSERT method to push with a priority, for example, use timestamp as priority. Call HEAP-MAXIMUM to pop. 6.5-8 HEAP-DELETE(A, i) A.heap-size < 1 error \"heap underflow\" A[i] = A[A.heap-size] A.heap-size = A.heap-size - 1 MAX-HEAPIFY(A, i) 6.5-9 First we create a k-elements min heap from the first element in each sorted list, this requires $O(k) + O(n) = O(n)$. Then we keep extracting the min value from min heap, and insert the next value after the min value in the original list, this requires $O(\\lg{k})$. And there are n elements, so it requires $O(n\\lg{k})$. Thus the running time is $O(n) + O(n\\lg{k}) = O(n\\lg{k})$. def merge_sorted_lists(lists): sorted_list = [] min_heap_elements = [] for i in range(len(lists)): min_heap_elements.append(MinHeapElement(i, 1, lists[i][0])) min_heap = MinHeap(min_heap_elements) while not min_heap.is_empty(): min_element = min_heap.extract_min() sorted_list.append(min_element.value) list_index = min_element.list_index next_index = min_element.next_index if next_index < len(lists[list_index]): next_element = MinHeapElement( list_index, next_index + 1, lists[list_index][next_index]) min_heap.insert(next_element) return sorted_list class MinHeapElement(): def __init__(self, list_index, next_index, value): self.list_index = list_index self.next_index = next_index self.value = value class MinHeap(): def __init__(self, elements): self.elements = elements self.heap_size = len(elements) self.build_min_heap() def extract_min(self): assert not self.is_empty() minimum = self.elements[0] self.elements[0] = self.elements[self.heap_size - 1] self.heap_size -= 1 self.min_heapify(0) return minimum def insert(self, element): self.heap_size += 1 if len(self.elements) < self.heap_size: self.elements.append(element) else: self.elements[self.heap_size - 1] = element self.decrease_element(self.heap_size - 1, element) def decrease_element(self, i, element): assert i < self.heap_size assert element.value <= self.elements[i].value while i > 0 and self.elements[(i - 1) // 2].value > element.value: self.elements[i] = self.elements[(i - 1) // 2] i = (i - 1) // 2 self.elements[i] = element def is_empty(self): return self.heap_size == 0 def min_heapify(self, i): left = 2 * i + 1 right = 2 * i + 2 minimum = i if (left <= self.heap_size - 1 and self.elements[left].value < self.elements[i].value): minimum = left if (right <= self.heap_size - 1 and self.elements[right].value < self.elements[minimum].value): minimum = right if minimum != i: self.elements[i], self.elements[minimum] = \\ self.elements[minimum], self.elements[i] self.min_heapify(minimum) def build_min_heap(self): for i in range((self.heap_size - 1) // 2, -1, -1): self.min_heapify(i)","title":"6.5 Priority queues"},{"location":"06-Heapsort/6.5-Priority-queues/#65-priority-queues","text":"","title":"6.5 Priority queues"},{"location":"06-Heapsort/6.5-Priority-queues/#65-1","text":"","title":"6.5-1"},{"location":"06-Heapsort/6.5-Priority-queues/#65-2","text":"","title":"6.5-2"},{"location":"06-Heapsort/6.5-Priority-queues/#65-3","text":"HEAP-MINIMUM(A) return A[1] HEAP-EXTRACT-MIN(A) if A.heap-size < 1 error \"heap underflow\" min = A[1] A[1] = A[A.heap-size] A.heap-size = A.heap-size - 1 MIN-HEAPIFY(A, 1) return max HEAP-DECREASE-KEY(A, i, key) if key > A[i] error \"new key is bigger than current key\" A[i] = key while i > 1 and A[PARENT(i)] > A[i] exchange A[i] with A[PARENT(i)] i = PARENT(i) MIN-HEAP-INSERT(A, key) A.heap-size = A.heap-size + 1 A[A.heap-size] = +\u221e HEAP-DECREASE-KEY(A, A.heap-size, key)","title":"6.5-3"},{"location":"06-Heapsort/6.5-Priority-queues/#65-4","text":"Setting the key of the inserted node to negative infinity to make sure it can pass the truth check in line 1 of HEAP-INCREASE-KEY(A, i, key) .","title":"6.5-4"},{"location":"06-Heapsort/6.5-Priority-queues/#65-5","text":"Initialization : Prior to the first iteration of the loop, the subarray A[1..A.heap-size] is satifies the max-heap before we set A[i] = key , after we set A[i] = key , it's possible that A[i] would be greater than its parent. And we cannot set a smaller key to A[i] , so there may be only one violation. Maintenance : In the for loop, we are exchanging A[i] and A[PARENT(i)] , before the exchange, the subtree rooted at A[i] is a max-heap, but it may be greater than its parent, so there may be on violation. Termination : When it's terminated, i maybe 1 or A[i] is smaller thant its parent, either makes A a max-heap.","title":"6.5-5"},{"location":"06-Heapsort/6.5-Priority-queues/#65-6","text":"HEAP-INCREASE-KEY(A, i, key) if key < A[i] error \"new key is smaller than current key\" while i > 1 and A[PARENT(i)] < key A[i] = A[PARENT(i)] i = PARENT(i) A[i] = key","title":"6.5-6"},{"location":"06-Heapsort/6.5-Priority-queues/#65-7","text":"Implement a first-in, first-out queue with a min priority queue. Call MIN-HEAP-INSERT method to enqueue with a priority, for example, use timestamp as priority. Call HEAP-MINIMUM to dequeue. Implement a stack with a max priority queue. Call MAX-HEAP-INSERT method to push with a priority, for example, use timestamp as priority. Call HEAP-MAXIMUM to pop.","title":"6.5-7"},{"location":"06-Heapsort/6.5-Priority-queues/#65-8","text":"HEAP-DELETE(A, i) A.heap-size < 1 error \"heap underflow\" A[i] = A[A.heap-size] A.heap-size = A.heap-size - 1 MAX-HEAPIFY(A, i)","title":"6.5-8"},{"location":"06-Heapsort/6.5-Priority-queues/#65-9","text":"First we create a k-elements min heap from the first element in each sorted list, this requires $O(k) + O(n) = O(n)$. Then we keep extracting the min value from min heap, and insert the next value after the min value in the original list, this requires $O(\\lg{k})$. And there are n elements, so it requires $O(n\\lg{k})$. Thus the running time is $O(n) + O(n\\lg{k}) = O(n\\lg{k})$. def merge_sorted_lists(lists): sorted_list = [] min_heap_elements = [] for i in range(len(lists)): min_heap_elements.append(MinHeapElement(i, 1, lists[i][0])) min_heap = MinHeap(min_heap_elements) while not min_heap.is_empty(): min_element = min_heap.extract_min() sorted_list.append(min_element.value) list_index = min_element.list_index next_index = min_element.next_index if next_index < len(lists[list_index]): next_element = MinHeapElement( list_index, next_index + 1, lists[list_index][next_index]) min_heap.insert(next_element) return sorted_list class MinHeapElement(): def __init__(self, list_index, next_index, value): self.list_index = list_index self.next_index = next_index self.value = value class MinHeap(): def __init__(self, elements): self.elements = elements self.heap_size = len(elements) self.build_min_heap() def extract_min(self): assert not self.is_empty() minimum = self.elements[0] self.elements[0] = self.elements[self.heap_size - 1] self.heap_size -= 1 self.min_heapify(0) return minimum def insert(self, element): self.heap_size += 1 if len(self.elements) < self.heap_size: self.elements.append(element) else: self.elements[self.heap_size - 1] = element self.decrease_element(self.heap_size - 1, element) def decrease_element(self, i, element): assert i < self.heap_size assert element.value <= self.elements[i].value while i > 0 and self.elements[(i - 1) // 2].value > element.value: self.elements[i] = self.elements[(i - 1) // 2] i = (i - 1) // 2 self.elements[i] = element def is_empty(self): return self.heap_size == 0 def min_heapify(self, i): left = 2 * i + 1 right = 2 * i + 2 minimum = i if (left <= self.heap_size - 1 and self.elements[left].value < self.elements[i].value): minimum = left if (right <= self.heap_size - 1 and self.elements[right].value < self.elements[minimum].value): minimum = right if minimum != i: self.elements[i], self.elements[minimum] = \\ self.elements[minimum], self.elements[i] self.min_heapify(minimum) def build_min_heap(self): for i in range((self.heap_size - 1) // 2, -1, -1): self.min_heapify(i)","title":"6.5-9"},{"location":"06-Heapsort/Problems/","text":"Problems 6-1 a Let A = [1, 2, 3, 4] , if we use BUILD-MAX-HEAP , we have A = [4, 2, 3, 1] . If we use BUILD-MAX-HEAP' , we have A = [4, 3, 2, 1] . That's a counterexample. b The worst-case happens when A is in increasing order, in each iteration, MAX-HEAP-INSERT(A, A[i]) requires $O(\\lg{k})$ to heapify the heap (move A[i] from bottom to top), where k is the number of elements in heap. Thus the running time is $\\sum_{i = 2}^{n}\\lg{i} = \\lg{(n!)} = \\Theta(n\\lg{n})$. 6-2 a For a node with index i (i = 1, 2, ..., n), its kth child index is d(i - 1) + k + 1 in array, and its parent's index is $\\lceil \\frac{i - 1}{d} \\rceil$. b The max number of elements in d-ary heap is $d^0 + d^1 + \\ldots + d^h = \\sum_{i = 0}^{h}d^h = \\frac{d^{h + 1} - 1}{d - 1}$, the min number of elements in d-ary heap is $d^0 + d^1 + \\ldots + d^{h - 1} + 1 = \\sum_{i = 0}^{h}d^{h - 1} + 1 = \\frac{d^h - 1}{d - 1} + 1$. Thus, $\\frac{d^h - 1}{d - 1} + 1 \\leq n \\leq \\frac{d^{h + 1} - 1}{d - 1}$. For a d-ary heap, we have $d \\geq 2$. Let's check the left part first. $\\frac{d^h - 1}{d - 1} + 1 = \\frac{d^h - 1 + (d - 2)}{d - 1} \\geq \\frac{d^h - 1}{d - 1}$, so $h \\leq \\log_d{(n(d - 1) + 1)}$. And: $$ \\begin{eqnarray} \\log_d{(n(d - 1) + 1)} - (\\log_d{(n(d - 1))} + 1) &=& \\log_d{(\\frac{1}{d}(1 + \\frac{1}{n(d - 1)}))} \\\\ &\\leq& \\log_d{(\\frac{1}{d}(1 + \\frac{1}{d - 1}))} \\\\ &=& \\log_d{\\frac{1}{d - 1}} \\\\ &\\leq& \\log_d{\\frac{1}{\\frac{d}{2}}} \\\\ &=& \\log_d{\\frac{2}{d}} \\\\ &=& \\log_d2 - 1 \\\\ &\\leq& \\log_2{2} - 1 \\\\ &=& 0 \\end{eqnarray} $$ So $\\log_d{(n(d - 1) + 1)} \\leq \\log_d{(n(d - 1))} + 1$, if n = 1 and d = 2, we have $\\log_d{(n(d - 1) + 1)} = \\log_d{(n(d - 1))} + 1$, for a d-ary heap, n is usually not 1, so we can say $\\log_d{(n(d - 1) + 1)} < \\log_d{(n(d - 1))} + 1$. Thus we have $h < \\log_d{(n(d - 1))} + 1$. Then let's check the right part. $\\frac{d^{h + 1} - 1}{d - 1} < \\frac{d^{h + 1}}{d - 1}$, so $h > \\log_d{(n(d - 1))} - 1$. So $h = \\lfloor \\log_d{(n(d - 1))} \\rfloor$. c It's similar like EXTRACT-MAX 2-ary heap. The running time of EXTRACT-MAX for d-ary heap is mainly the running time of MAX-HEAPIFY . We need to change MAX-HEAPIFY a little bit. MAX-HEAPIFY(A, i) largest = i for i = 1 to d child = CHILD(i) if child <= A.heap-size and A[child] > A[largest] largest = child if largest != i exchange A[i] with A[largest] MAX-HEAPIFY(A, largest) So the running time of MAX-HEAPIFY for d-ary heap is $O(dh) = O(d\\log_d{(n(d - 1))})$. d The INSERT method only compares node i with its parent, so the running time is $O(h) = O(\\log_d{(n(d - 1))})$. e Since the INSERT method calls INCREASE-KEY method, the running time of INCREASE-KEY is also $O(h) = O(\\log_d{(n(d - 1))})$. 6-3 a $$ \\begin{matrix} 2 & 3 & 4 & 5 \\\\ 8 & 9 & 12 & 14 \\\\ 16 & \\infty & \\infty & \\infty \\\\ \\infty & \\infty & \\infty & \\infty \\end{matrix} $$ b If $Y[1, 1] = \\infty$, then no cell can has a value greater than Y[1, 1] , other cells are all $\\infty$, so Y is empty. Y[m, n] is the largest in Y, if it's not $\\infty$, then others are also not $\\infty$, so Y is full. c Y[1, 1] is the smallest, after we extract it, we need to make Y to a Young tableau again. First we start at Y[1, 1] , then we compare its right element and below element, if right element is smaller than below element, we put right element to Y[1, 1] , which makes Y[1, 1] to Y[1, m] sorted, but Y[1, 2] to Y[m, n] might be not a Young tableau. if right element is greater than below element, we put below element to Y[1, 1] , which makes Y[1, 1] to Y[1, n] sorted, but Y[2, 1] to Y[m, n] might be not a Young tableau. EXTRACT-MIN(Y) smallest = Y[1, 1] Y[1, 1] = \u221e YOUNGIFY(Y, 1, 1) return smallest YOUNGIFY(Y, row, column) smallest_row = row smallest_column = column if row + 1 <= m and Y[row + 1, column] < Y[row, column] smallest_row = row + 1 if column + 1 <= n and Y[row, column + 1] < Y[smallest_row, smallest_column] smallest_row = row smallest_column = column + 1 if smallest_row != row or smallest_column != column exchange Y[row, column] with Y[smallest_row, smallest_column] YOUNGIFY(Y, smallest_row, smallest_column) After each recursive procedure, the YOUNGIFY method cuts a row or a column, which reduces the problem size to (m - 1) x n or m x (n - 1). Notice that both (m - 1) + n = m + n - 1 and m + (n - 1) = m + n - 1, so m + n is decreased by 1 in each recursive procedure. so T(p) = T(p - 1) + O(1), it's obvious to know that T(p) = O(p) = O(m + n). d Similar like EXTRACT-MIN , we first put the new element to Y[m, n] , then youngify the tableau from Y[m, n] . The running time is O(m + n). INSERT(Y, key) Y[m][n] = key YOUNGIFY-INSERT(Y, m, n) YOUNGIFY-INSERT(Y, row, column) largest_row = row largest_column = column if row - 1 >= 1 and Y[row - 1][column] > Y[row][column] largest_row = row - 1 if column - 1 >= 1 and Y[row][column - 1] > Y[largest_row][largest_column] largest_row = row largest_column = column - 1 if largest_row != row or largest_column != column exchange Y[row, column] with Y[largest_row, largest_column] YOUNGIFY-INSERT(Y, largest_row, largest_column) e The INSERT operation takes O(n + n) = O(n) time, and there are $n^2$ elements, so we can insert each element into the n x n Young tableau, thus the running time is $n^2O(n) = O(n^3)$. f We start from Y[m, 1] , and compare Y[m, 1] with target value, if Y[m, 1] is greater than target value, we move to Y[m - 1, 1] , if it's smaller than target value, we move to Y[m, 2] , otherwise, we find the target value. FIND(Y, key) row = m column = 1 while row >= 1 and column <= n if Y[row][column] == key return True else if Y[row][column] < key column += 1 else if Y[row][column] > key row -= 1 return False Similar like YOUNGIFY , it reduces the problem size to (m - 1) * n or m * (n - 1), so the running time is O(m + n).","title":"Problems"},{"location":"06-Heapsort/Problems/#problems","text":"","title":"Problems"},{"location":"06-Heapsort/Problems/#6-1","text":"","title":"6-1"},{"location":"06-Heapsort/Problems/#a","text":"Let A = [1, 2, 3, 4] , if we use BUILD-MAX-HEAP , we have A = [4, 2, 3, 1] . If we use BUILD-MAX-HEAP' , we have A = [4, 3, 2, 1] . That's a counterexample.","title":"a"},{"location":"06-Heapsort/Problems/#b","text":"The worst-case happens when A is in increasing order, in each iteration, MAX-HEAP-INSERT(A, A[i]) requires $O(\\lg{k})$ to heapify the heap (move A[i] from bottom to top), where k is the number of elements in heap. Thus the running time is $\\sum_{i = 2}^{n}\\lg{i} = \\lg{(n!)} = \\Theta(n\\lg{n})$.","title":"b"},{"location":"06-Heapsort/Problems/#6-2","text":"","title":"6-2"},{"location":"06-Heapsort/Problems/#a_1","text":"For a node with index i (i = 1, 2, ..., n), its kth child index is d(i - 1) + k + 1 in array, and its parent's index is $\\lceil \\frac{i - 1}{d} \\rceil$.","title":"a"},{"location":"06-Heapsort/Problems/#b_1","text":"The max number of elements in d-ary heap is $d^0 + d^1 + \\ldots + d^h = \\sum_{i = 0}^{h}d^h = \\frac{d^{h + 1} - 1}{d - 1}$, the min number of elements in d-ary heap is $d^0 + d^1 + \\ldots + d^{h - 1} + 1 = \\sum_{i = 0}^{h}d^{h - 1} + 1 = \\frac{d^h - 1}{d - 1} + 1$. Thus, $\\frac{d^h - 1}{d - 1} + 1 \\leq n \\leq \\frac{d^{h + 1} - 1}{d - 1}$. For a d-ary heap, we have $d \\geq 2$. Let's check the left part first. $\\frac{d^h - 1}{d - 1} + 1 = \\frac{d^h - 1 + (d - 2)}{d - 1} \\geq \\frac{d^h - 1}{d - 1}$, so $h \\leq \\log_d{(n(d - 1) + 1)}$. And: $$ \\begin{eqnarray} \\log_d{(n(d - 1) + 1)} - (\\log_d{(n(d - 1))} + 1) &=& \\log_d{(\\frac{1}{d}(1 + \\frac{1}{n(d - 1)}))} \\\\ &\\leq& \\log_d{(\\frac{1}{d}(1 + \\frac{1}{d - 1}))} \\\\ &=& \\log_d{\\frac{1}{d - 1}} \\\\ &\\leq& \\log_d{\\frac{1}{\\frac{d}{2}}} \\\\ &=& \\log_d{\\frac{2}{d}} \\\\ &=& \\log_d2 - 1 \\\\ &\\leq& \\log_2{2} - 1 \\\\ &=& 0 \\end{eqnarray} $$ So $\\log_d{(n(d - 1) + 1)} \\leq \\log_d{(n(d - 1))} + 1$, if n = 1 and d = 2, we have $\\log_d{(n(d - 1) + 1)} = \\log_d{(n(d - 1))} + 1$, for a d-ary heap, n is usually not 1, so we can say $\\log_d{(n(d - 1) + 1)} < \\log_d{(n(d - 1))} + 1$. Thus we have $h < \\log_d{(n(d - 1))} + 1$. Then let's check the right part. $\\frac{d^{h + 1} - 1}{d - 1} < \\frac{d^{h + 1}}{d - 1}$, so $h > \\log_d{(n(d - 1))} - 1$. So $h = \\lfloor \\log_d{(n(d - 1))} \\rfloor$.","title":"b"},{"location":"06-Heapsort/Problems/#c","text":"It's similar like EXTRACT-MAX 2-ary heap. The running time of EXTRACT-MAX for d-ary heap is mainly the running time of MAX-HEAPIFY . We need to change MAX-HEAPIFY a little bit. MAX-HEAPIFY(A, i) largest = i for i = 1 to d child = CHILD(i) if child <= A.heap-size and A[child] > A[largest] largest = child if largest != i exchange A[i] with A[largest] MAX-HEAPIFY(A, largest) So the running time of MAX-HEAPIFY for d-ary heap is $O(dh) = O(d\\log_d{(n(d - 1))})$.","title":"c"},{"location":"06-Heapsort/Problems/#d","text":"The INSERT method only compares node i with its parent, so the running time is $O(h) = O(\\log_d{(n(d - 1))})$.","title":"d"},{"location":"06-Heapsort/Problems/#e","text":"Since the INSERT method calls INCREASE-KEY method, the running time of INCREASE-KEY is also $O(h) = O(\\log_d{(n(d - 1))})$.","title":"e"},{"location":"06-Heapsort/Problems/#6-3","text":"","title":"6-3"},{"location":"06-Heapsort/Problems/#a_2","text":"$$ \\begin{matrix} 2 & 3 & 4 & 5 \\\\ 8 & 9 & 12 & 14 \\\\ 16 & \\infty & \\infty & \\infty \\\\ \\infty & \\infty & \\infty & \\infty \\end{matrix} $$","title":"a"},{"location":"06-Heapsort/Problems/#b_2","text":"If $Y[1, 1] = \\infty$, then no cell can has a value greater than Y[1, 1] , other cells are all $\\infty$, so Y is empty. Y[m, n] is the largest in Y, if it's not $\\infty$, then others are also not $\\infty$, so Y is full.","title":"b"},{"location":"06-Heapsort/Problems/#c_1","text":"Y[1, 1] is the smallest, after we extract it, we need to make Y to a Young tableau again. First we start at Y[1, 1] , then we compare its right element and below element, if right element is smaller than below element, we put right element to Y[1, 1] , which makes Y[1, 1] to Y[1, m] sorted, but Y[1, 2] to Y[m, n] might be not a Young tableau. if right element is greater than below element, we put below element to Y[1, 1] , which makes Y[1, 1] to Y[1, n] sorted, but Y[2, 1] to Y[m, n] might be not a Young tableau. EXTRACT-MIN(Y) smallest = Y[1, 1] Y[1, 1] = \u221e YOUNGIFY(Y, 1, 1) return smallest YOUNGIFY(Y, row, column) smallest_row = row smallest_column = column if row + 1 <= m and Y[row + 1, column] < Y[row, column] smallest_row = row + 1 if column + 1 <= n and Y[row, column + 1] < Y[smallest_row, smallest_column] smallest_row = row smallest_column = column + 1 if smallest_row != row or smallest_column != column exchange Y[row, column] with Y[smallest_row, smallest_column] YOUNGIFY(Y, smallest_row, smallest_column) After each recursive procedure, the YOUNGIFY method cuts a row or a column, which reduces the problem size to (m - 1) x n or m x (n - 1). Notice that both (m - 1) + n = m + n - 1 and m + (n - 1) = m + n - 1, so m + n is decreased by 1 in each recursive procedure. so T(p) = T(p - 1) + O(1), it's obvious to know that T(p) = O(p) = O(m + n).","title":"c"},{"location":"06-Heapsort/Problems/#d_1","text":"Similar like EXTRACT-MIN , we first put the new element to Y[m, n] , then youngify the tableau from Y[m, n] . The running time is O(m + n). INSERT(Y, key) Y[m][n] = key YOUNGIFY-INSERT(Y, m, n) YOUNGIFY-INSERT(Y, row, column) largest_row = row largest_column = column if row - 1 >= 1 and Y[row - 1][column] > Y[row][column] largest_row = row - 1 if column - 1 >= 1 and Y[row][column - 1] > Y[largest_row][largest_column] largest_row = row largest_column = column - 1 if largest_row != row or largest_column != column exchange Y[row, column] with Y[largest_row, largest_column] YOUNGIFY-INSERT(Y, largest_row, largest_column)","title":"d"},{"location":"06-Heapsort/Problems/#e_1","text":"The INSERT operation takes O(n + n) = O(n) time, and there are $n^2$ elements, so we can insert each element into the n x n Young tableau, thus the running time is $n^2O(n) = O(n^3)$.","title":"e"},{"location":"06-Heapsort/Problems/#f","text":"We start from Y[m, 1] , and compare Y[m, 1] with target value, if Y[m, 1] is greater than target value, we move to Y[m - 1, 1] , if it's smaller than target value, we move to Y[m, 2] , otherwise, we find the target value. FIND(Y, key) row = m column = 1 while row >= 1 and column <= n if Y[row][column] == key return True else if Y[row][column] < key column += 1 else if Y[row][column] > key row -= 1 return False Similar like YOUNGIFY , it reduces the problem size to (m - 1) * n or m * (n - 1), so the running time is O(m + n).","title":"f"},{"location":"07-Quicksort/7.1-Description-of-quicksort/","text":"7.1 Description of quicksort 7.1-1 7.1-2 When all elements in the array A[p..r] have the same value, it returns r . PARTITION(A, p, r) x = A[r] i = p - 1 same_value_count = 0 for j = p to r - 1 if A[j] <= x if A[j] == x same_value_count = same_value_count + 1 i = i + 1 exchange A[i] with A[j] exchange A[i + 1] with A[r] if same_value_count = r - p return Math.floor((p + 2) / 2) else return i + 1 7.1-3 The PARTITION takes $\\Theta(r - p)$ because it iterates the array from j to p. So the PARTITION on a subarray of size n is $\\Theta(n)$. 7.1-4 QUICKSORT(A, p, r) if p < r q = PARTITION(A, p, r) QUICKSORT(A, p, q - 1) QUICKSORT(A, q + 1, r) PARTITION(A, p, r) x = A[r] i = p - 1 for j = p to r - 1 if A[j] >= x i = i + 1 exchange A[i] with A[j] exchange A[i + 1] with A[r] return i + 1","title":"7.1 Description of quicksort"},{"location":"07-Quicksort/7.1-Description-of-quicksort/#71-description-of-quicksort","text":"","title":"7.1 Description of quicksort"},{"location":"07-Quicksort/7.1-Description-of-quicksort/#71-1","text":"","title":"7.1-1"},{"location":"07-Quicksort/7.1-Description-of-quicksort/#71-2","text":"When all elements in the array A[p..r] have the same value, it returns r . PARTITION(A, p, r) x = A[r] i = p - 1 same_value_count = 0 for j = p to r - 1 if A[j] <= x if A[j] == x same_value_count = same_value_count + 1 i = i + 1 exchange A[i] with A[j] exchange A[i + 1] with A[r] if same_value_count = r - p return Math.floor((p + 2) / 2) else return i + 1","title":"7.1-2"},{"location":"07-Quicksort/7.1-Description-of-quicksort/#71-3","text":"The PARTITION takes $\\Theta(r - p)$ because it iterates the array from j to p. So the PARTITION on a subarray of size n is $\\Theta(n)$.","title":"7.1-3"},{"location":"07-Quicksort/7.1-Description-of-quicksort/#71-4","text":"QUICKSORT(A, p, r) if p < r q = PARTITION(A, p, r) QUICKSORT(A, p, q - 1) QUICKSORT(A, q + 1, r) PARTITION(A, p, r) x = A[r] i = p - 1 for j = p to r - 1 if A[j] >= x i = i + 1 exchange A[i] with A[j] exchange A[i + 1] with A[r] return i + 1","title":"7.1-4"},{"location":"07-Quicksort/7.2-Performance-of-quicksort/","text":"7.2 Performance of quicksort 7.2-1 We start by assuming that this bound holds for all possitive m < n, in particular for m = n - 1, yielding $T(n - 1) \\leq c(n - 1)^2$. Substituting into the recurrence yields: $T(n) = T(n - 1) + \\Theta(n) \\leq c(n - 1)^2 + dn = cn^2 + (d-2c)n + c \\leq cn^2$ where the last step holds as long as $c > \\frac{d}{2}$ and $n \\geq \\frac{c}{2c - d}$. 7.2-2 When all elements of array A have the same value, the PARTITION method returns r , thus it reduces the problem size to n - 1 and 0, which yields the recurrence in the above exercise. So the running time is $\\Theta(n^2)$. 7.2-3 When the array A contains decreasing sorted elements, the PARTITION method returns p , thus it reduces the problem size to 0 and n - 1, same like the above exercise. The running time is $\\Theta(n^2)$. 7.2-4 Suppose there are k exceptions in an almost-sorted array, in INSERTION-SORT , we need to move the k exceptions to right position, each takes at most $O(n)$, so the total running time is O(n + kn). When k is small enough, INSERTION-SORT tends to be linear sort. In QUICKSORT , we need to split the array into two parts, since the array is almost-sorted, it's quite possible that it will create 0 size subarray, thus the running time tends to be $O(n^2)$. So for an almost-sorted array, INSERTION-SORT would tend to beat QUICKSORT . 7.2-5 We have proved it in exercise 4.4-9. When $0 < \\alpha \\leq \\frac{1}{2}$, the minimum depth of a leaf is $\\log_{\\alpha}{\\frac{1}{n}} = -\\log_{\\alpha}{n} = -\\frac{\\lg{n}}{\\lg{\\alpha}}$. The maximum depth is $\\log_{1 - \\alpha}{\\frac{1}{n}} = -\\log_{1 - \\alpha}{n} = -\\frac{\\lg{n}}{\\lg{1 - \\alpha}}$. 7.2-6 The splits at each level quicksort are in the proportion $1 - \\alpha$ and $\\alpha$, thus the probability that PARTITION produces a more balanced split is $\\frac{(1 - \\alpha - \\alpha)n}{n} = 1 - 2\\alpha$.","title":"7.2 Performance of quicksort"},{"location":"07-Quicksort/7.2-Performance-of-quicksort/#72-performance-of-quicksort","text":"","title":"7.2 Performance of quicksort"},{"location":"07-Quicksort/7.2-Performance-of-quicksort/#72-1","text":"We start by assuming that this bound holds for all possitive m < n, in particular for m = n - 1, yielding $T(n - 1) \\leq c(n - 1)^2$. Substituting into the recurrence yields: $T(n) = T(n - 1) + \\Theta(n) \\leq c(n - 1)^2 + dn = cn^2 + (d-2c)n + c \\leq cn^2$ where the last step holds as long as $c > \\frac{d}{2}$ and $n \\geq \\frac{c}{2c - d}$.","title":"7.2-1"},{"location":"07-Quicksort/7.2-Performance-of-quicksort/#72-2","text":"When all elements of array A have the same value, the PARTITION method returns r , thus it reduces the problem size to n - 1 and 0, which yields the recurrence in the above exercise. So the running time is $\\Theta(n^2)$.","title":"7.2-2"},{"location":"07-Quicksort/7.2-Performance-of-quicksort/#72-3","text":"When the array A contains decreasing sorted elements, the PARTITION method returns p , thus it reduces the problem size to 0 and n - 1, same like the above exercise. The running time is $\\Theta(n^2)$.","title":"7.2-3"},{"location":"07-Quicksort/7.2-Performance-of-quicksort/#72-4","text":"Suppose there are k exceptions in an almost-sorted array, in INSERTION-SORT , we need to move the k exceptions to right position, each takes at most $O(n)$, so the total running time is O(n + kn). When k is small enough, INSERTION-SORT tends to be linear sort. In QUICKSORT , we need to split the array into two parts, since the array is almost-sorted, it's quite possible that it will create 0 size subarray, thus the running time tends to be $O(n^2)$. So for an almost-sorted array, INSERTION-SORT would tend to beat QUICKSORT .","title":"7.2-4"},{"location":"07-Quicksort/7.2-Performance-of-quicksort/#72-5","text":"We have proved it in exercise 4.4-9. When $0 < \\alpha \\leq \\frac{1}{2}$, the minimum depth of a leaf is $\\log_{\\alpha}{\\frac{1}{n}} = -\\log_{\\alpha}{n} = -\\frac{\\lg{n}}{\\lg{\\alpha}}$. The maximum depth is $\\log_{1 - \\alpha}{\\frac{1}{n}} = -\\log_{1 - \\alpha}{n} = -\\frac{\\lg{n}}{\\lg{1 - \\alpha}}$.","title":"7.2-5"},{"location":"07-Quicksort/7.2-Performance-of-quicksort/#72-6","text":"The splits at each level quicksort are in the proportion $1 - \\alpha$ and $\\alpha$, thus the probability that PARTITION produces a more balanced split is $\\frac{(1 - \\alpha - \\alpha)n}{n} = 1 - 2\\alpha$.","title":"7.2-6"},{"location":"07-Quicksort/7.3-A-randomized-version-of-quicksort/","text":"7.3 A randomized version of quicksort 7.3-1 The worst-case is a special case, analyzing the expected running time can be more resonable that reflects the complexity of the algorithm. 7.3-2 The worst-case satifies $T(n) = T(n - 1) + \\Theta(1)$, thus $T(n) = \\Theta(n)$. The best case satifies $T(n) = 2T(\\frac{n}{2}) + \\Theta(1)$, thus $T(n) = \\Theta(n)$.","title":"7.3 A randomized version of quicksort"},{"location":"07-Quicksort/7.3-A-randomized-version-of-quicksort/#73-a-randomized-version-of-quicksort","text":"","title":"7.3 A randomized version of quicksort"},{"location":"07-Quicksort/7.3-A-randomized-version-of-quicksort/#73-1","text":"The worst-case is a special case, analyzing the expected running time can be more resonable that reflects the complexity of the algorithm.","title":"7.3-1"},{"location":"07-Quicksort/7.3-A-randomized-version-of-quicksort/#73-2","text":"The worst-case satifies $T(n) = T(n - 1) + \\Theta(1)$, thus $T(n) = \\Theta(n)$. The best case satifies $T(n) = 2T(\\frac{n}{2}) + \\Theta(1)$, thus $T(n) = \\Theta(n)$.","title":"7.3-2"},{"location":"07-Quicksort/7.4-Analysis-of-quicksort/","text":"7.4 Analysis of quicksort 7.4-1 We guess that $T(n) \\geq cn^2$ for some constant c. Substituting this guess into recurrence, we obtain: $$ \\begin{eqnarray} T(n) &=& \\max_{0 \\leq q \\leq n - 1}(T(q) + T(n - q - 1)) + \\Theta(n) \\\\ &\\geq& \\max_{0 \\leq q \\leq n - 1}(cq^2 + c(n - q - 1)^2) + \\Theta(n) \\\\ &=& c\\max_{0 \\leq q \\leq n - 1}(q^2 + (n - q - 1)^2) + \\Theta(n) \\\\ &=& c\\max_{0 \\leq q \\leq n - 1}(2q^2 - 2(n - 1)q + (n - 1)^2) + \\Theta(n) \\end{eqnarray} $$ $f(q) = 2q^2 - 2(n - 1)q + (n - 1)^2$ is monotonically decreasing at $[0, \\frac{n - 1}{2}]$, and monotonically increasing at $[\\frac{n - 1}{2}, n - 1]$. So $\\max_{0 \\leq q \\leq n - 1}(2q^2 - 2(n - 1)q + (n - 1)^2) = f(0) = f(n) = (n - 1)^2$. Thus: $$ \\begin{eqnarray} T(n) &\\geq& c(n - 1)^2 + \\Theta(n) \\\\ &=& c(n - 1)^2 + dn \\\\ &=& cn^2 + (d - 2c)n + c \\\\ &>& cn^2 \\end{eqnarray} $$ where the last step holds as long as $ d \\geq 2c$. So $T(n) = \\Omega(n^2)$ 7.4-2 The recurrence for the best-case is $T(n) = 2T(\\frac{n}{2}) + \\Theta(n)$. Let's solve it with the master method. We have $a = 2, b = 2, f(n) = \\Theta(n)$, and thus we have that $n^{\\log_b{a}} = n^{\\log_2{2}} = n = \\Theta(n)$. Since $f(n) = \\Theta(n)$, we can apply case 2 of the master theorem and conclude that the solution is $T(n) = \\Theta(n\\lg{n})$. Thus, $T(n) = \\Omega(n\\lg{n})$. 7.4-3 We've already knew it in the first exercise. 7.4-4 We have: $$ \\begin{eqnarray} E[X] &=& \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n \\frac{2}{j - i + 1} \\\\ &=& \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{2}{k + 1} \\\\ &\\geq& \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{2}{2k} \\\\ &=& \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{1}{k} \\\\ &=& \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{i} \\frac{1}{k} \\\\ &=& \\sum_{i = 1}^{n - 1}(\\ln{i} + O(1)) & \\text{(A.7)} \\\\ &=& \\ln{((n - 1)!)} + (n - 1)O(1) \\\\ &=& \\Omega(n\\lg{n}) + O(n) & \\text{(Exercise 3.2-3)} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ 7.4-5 Quicksort splits the array into two parts, let's say $T(n) = T(\\alpha{n}) + T((1 - \\alpha)n) + \\Theta(n)$. In exercise 7.2-5, we know when $0 \\leq \\alpha \\leq \\frac{1}{2}$, the minimum depth of a leaf is $\\log_{\\alpha}{\\frac{1}{n}}$ and the maximum depth is $\\log_{1 - \\alpha}{\\frac{1}{n}}$, when $\\frac{1}{2} < \\alpha < 1$, it's opposite. To make it simple, let the recursion tree depth be $\\log_{\\beta}n$, since the new algorithm terminates when a subarray contains fewer than k elements, so we have $\\frac{n}{\\beta^{i}} = k$ and $i = \\log_{\\beta}{\\frac{n}{k}}$, so the depth is $O(\\lg{\\frac{n}{k}})$. Since each recursion tree level costs $O(n)$, the running time of quicksort is $O(n\\lg{\\frac{n}{k}})$. The last step is sorting the array by insertion sort. Let's assume n is exactly divisible by k, thus, after quicksort, the $\\frac{n}{k}$ subarrays (each with length $\\frac{n}{k}$) is sorted, but it's not sorted within the subarray. It requires $O(k^2)$ to sort each subarray, so it requires $\\frac{n}{k}k^2 = nk$ to sort entir array. So the running time is $O(nk + n\\lg{\\frac{n}{k}})$. In order to beat quicksort, we need to find a k such that $O(nk + n\\lg{\\frac{n}{k}}) < O(n\\lg{n})$, let's compare $nk + n\\lg{\\frac{n}{k}}$ and $n\\lg{n}$. $$ \\begin{eqnarray} nk + n\\lg{\\frac{n}{k}} - n\\lg{n} &=& nk + n\\lg{\\frac{1}{k}} \\\\ &=& nk - n\\lg{k} \\\\ &=& n(k - \\lg{k}) \\end{eqnarray} $$ But k grows faster than $\\lg{k}$, so we cannot pick a good k here. Thus we can multiply a constant c to $n\\lg{n}$. Thus: $$ \\begin{eqnarray} nk + n\\lg{\\frac{n}{k}} - cn\\lg{n} &=& nk + n\\lg{n} - n\\lg{k} - cn\\lg{n} \\\\ &=& n(\\lg{2^k} + \\lg{n} - \\lg{k} - \\lg{n^c}) \\\\ &=& n\\lg{\\frac{2^k}{kn^{c - 1}}} \\end{eqnarray} $$ So we only need to find a k that satifies $\\frac{2^k}{kn^{c - 1}} < 1$, which is possible. In practice, we need to do some tests to find a proper k. 7.4-6","title":"7.4 Analysis of quicksort"},{"location":"07-Quicksort/7.4-Analysis-of-quicksort/#74-analysis-of-quicksort","text":"","title":"7.4 Analysis of quicksort"},{"location":"07-Quicksort/7.4-Analysis-of-quicksort/#74-1","text":"We guess that $T(n) \\geq cn^2$ for some constant c. Substituting this guess into recurrence, we obtain: $$ \\begin{eqnarray} T(n) &=& \\max_{0 \\leq q \\leq n - 1}(T(q) + T(n - q - 1)) + \\Theta(n) \\\\ &\\geq& \\max_{0 \\leq q \\leq n - 1}(cq^2 + c(n - q - 1)^2) + \\Theta(n) \\\\ &=& c\\max_{0 \\leq q \\leq n - 1}(q^2 + (n - q - 1)^2) + \\Theta(n) \\\\ &=& c\\max_{0 \\leq q \\leq n - 1}(2q^2 - 2(n - 1)q + (n - 1)^2) + \\Theta(n) \\end{eqnarray} $$ $f(q) = 2q^2 - 2(n - 1)q + (n - 1)^2$ is monotonically decreasing at $[0, \\frac{n - 1}{2}]$, and monotonically increasing at $[\\frac{n - 1}{2}, n - 1]$. So $\\max_{0 \\leq q \\leq n - 1}(2q^2 - 2(n - 1)q + (n - 1)^2) = f(0) = f(n) = (n - 1)^2$. Thus: $$ \\begin{eqnarray} T(n) &\\geq& c(n - 1)^2 + \\Theta(n) \\\\ &=& c(n - 1)^2 + dn \\\\ &=& cn^2 + (d - 2c)n + c \\\\ &>& cn^2 \\end{eqnarray} $$ where the last step holds as long as $ d \\geq 2c$. So $T(n) = \\Omega(n^2)$","title":"7.4-1"},{"location":"07-Quicksort/7.4-Analysis-of-quicksort/#74-2","text":"The recurrence for the best-case is $T(n) = 2T(\\frac{n}{2}) + \\Theta(n)$. Let's solve it with the master method. We have $a = 2, b = 2, f(n) = \\Theta(n)$, and thus we have that $n^{\\log_b{a}} = n^{\\log_2{2}} = n = \\Theta(n)$. Since $f(n) = \\Theta(n)$, we can apply case 2 of the master theorem and conclude that the solution is $T(n) = \\Theta(n\\lg{n})$. Thus, $T(n) = \\Omega(n\\lg{n})$.","title":"7.4-2"},{"location":"07-Quicksort/7.4-Analysis-of-quicksort/#74-3","text":"We've already knew it in the first exercise.","title":"7.4-3"},{"location":"07-Quicksort/7.4-Analysis-of-quicksort/#74-4","text":"We have: $$ \\begin{eqnarray} E[X] &=& \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n \\frac{2}{j - i + 1} \\\\ &=& \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{2}{k + 1} \\\\ &\\geq& \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{2}{2k} \\\\ &=& \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{n - i} \\frac{1}{k} \\\\ &=& \\sum_{i = 1}^{n - 1}\\sum_{k = 1}^{i} \\frac{1}{k} \\\\ &=& \\sum_{i = 1}^{n - 1}(\\ln{i} + O(1)) & \\text{(A.7)} \\\\ &=& \\ln{((n - 1)!)} + (n - 1)O(1) \\\\ &=& \\Omega(n\\lg{n}) + O(n) & \\text{(Exercise 3.2-3)} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$","title":"7.4-4"},{"location":"07-Quicksort/7.4-Analysis-of-quicksort/#74-5","text":"Quicksort splits the array into two parts, let's say $T(n) = T(\\alpha{n}) + T((1 - \\alpha)n) + \\Theta(n)$. In exercise 7.2-5, we know when $0 \\leq \\alpha \\leq \\frac{1}{2}$, the minimum depth of a leaf is $\\log_{\\alpha}{\\frac{1}{n}}$ and the maximum depth is $\\log_{1 - \\alpha}{\\frac{1}{n}}$, when $\\frac{1}{2} < \\alpha < 1$, it's opposite. To make it simple, let the recursion tree depth be $\\log_{\\beta}n$, since the new algorithm terminates when a subarray contains fewer than k elements, so we have $\\frac{n}{\\beta^{i}} = k$ and $i = \\log_{\\beta}{\\frac{n}{k}}$, so the depth is $O(\\lg{\\frac{n}{k}})$. Since each recursion tree level costs $O(n)$, the running time of quicksort is $O(n\\lg{\\frac{n}{k}})$. The last step is sorting the array by insertion sort. Let's assume n is exactly divisible by k, thus, after quicksort, the $\\frac{n}{k}$ subarrays (each with length $\\frac{n}{k}$) is sorted, but it's not sorted within the subarray. It requires $O(k^2)$ to sort each subarray, so it requires $\\frac{n}{k}k^2 = nk$ to sort entir array. So the running time is $O(nk + n\\lg{\\frac{n}{k}})$. In order to beat quicksort, we need to find a k such that $O(nk + n\\lg{\\frac{n}{k}}) < O(n\\lg{n})$, let's compare $nk + n\\lg{\\frac{n}{k}}$ and $n\\lg{n}$. $$ \\begin{eqnarray} nk + n\\lg{\\frac{n}{k}} - n\\lg{n} &=& nk + n\\lg{\\frac{1}{k}} \\\\ &=& nk - n\\lg{k} \\\\ &=& n(k - \\lg{k}) \\end{eqnarray} $$ But k grows faster than $\\lg{k}$, so we cannot pick a good k here. Thus we can multiply a constant c to $n\\lg{n}$. Thus: $$ \\begin{eqnarray} nk + n\\lg{\\frac{n}{k}} - cn\\lg{n} &=& nk + n\\lg{n} - n\\lg{k} - cn\\lg{n} \\\\ &=& n(\\lg{2^k} + \\lg{n} - \\lg{k} - \\lg{n^c}) \\\\ &=& n\\lg{\\frac{2^k}{kn^{c - 1}}} \\end{eqnarray} $$ So we only need to find a k that satifies $\\frac{2^k}{kn^{c - 1}} < 1$, which is possible. In practice, we need to do some tests to find a proper k.","title":"7.4-5"},{"location":"07-Quicksort/7.4-Analysis-of-quicksort/#74-6","text":"","title":"7.4-6"},{"location":"07-Quicksort/Problems/","text":"Problems 7-1 a b We repeat j = j - 1 from r + 1 and repeat i = i + 1 from p - 1 , so i moves from left to right and j moves from right to left, the while loop terminates when i >= j , thus, it will not access an element of A outside the subarray A[p..r] . But will it keep increasing i such that i is bigger than r ? No, it's not possible, because we choose A[p] as pivot. So in the first iteration, i stops at p , and if i < j , we exchange A[i] with A[j] , so the pivot is exchanged to right, thus it makes sure it stops keep increasing i when it meets the pivot in right. c Suppose the array is in increasing order, so it keeps increasing j until j = p because A[p] <= x . So j could be p . In the while loop, j will be r first, because there are at least two elements in the array, it will keep increasing j or exchange A[i] with A[j] , if it increase j , we get j < r , if we exchange A[i] with A[j] , then it also increase j in the next iteration, either makes j > r . So p <= j < r . d j stopes decreasing when A[j] <= x and i stops increasing when A[i] >= x , after exchanging A[i] with A[j] , it makes sure every element of A[p..i] is less than or equal to every element of A[j..r] . When it terminates, we have j - i <= 1 , so every element of A[p..j] is less than or equal to every element of A[j + 1..r] . e QUICKSORT(A, p, r) if p < r q = HOARE-PARTITION(A, p, r) QUICKSORT(A, p, q) QUICKSORT(A, q + 1, r) HOARE-PARTITION(A, p, r) x = A[p] i = p - 1 j = r + 1 while True repeat j = j - 1 until A[j] <= x repeat i = i + 1 until A[i] >= x if i < j exchange A[i] with A[j] else return j 7-2 a If all elements values are equal, then randomize the array won't make a difference. In exercise 7.2-2 we know the running time is $\\Theta(n^2)$. b PARTITION'(A, p, r) x = A[r] i = p - 1 t = p - 1 for j = p to r - 1 if A[j] < x t = t + 1 i = i + 1 exchange A[t] with A[i] if t != j: exchange A[j] with A[i] else if A[j] == x t = t + 1 exchange A[t] with A[j] exchange A[t + 1] with A[r] return i + 1, t + 1 The running time is obviously $\\Theta(r - p)$. c RANDOMIZED-QUICKSORT'(A, p, r) if p < r q, t = RANDOMIZED-PARTITION'(A, p r) RANDOMIZED-QUICKSORT'(A, p, q - 1) RANDOMIZED-QUICKSORT'(A, t + 1, r) RANDOMIZED-PARTITION'(A, p, r) i = RANDOM(p, r) exchange A[r] with A[i] return PARTITION'(A, p, r) QUICKSORT'(A, p, r) if p < r q, t = RANDOMIZED-PARTITION'(A, p r) QUICKSORT'(A, p, q - 1) QUICKSORT'(A, t + 1, r) d If we use QUICKSORT' , then the worst-case becomes a best-case. If all elements of the array are the same, then the PARTITION' on the whole array splits the problem size to [p, p - 1] and [r + 1, r] , then the subproblem terminates. The running time is $\\Theta(n)$. So we can avoid the assumption that all elements are distinct, because if an array with size n contains duplicate elements, the running time is less than the running time of the array with distinct elements. 7-3 a We choose the last element as pivot, but we randomize the array before that. It has the probability $\\frac{1}{n}$ that an element will be put to the last position in array. So any particular element is chosen as the pivot is $\\frac{1}{n}$. And $E[X_i] = \\frac{1}{n}$. b First let's check the expected running time when element i is chosen as pivot. The PARTITION method takes $\\Theta(n)$, and the problem size is splited to [p, i - 1] and [i + 1, r] , the running time of the two subproblems are $T(i - 1 - p + 1) = T(i - p)$ and $T(r - (i + 1) + 1) = T(r - i)$. Thus the running time is $E[X_i(T(i - p) + T(r - i) + \\Theta(n))]$, where p = 1, r = n. Thus $E[T(n)] = E[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))]$. c $$ \\begin{eqnarray} E[T(n)] &=& E[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))] \\\\ &=& E[\\sum_{q = 1}^n \\frac{1}{n}(T(q - 1) + T(n - q) + \\Theta(n))] \\\\ &=& \\frac{1}{n}E[T(0) + T(n - 1) + \\Theta(n) + T(1) + T(n - 2) + \\Theta(n) + \\ldots + T(n - 1) + T(0) + \\Theta(n)] \\\\ &=& \\frac{1}{n}E[(T(0) + T(1) + \\ldots + T(n - 1)) + (T(n - 1) + T(n - 2) + \\ldots + T(0)) + n\\Theta(n)] \\\\ &=& \\frac{1}{n}E[2\\sum_{q = 0}^{n - 1}T(q) + n\\Theta(n)] \\\\ &=& \\frac{2}{n}E[\\sum_{q = 0}^{n - 1}T(q)] + \\Theta(n) \\\\ &=& \\frac{2}{n}\\sum_{q = 0}^{n - 1}E[T(q)] + \\Theta(n) \\\\ &=& \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\frac{2}{n}(E[T(0)] + E[T(1)]) + \\Theta(n) \\\\ &=& \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\Theta(n) \\end{eqnarray} $$ d $$ \\begin{eqnarray} \\sum_{k = 2}^{n - 1} k\\lg{k} &=& \\sum_{k = 2}^{\\lceil \\frac{n}{2} \\rceil - 1} k\\lg{k} + \\sum_{k = \\lceil \\frac{n}{2} \\rceil}^{n - 1} k\\lg{k} \\\\ &<& \\sum_{k = 2}^{\\frac{n}{2} + 1 - 1} k\\lg{k} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{k} \\\\ &=& \\sum_{k = 2}^{\\frac{n}{2}} k\\lg{k} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{k} \\\\ &\\leq& \\sum_{k = 2}^{\\frac{n}{2}} k\\lg{\\frac{n}{2}} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{n} \\\\ &=& (\\lg{n} - 1)\\frac{(2 + \\frac{n}{2})(\\frac{n}{2} - 2 + 1)}{2} + \\lg{n}\\frac{(\\frac{n}{2} + n - 1)(n - 1 - \\frac{n}{2} + 1)}{2} \\\\ &=& (\\lg{n} - 1)(\\frac{n^2}{8} + \\frac{n}{4} - 1) + \\lg{n}(\\frac{3n^2}{8} - \\frac{n}{4}) \\\\ &=& \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 -\\lg{n} - \\frac{n}{4} + 1 \\\\ &<=& \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 - \\lg3 - \\frac{3}{4} + 1 \\\\ &<& \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 \\end{eqnarray} $$ e We stary by assuming that $E[T(n)] \\leq an\\lg{n}$ for all positive m < n, yielding $E[T(q)] \\leq aq\\lg{q}$. Substituting into the equation: $$ \\begin{eqnarray} E[T(n)]&=& \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\Theta(n) \\\\ &\\leq& \\frac{2}{n}\\sum_{q = 2}^{n - 1} aq\\lg{q} + \\Theta(n) \\\\ &=& \\frac{2a}{n}\\sum_{q = 2}^{n - 1} q\\lg{q} + \\Theta(n) \\\\ &\\leq& \\frac{2a}{n}(\\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2) + \\Theta(n) \\\\ &=& an\\lg{n} - \\frac{a}{4}n + \\Theta(n) \\\\ &\\leq& an\\lg{n} - \\frac{a}{4}n + cn \\\\ &=& an\\lg{n} - (\\frac{a}{4} - c)n \\\\ &\\leq& an\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $\\frac{a}{4} \\geq c$. Thus $E[T(n)] = \\Theta(n\\lg{n})$. 7-4 a The PARTITION method splits the array into two parts, then it sorts the left part first. At last it assigns the right part start index as the new value of p, so in the next iteration, it starts to sort the right part. Thus the whole array is sorted. b For example, when all elements in the array are same. The PARTITION method returns n, so the size of subproblems are n - 1 and 0, and it continues to call TAIL-RECURSIVE-QUICKSORT with size n - 1, but each recursive call only reduces the problem size by 1, thus the stack depth becomes $\\Theta(n)$. c After we split the array into two parts, we call TAIL-RECURSIVE-QUICKSORT on the smaller part. The max size of smaller part is $\\frac{n}{2}$, thus the max stack depth is $\\Theta(\\lg{n})$. TAIL-RECURSIVE-QUICKSORT(A, p, r) while p < r // Partition and sort left subarray. q = PARTITION(A, p, r) if q - p > r - q TAIL-RECURSIVE-QUICKSORT(A, q + 1, r) r = q - 1 else TAIL-RECURSIVE-QUICKSORT(A, p, q - 1) p = q + 1 7-5 a If an element at index i is chosen as median, we need to pick one element in A[1..i - 1] and one element in A[i + 1..n] , and the picking order is not unique, so there are $A_3^{3}$ permutations. Thus the probability is $p_i = \\frac{(i - 1)(n - i)A_3^{3}}{A_n{3}} = \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)}$. b The $p_i$ of oridinary implementation is $\\frac{1}{n}$. So: $$ \\begin{eqnarray} \\frac{p_{\\lfloor \\frac{n + 1}{2} \\rfloor}\\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)}}{\\frac{1}{n}} &=& \\frac{\\lim_{n \\to \\infty}\\frac{6(\\lfloor \\frac{n + 1}{2} \\rfloor - 1)(n - \\lfloor \\frac{n + 1}{2} \\rfloor)}{n(n - 1)(n - 2)}}{\\frac{1}{n}} \\\\ &=& \\lim_{n \\to \\infty}\\frac{6(\\frac{n + 1}{2} - 1)(n - \\frac{n + 1}{2})}{(n - 1)(n - 2)} \\\\ &=& \\lim_{n \\to \\infty}\\frac{6(\\frac{n - 1}{2})^2}{(n - 1)(n - 2)} \\\\ &=& \\frac{3}{2} \\end{eqnarray} $$ c The likehood of getting a good split of the ordinary implementation is: $$ \\begin{eqnarray} \\lim_{n \\to \\infty} \\sum_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{1}{n} &=& \\lim_{n \\to \\infty} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{1}{n} di \\quad (\\int \\frac{1}{n} di = \\frac{i}{n}) \\\\ &=& \\lim_{n \\to \\infty} \\frac{\\frac{2n}{3}}{n} - \\frac{\\frac{n}{3}}{n} \\\\ &=& \\lim_{n \\to \\infty} (\\frac{2}{3} - \\frac{1}{3}) \\\\ &=& \\frac{1}{3} \\end{eqnarray} $$ The likehood of getting a good split of the new implementation is: $$ \\begin{eqnarray} \\lim_{n \\to \\infty} \\sum_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)} &=& \\lim_{n \\to \\infty} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)} di \\\\ &=& \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} (i - 1)(n - i) di \\quad (\\int (i - 1)(n - i) di = -\\frac{1}{3}i^3 + \\frac{n + 1}{2}i^2 - ni) \\\\ &=& \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)} (-\\frac{1}{3}(\\frac{2n}{3})^3 + \\frac{n + 1}{2}(\\frac{2n}{3})^2 - n\\frac{2n}{3} - (-\\frac{1}{3}(\\frac{n}{3})^3 + \\frac{n + 1}{2}(\\frac{n}{3})^2 - n\\frac{n}{3})) \\\\ &=& \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)}(-\\frac{7}{81}n^3 + \\frac{1}{6}n^3 - \\frac{1}{6}n^2) \\\\ &=& 6(-\\frac{7}{81} + \\frac{1}{6}) \\\\ &=& \\frac{13}{27} \\end{eqnarray} $$ And the amount of improvement is $\\frac{\\frac{13}{27}}{\\frac{1}{3}} = \\frac{13}{9}$. d The best-case of quicksort happens when the middle element is chosen as the pivot. And the running time is $\\Omega(n\\lg{n})$. Event the median-of-3 method picks the middle element as the pivot each time, the running time is still $\\Omega(n\\lg{n})$, thus it affects only the constant factor. 7-6 a The idea is similar like problem 7-2. Given an interval as pivot, we treat the intervals which overlap with the pivot are \"equal element values\". The PARTITION method returns two indices q and t, where $p \\leq q \\leq t \\leq r$, such that: all intervals of A[q..t] overlap with A[t] , the right ending of each interval of A[p..q - 1] is less than the left ending of A[t] , the left ending of each interval of A[t + 1..r] is greater than the right ending of A[t] . FUZZY-SORTING-OF-INTERVALS(A, p, r) if p < r q, t = RANDOMIZED-PARTITION(A, p r) FUZZY-SORTING-OF-INTERVALS(A, p, q - 1) FUZZY-SORTING-OF-INTERVALS(A, t + 1, r) RANDOMIZED-PARTITION(A, p, r) i = RANDOM(p, r) exchange A[r] with A[i] return PARTITION(A, p, r) PARTITION(A, p, r) x = A[r] i = p - 1 t = p - 1 for j = p to r - 1 if A[j] is before x t = t + 1 i = i + 1 exchange A[t] with A[i] if t != j: exchange A[j] with A[i] else if A[j] overlaps with x t = t + 1 exchange A[t] with A[j] exchange A[t + 1] with A[r] return i + 1, t + 1 b It's just a variation of problem 7-2, expected we use different comparision checks, so the expected running time is $\\Theta(n\\lg{n})$ in general. When all of the intervals overlap, we treat them like all of the intervals are equal, thus like the analysis in problem 7-2, the expected running time is $\\Theta(n)$.","title":"Problems"},{"location":"07-Quicksort/Problems/#problems","text":"","title":"Problems"},{"location":"07-Quicksort/Problems/#7-1","text":"","title":"7-1"},{"location":"07-Quicksort/Problems/#a","text":"","title":"a"},{"location":"07-Quicksort/Problems/#b","text":"We repeat j = j - 1 from r + 1 and repeat i = i + 1 from p - 1 , so i moves from left to right and j moves from right to left, the while loop terminates when i >= j , thus, it will not access an element of A outside the subarray A[p..r] . But will it keep increasing i such that i is bigger than r ? No, it's not possible, because we choose A[p] as pivot. So in the first iteration, i stops at p , and if i < j , we exchange A[i] with A[j] , so the pivot is exchanged to right, thus it makes sure it stops keep increasing i when it meets the pivot in right.","title":"b"},{"location":"07-Quicksort/Problems/#c","text":"Suppose the array is in increasing order, so it keeps increasing j until j = p because A[p] <= x . So j could be p . In the while loop, j will be r first, because there are at least two elements in the array, it will keep increasing j or exchange A[i] with A[j] , if it increase j , we get j < r , if we exchange A[i] with A[j] , then it also increase j in the next iteration, either makes j > r . So p <= j < r .","title":"c"},{"location":"07-Quicksort/Problems/#d","text":"j stopes decreasing when A[j] <= x and i stops increasing when A[i] >= x , after exchanging A[i] with A[j] , it makes sure every element of A[p..i] is less than or equal to every element of A[j..r] . When it terminates, we have j - i <= 1 , so every element of A[p..j] is less than or equal to every element of A[j + 1..r] .","title":"d"},{"location":"07-Quicksort/Problems/#e","text":"QUICKSORT(A, p, r) if p < r q = HOARE-PARTITION(A, p, r) QUICKSORT(A, p, q) QUICKSORT(A, q + 1, r) HOARE-PARTITION(A, p, r) x = A[p] i = p - 1 j = r + 1 while True repeat j = j - 1 until A[j] <= x repeat i = i + 1 until A[i] >= x if i < j exchange A[i] with A[j] else return j","title":"e"},{"location":"07-Quicksort/Problems/#7-2","text":"","title":"7-2"},{"location":"07-Quicksort/Problems/#a_1","text":"If all elements values are equal, then randomize the array won't make a difference. In exercise 7.2-2 we know the running time is $\\Theta(n^2)$.","title":"a"},{"location":"07-Quicksort/Problems/#b_1","text":"PARTITION'(A, p, r) x = A[r] i = p - 1 t = p - 1 for j = p to r - 1 if A[j] < x t = t + 1 i = i + 1 exchange A[t] with A[i] if t != j: exchange A[j] with A[i] else if A[j] == x t = t + 1 exchange A[t] with A[j] exchange A[t + 1] with A[r] return i + 1, t + 1 The running time is obviously $\\Theta(r - p)$.","title":"b"},{"location":"07-Quicksort/Problems/#c_1","text":"RANDOMIZED-QUICKSORT'(A, p, r) if p < r q, t = RANDOMIZED-PARTITION'(A, p r) RANDOMIZED-QUICKSORT'(A, p, q - 1) RANDOMIZED-QUICKSORT'(A, t + 1, r) RANDOMIZED-PARTITION'(A, p, r) i = RANDOM(p, r) exchange A[r] with A[i] return PARTITION'(A, p, r) QUICKSORT'(A, p, r) if p < r q, t = RANDOMIZED-PARTITION'(A, p r) QUICKSORT'(A, p, q - 1) QUICKSORT'(A, t + 1, r)","title":"c"},{"location":"07-Quicksort/Problems/#d_1","text":"If we use QUICKSORT' , then the worst-case becomes a best-case. If all elements of the array are the same, then the PARTITION' on the whole array splits the problem size to [p, p - 1] and [r + 1, r] , then the subproblem terminates. The running time is $\\Theta(n)$. So we can avoid the assumption that all elements are distinct, because if an array with size n contains duplicate elements, the running time is less than the running time of the array with distinct elements.","title":"d"},{"location":"07-Quicksort/Problems/#7-3","text":"","title":"7-3"},{"location":"07-Quicksort/Problems/#a_2","text":"We choose the last element as pivot, but we randomize the array before that. It has the probability $\\frac{1}{n}$ that an element will be put to the last position in array. So any particular element is chosen as the pivot is $\\frac{1}{n}$. And $E[X_i] = \\frac{1}{n}$.","title":"a"},{"location":"07-Quicksort/Problems/#b_2","text":"First let's check the expected running time when element i is chosen as pivot. The PARTITION method takes $\\Theta(n)$, and the problem size is splited to [p, i - 1] and [i + 1, r] , the running time of the two subproblems are $T(i - 1 - p + 1) = T(i - p)$ and $T(r - (i + 1) + 1) = T(r - i)$. Thus the running time is $E[X_i(T(i - p) + T(r - i) + \\Theta(n))]$, where p = 1, r = n. Thus $E[T(n)] = E[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))]$.","title":"b"},{"location":"07-Quicksort/Problems/#c_2","text":"$$ \\begin{eqnarray} E[T(n)] &=& E[\\sum_{q = 1}^n X_q(T(q - 1) + T(n - q) + \\Theta(n))] \\\\ &=& E[\\sum_{q = 1}^n \\frac{1}{n}(T(q - 1) + T(n - q) + \\Theta(n))] \\\\ &=& \\frac{1}{n}E[T(0) + T(n - 1) + \\Theta(n) + T(1) + T(n - 2) + \\Theta(n) + \\ldots + T(n - 1) + T(0) + \\Theta(n)] \\\\ &=& \\frac{1}{n}E[(T(0) + T(1) + \\ldots + T(n - 1)) + (T(n - 1) + T(n - 2) + \\ldots + T(0)) + n\\Theta(n)] \\\\ &=& \\frac{1}{n}E[2\\sum_{q = 0}^{n - 1}T(q) + n\\Theta(n)] \\\\ &=& \\frac{2}{n}E[\\sum_{q = 0}^{n - 1}T(q)] + \\Theta(n) \\\\ &=& \\frac{2}{n}\\sum_{q = 0}^{n - 1}E[T(q)] + \\Theta(n) \\\\ &=& \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\frac{2}{n}(E[T(0)] + E[T(1)]) + \\Theta(n) \\\\ &=& \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\Theta(n) \\end{eqnarray} $$","title":"c"},{"location":"07-Quicksort/Problems/#d_2","text":"$$ \\begin{eqnarray} \\sum_{k = 2}^{n - 1} k\\lg{k} &=& \\sum_{k = 2}^{\\lceil \\frac{n}{2} \\rceil - 1} k\\lg{k} + \\sum_{k = \\lceil \\frac{n}{2} \\rceil}^{n - 1} k\\lg{k} \\\\ &<& \\sum_{k = 2}^{\\frac{n}{2} + 1 - 1} k\\lg{k} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{k} \\\\ &=& \\sum_{k = 2}^{\\frac{n}{2}} k\\lg{k} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{k} \\\\ &\\leq& \\sum_{k = 2}^{\\frac{n}{2}} k\\lg{\\frac{n}{2}} + \\sum_{k = \\frac{n}{2}}^{n - 1} k\\lg{n} \\\\ &=& (\\lg{n} - 1)\\frac{(2 + \\frac{n}{2})(\\frac{n}{2} - 2 + 1)}{2} + \\lg{n}\\frac{(\\frac{n}{2} + n - 1)(n - 1 - \\frac{n}{2} + 1)}{2} \\\\ &=& (\\lg{n} - 1)(\\frac{n^2}{8} + \\frac{n}{4} - 1) + \\lg{n}(\\frac{3n^2}{8} - \\frac{n}{4}) \\\\ &=& \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 -\\lg{n} - \\frac{n}{4} + 1 \\\\ &<=& \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 - \\lg3 - \\frac{3}{4} + 1 \\\\ &<& \\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2 \\end{eqnarray} $$","title":"d"},{"location":"07-Quicksort/Problems/#e_1","text":"We stary by assuming that $E[T(n)] \\leq an\\lg{n}$ for all positive m < n, yielding $E[T(q)] \\leq aq\\lg{q}$. Substituting into the equation: $$ \\begin{eqnarray} E[T(n)]&=& \\frac{2}{n}\\sum_{q = 2}^{n - 1}E[T(q)] + \\Theta(n) \\\\ &\\leq& \\frac{2}{n}\\sum_{q = 2}^{n - 1} aq\\lg{q} + \\Theta(n) \\\\ &=& \\frac{2a}{n}\\sum_{q = 2}^{n - 1} q\\lg{q} + \\Theta(n) \\\\ &\\leq& \\frac{2a}{n}(\\frac{1}{2}n^2\\lg{n} - \\frac{1}{8}n^2) + \\Theta(n) \\\\ &=& an\\lg{n} - \\frac{a}{4}n + \\Theta(n) \\\\ &\\leq& an\\lg{n} - \\frac{a}{4}n + cn \\\\ &=& an\\lg{n} - (\\frac{a}{4} - c)n \\\\ &\\leq& an\\lg{n} \\end{eqnarray} $$ where the last step holds as long as $\\frac{a}{4} \\geq c$. Thus $E[T(n)] = \\Theta(n\\lg{n})$.","title":"e"},{"location":"07-Quicksort/Problems/#7-4","text":"","title":"7-4"},{"location":"07-Quicksort/Problems/#a_3","text":"The PARTITION method splits the array into two parts, then it sorts the left part first. At last it assigns the right part start index as the new value of p, so in the next iteration, it starts to sort the right part. Thus the whole array is sorted.","title":"a"},{"location":"07-Quicksort/Problems/#b_3","text":"For example, when all elements in the array are same. The PARTITION method returns n, so the size of subproblems are n - 1 and 0, and it continues to call TAIL-RECURSIVE-QUICKSORT with size n - 1, but each recursive call only reduces the problem size by 1, thus the stack depth becomes $\\Theta(n)$.","title":"b"},{"location":"07-Quicksort/Problems/#c_3","text":"After we split the array into two parts, we call TAIL-RECURSIVE-QUICKSORT on the smaller part. The max size of smaller part is $\\frac{n}{2}$, thus the max stack depth is $\\Theta(\\lg{n})$. TAIL-RECURSIVE-QUICKSORT(A, p, r) while p < r // Partition and sort left subarray. q = PARTITION(A, p, r) if q - p > r - q TAIL-RECURSIVE-QUICKSORT(A, q + 1, r) r = q - 1 else TAIL-RECURSIVE-QUICKSORT(A, p, q - 1) p = q + 1","title":"c"},{"location":"07-Quicksort/Problems/#7-5","text":"","title":"7-5"},{"location":"07-Quicksort/Problems/#a_4","text":"If an element at index i is chosen as median, we need to pick one element in A[1..i - 1] and one element in A[i + 1..n] , and the picking order is not unique, so there are $A_3^{3}$ permutations. Thus the probability is $p_i = \\frac{(i - 1)(n - i)A_3^{3}}{A_n{3}} = \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)}$.","title":"a"},{"location":"07-Quicksort/Problems/#b_4","text":"The $p_i$ of oridinary implementation is $\\frac{1}{n}$. So: $$ \\begin{eqnarray} \\frac{p_{\\lfloor \\frac{n + 1}{2} \\rfloor}\\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)}}{\\frac{1}{n}} &=& \\frac{\\lim_{n \\to \\infty}\\frac{6(\\lfloor \\frac{n + 1}{2} \\rfloor - 1)(n - \\lfloor \\frac{n + 1}{2} \\rfloor)}{n(n - 1)(n - 2)}}{\\frac{1}{n}} \\\\ &=& \\lim_{n \\to \\infty}\\frac{6(\\frac{n + 1}{2} - 1)(n - \\frac{n + 1}{2})}{(n - 1)(n - 2)} \\\\ &=& \\lim_{n \\to \\infty}\\frac{6(\\frac{n - 1}{2})^2}{(n - 1)(n - 2)} \\\\ &=& \\frac{3}{2} \\end{eqnarray} $$","title":"b"},{"location":"07-Quicksort/Problems/#c_4","text":"The likehood of getting a good split of the ordinary implementation is: $$ \\begin{eqnarray} \\lim_{n \\to \\infty} \\sum_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{1}{n} &=& \\lim_{n \\to \\infty} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{1}{n} di \\quad (\\int \\frac{1}{n} di = \\frac{i}{n}) \\\\ &=& \\lim_{n \\to \\infty} \\frac{\\frac{2n}{3}}{n} - \\frac{\\frac{n}{3}}{n} \\\\ &=& \\lim_{n \\to \\infty} (\\frac{2}{3} - \\frac{1}{3}) \\\\ &=& \\frac{1}{3} \\end{eqnarray} $$ The likehood of getting a good split of the new implementation is: $$ \\begin{eqnarray} \\lim_{n \\to \\infty} \\sum_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)} &=& \\lim_{n \\to \\infty} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} \\frac{6(i - 1)(n - i)}{n(n - 1)(n - 2)} di \\\\ &=& \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)} \\int_{\\frac{n}{3}}^{\\frac{2n}{3}} (i - 1)(n - i) di \\quad (\\int (i - 1)(n - i) di = -\\frac{1}{3}i^3 + \\frac{n + 1}{2}i^2 - ni) \\\\ &=& \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)} (-\\frac{1}{3}(\\frac{2n}{3})^3 + \\frac{n + 1}{2}(\\frac{2n}{3})^2 - n\\frac{2n}{3} - (-\\frac{1}{3}(\\frac{n}{3})^3 + \\frac{n + 1}{2}(\\frac{n}{3})^2 - n\\frac{n}{3})) \\\\ &=& \\lim_{n \\to \\infty} \\frac{6}{n(n - 1)(n - 2)}(-\\frac{7}{81}n^3 + \\frac{1}{6}n^3 - \\frac{1}{6}n^2) \\\\ &=& 6(-\\frac{7}{81} + \\frac{1}{6}) \\\\ &=& \\frac{13}{27} \\end{eqnarray} $$ And the amount of improvement is $\\frac{\\frac{13}{27}}{\\frac{1}{3}} = \\frac{13}{9}$.","title":"c"},{"location":"07-Quicksort/Problems/#d_3","text":"The best-case of quicksort happens when the middle element is chosen as the pivot. And the running time is $\\Omega(n\\lg{n})$. Event the median-of-3 method picks the middle element as the pivot each time, the running time is still $\\Omega(n\\lg{n})$, thus it affects only the constant factor.","title":"d"},{"location":"07-Quicksort/Problems/#7-6","text":"","title":"7-6"},{"location":"07-Quicksort/Problems/#a_5","text":"The idea is similar like problem 7-2. Given an interval as pivot, we treat the intervals which overlap with the pivot are \"equal element values\". The PARTITION method returns two indices q and t, where $p \\leq q \\leq t \\leq r$, such that: all intervals of A[q..t] overlap with A[t] , the right ending of each interval of A[p..q - 1] is less than the left ending of A[t] , the left ending of each interval of A[t + 1..r] is greater than the right ending of A[t] . FUZZY-SORTING-OF-INTERVALS(A, p, r) if p < r q, t = RANDOMIZED-PARTITION(A, p r) FUZZY-SORTING-OF-INTERVALS(A, p, q - 1) FUZZY-SORTING-OF-INTERVALS(A, t + 1, r) RANDOMIZED-PARTITION(A, p, r) i = RANDOM(p, r) exchange A[r] with A[i] return PARTITION(A, p, r) PARTITION(A, p, r) x = A[r] i = p - 1 t = p - 1 for j = p to r - 1 if A[j] is before x t = t + 1 i = i + 1 exchange A[t] with A[i] if t != j: exchange A[j] with A[i] else if A[j] overlaps with x t = t + 1 exchange A[t] with A[j] exchange A[t + 1] with A[r] return i + 1, t + 1","title":"a"},{"location":"07-Quicksort/Problems/#b_5","text":"It's just a variation of problem 7-2, expected we use different comparision checks, so the expected running time is $\\Theta(n\\lg{n})$ in general. When all of the intervals overlap, we treat them like all of the intervals are equal, thus like the analysis in problem 7-2, the expected running time is $\\Theta(n)$.","title":"b"},{"location":"08-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/","text":"8.1 Lower bounds for sorting 8.1-1 It's $\\Theta(n)$, when the array is already sorted. 8.1-2 By A.11 we have $\\int_0^n \\lg{k} dk \\leq \\sum_{k = 1}^n \\lg{k} \\leq \\int_1^{n + 1} \\lg{k} dk$. And $\\int \\lg{k} dk = k\\lg{k} - \\frac{1}{\\ln2}k + c = k\\lg{k} - k\\lg{e} + c$, but k could not be 0, so we cannot calculate the left part. Notice that when k = 1, $\\lg{k} = 0$, so $\\sum_{k = 1}^n \\lg{k} = \\sum_{k = 2}^n \\lg{k}$ for $n \\geq 2$. Thus we have $\\int_1^n \\lg{k} dk \\leq \\sum_{k = 2}^n \\lg{k} \\leq \\int_2^{n + 1} \\lg{k} dk$. For the left part: $$ \\begin{eqnarray} \\int_1^n \\lg{k} dk &=& n\\lg{n} - n\\lg{e} + c - (1\\lg1 - 1\\lg{e} + c) \\\\ &=& n(\\lg{n} - \\lg{e}) + \\lg{e} \\\\ &\\geq& n(\\lg{n} - \\frac{1}{2}\\lg{n}) + \\lg{e} \\quad (\\text{when } n \\geq e^2) \\\\ &=& \\frac{1}{2}n\\lg{n} + \\lg{e} \\\\ &>& \\frac{1}{2}n\\lg{n} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ For the right part: $$ \\begin{eqnarray} \\int_2^{n + 1} \\lg{k} dk &=& (n + 1)\\lg{(n + 1)} - (n + 1)\\lg{e} + c - (2\\lg2 - 2\\lg{e} + c) \\\\ &=& (n + 1)\\lg{\\frac{n + 1}{e}} + 2(\\lg{e} - 1) \\\\ &\\leq& (n + 1)\\lg{\\frac{n + 1}{e}} + (n + 1)\\lg{e} \\quad (\\text{when } n \\geq 1 - \\frac{1}{\\lg{e}}) \\\\ &=& (n + 1)\\lg{(n + 1)} \\\\ &<& 2n\\lg{n^2} \\quad (\\text{when } n \\geq 2) \\\\ &=& 4n\\lg{n} \\\\ &=&O(n\\lg{n}) \\end{eqnarray} $$ So $\\lg{(n!)} = \\Theta(n\\lg{n})$. 8.1-3 In the proof of theorem 8.1 we know that given m permutations, we have $m \\leq l$, where l is the number of reachable leaves, and since a binary tree of height h has no more than $2^h$ leaves, we have $m \\leq l \\leq 2^h$, so $h \\geq \\lg{m}$. When $m = \\frac{n!}{2}$, $\\lg{m} = \\lg{\\frac{n!}{2}} = \\lg{(n!)} - 1 = \\Omega(n\\lg{n})$. When $m = \\frac{n!}{n}$, $\\lg{m} = \\lg{\\frac{n!}{n}} = \\lg{(n!)} - \\lg{n} = \\Omega(n\\lg{n})$. When $m = \\frac{n!}{2^n}$, $\\lg{m} = \\lg{\\frac{n!}{2^n}} = \\lg{(n!)} - n = \\Omega(n\\lg{n})$. Thus none of the above m is linear. 8.1-4 In each subsequence, we need at least $\\Omega(k\\lg{k})$ comparisions, thus for $\\frac{n}{k}$ subsequences, it needs $\\frac{n}{k}\\Omega(k\\lg{k}) = \\Omega(n\\lg{k})$ comparisions.","title":"8.1 Lower bounds for sorting"},{"location":"08-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-lower-bounds-for-sorting","text":"","title":"8.1 Lower bounds for sorting"},{"location":"08-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-1","text":"It's $\\Theta(n)$, when the array is already sorted.","title":"8.1-1"},{"location":"08-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-2","text":"By A.11 we have $\\int_0^n \\lg{k} dk \\leq \\sum_{k = 1}^n \\lg{k} \\leq \\int_1^{n + 1} \\lg{k} dk$. And $\\int \\lg{k} dk = k\\lg{k} - \\frac{1}{\\ln2}k + c = k\\lg{k} - k\\lg{e} + c$, but k could not be 0, so we cannot calculate the left part. Notice that when k = 1, $\\lg{k} = 0$, so $\\sum_{k = 1}^n \\lg{k} = \\sum_{k = 2}^n \\lg{k}$ for $n \\geq 2$. Thus we have $\\int_1^n \\lg{k} dk \\leq \\sum_{k = 2}^n \\lg{k} \\leq \\int_2^{n + 1} \\lg{k} dk$. For the left part: $$ \\begin{eqnarray} \\int_1^n \\lg{k} dk &=& n\\lg{n} - n\\lg{e} + c - (1\\lg1 - 1\\lg{e} + c) \\\\ &=& n(\\lg{n} - \\lg{e}) + \\lg{e} \\\\ &\\geq& n(\\lg{n} - \\frac{1}{2}\\lg{n}) + \\lg{e} \\quad (\\text{when } n \\geq e^2) \\\\ &=& \\frac{1}{2}n\\lg{n} + \\lg{e} \\\\ &>& \\frac{1}{2}n\\lg{n} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ For the right part: $$ \\begin{eqnarray} \\int_2^{n + 1} \\lg{k} dk &=& (n + 1)\\lg{(n + 1)} - (n + 1)\\lg{e} + c - (2\\lg2 - 2\\lg{e} + c) \\\\ &=& (n + 1)\\lg{\\frac{n + 1}{e}} + 2(\\lg{e} - 1) \\\\ &\\leq& (n + 1)\\lg{\\frac{n + 1}{e}} + (n + 1)\\lg{e} \\quad (\\text{when } n \\geq 1 - \\frac{1}{\\lg{e}}) \\\\ &=& (n + 1)\\lg{(n + 1)} \\\\ &<& 2n\\lg{n^2} \\quad (\\text{when } n \\geq 2) \\\\ &=& 4n\\lg{n} \\\\ &=&O(n\\lg{n}) \\end{eqnarray} $$ So $\\lg{(n!)} = \\Theta(n\\lg{n})$.","title":"8.1-2"},{"location":"08-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-3","text":"In the proof of theorem 8.1 we know that given m permutations, we have $m \\leq l$, where l is the number of reachable leaves, and since a binary tree of height h has no more than $2^h$ leaves, we have $m \\leq l \\leq 2^h$, so $h \\geq \\lg{m}$. When $m = \\frac{n!}{2}$, $\\lg{m} = \\lg{\\frac{n!}{2}} = \\lg{(n!)} - 1 = \\Omega(n\\lg{n})$. When $m = \\frac{n!}{n}$, $\\lg{m} = \\lg{\\frac{n!}{n}} = \\lg{(n!)} - \\lg{n} = \\Omega(n\\lg{n})$. When $m = \\frac{n!}{2^n}$, $\\lg{m} = \\lg{\\frac{n!}{2^n}} = \\lg{(n!)} - n = \\Omega(n\\lg{n})$. Thus none of the above m is linear.","title":"8.1-3"},{"location":"08-Sorting-in-Linear-Time/8.1-Lower-bounds-for-sorting/#81-4","text":"In each subsequence, we need at least $\\Omega(k\\lg{k})$ comparisions, thus for $\\frac{n}{k}$ subsequences, it needs $\\frac{n}{k}\\Omega(k\\lg{k}) = \\Omega(n\\lg{k})$ comparisions.","title":"8.1-4"},{"location":"08-Sorting-in-Linear-Time/8.2-Counting-sort/","text":"8.2 Counting sort 8.2-1 8.2-2 Suppose for two indices i < j, we have A[i] = A[j]. And we scan A from n to 1, so we met A[j] first, and then we put A[j] in B and subtract 1 from C[A[j]], so when we met A[i], it will be put before A[j], so the numbers with the same value appear in the output array in the same order as they do in the input array, COUNTING-SORT is stable. 8.2-3 The new algorithm works properly because it only affects the output of the elements that have same value, the relative order of distinct elements is not affected. But the algorithm is not stable, because the numbers with the same value appear in the output array in the opposite order now. 8.2-4 We can reuse the code from line 1 to 8 in COUNTING-SORT . C[i] contains the number of elements less than or equal to i, so the number of elements fall into the range [a..b] is C[b] - C[a - 1] . We assume C[i] = 0 when i is negative. PREPROCESS(A, k) let C[0..k] be a new array for i = 0 to k C[i] = 0 for j = 1 to A.length C[A[j]] = C[A[j]] + 1 for i = 1 to k C[i] = C[i] + C[i - 1] QUERY(a, b) if a - 1 < 0 return C[b] return C[b] - C[a - 1] As discussed in the book, the running time to preprocess is $\\Theta(n + k)$.","title":"8.2 Counting sort"},{"location":"08-Sorting-in-Linear-Time/8.2-Counting-sort/#82-counting-sort","text":"","title":"8.2 Counting sort"},{"location":"08-Sorting-in-Linear-Time/8.2-Counting-sort/#82-1","text":"","title":"8.2-1"},{"location":"08-Sorting-in-Linear-Time/8.2-Counting-sort/#82-2","text":"Suppose for two indices i < j, we have A[i] = A[j]. And we scan A from n to 1, so we met A[j] first, and then we put A[j] in B and subtract 1 from C[A[j]], so when we met A[i], it will be put before A[j], so the numbers with the same value appear in the output array in the same order as they do in the input array, COUNTING-SORT is stable.","title":"8.2-2"},{"location":"08-Sorting-in-Linear-Time/8.2-Counting-sort/#82-3","text":"The new algorithm works properly because it only affects the output of the elements that have same value, the relative order of distinct elements is not affected. But the algorithm is not stable, because the numbers with the same value appear in the output array in the opposite order now.","title":"8.2-3"},{"location":"08-Sorting-in-Linear-Time/8.2-Counting-sort/#82-4","text":"We can reuse the code from line 1 to 8 in COUNTING-SORT . C[i] contains the number of elements less than or equal to i, so the number of elements fall into the range [a..b] is C[b] - C[a - 1] . We assume C[i] = 0 when i is negative. PREPROCESS(A, k) let C[0..k] be a new array for i = 0 to k C[i] = 0 for j = 1 to A.length C[A[j]] = C[A[j]] + 1 for i = 1 to k C[i] = C[i] + C[i - 1] QUERY(a, b) if a - 1 < 0 return C[b] return C[b] - C[a - 1] As discussed in the book, the running time to preprocess is $\\Theta(n + k)$.","title":"8.2-4"},{"location":"08-Sorting-in-Linear-Time/8.3-Radix-sort/","text":"8.3 Radix sort 8.3-1 1 2 3 4 COW SE A T A B B AR DOG TE A B A R B IG SEA MO B E A R B OX RUG TA B T A R C OW ROW DO G S E A D IG MOB RU G T E A D OG BOX DI G D I G EAR TAB BI G B I G F OX BAR BA R M O B M OB EAR EA R D O G N OW TAR TA R C O W R OW DIG CO W R O W R UG BIG RO W N O W S EA TEA NO W B O X T AB NOW BO X F O X T AR FOX FO X R U G T EA 8.3-2 Insertion sort and merge sort are stable, heapsort and quicksort are not stable. We can create an additional array to record the index of each element in the original array. When comparing two elements, if they are not equal, we compare them in the usualy way, if they are equal, we compare their indices. The scheme doesn't affect the running time since the new comparision method still requires O(1), but it needs additional O(n) space. 8.3-3 We use the following loop invariant: At the start of each iteration of the for loop of 1-2 lines, the n-digit numbers are sorted by digit 1 to i - 1. Initialization : Prior to the first iteration of the loop, i = 1, digit 1 to i - 1 contains 0 digit, thus we can say the n-digit numbers are sorted by digit 1 to 0. Maintenance : After the ith iteration, the numbers are sorted in the ith digit, because we used a stable sorting algorithm, so if two numbers has same value on digit i, it won't change their relative order, and digit 1 to digit i - 1 is already sorted, so digit 1 to digit i is sorted. Termination : At termination, i = d + 1, numbers are sorted by digit 1 to digit d, so the array is sorted. We need the assumption in the maintenance step. 8.3-4 We can treat the numbers are in base n, each number contains 3 digits, thus the max number is $(n - 1)n^2 + (n - 1)n + (n - 1) = n^3 - 1$, so d = 3. There are n possible values in each digit, so k = n, so we can use the radix sort to sort the numbers. The running time is $d\\Theta(n + k) = 3\\Theta(n + n) = \\Theta(n)$, which is also O(n). 8.3-5","title":"8.3 Radix sort"},{"location":"08-Sorting-in-Linear-Time/8.3-Radix-sort/#83-radix-sort","text":"","title":"8.3 Radix sort"},{"location":"08-Sorting-in-Linear-Time/8.3-Radix-sort/#83-1","text":"1 2 3 4 COW SE A T A B B AR DOG TE A B A R B IG SEA MO B E A R B OX RUG TA B T A R C OW ROW DO G S E A D IG MOB RU G T E A D OG BOX DI G D I G EAR TAB BI G B I G F OX BAR BA R M O B M OB EAR EA R D O G N OW TAR TA R C O W R OW DIG CO W R O W R UG BIG RO W N O W S EA TEA NO W B O X T AB NOW BO X F O X T AR FOX FO X R U G T EA","title":"8.3-1"},{"location":"08-Sorting-in-Linear-Time/8.3-Radix-sort/#83-2","text":"Insertion sort and merge sort are stable, heapsort and quicksort are not stable. We can create an additional array to record the index of each element in the original array. When comparing two elements, if they are not equal, we compare them in the usualy way, if they are equal, we compare their indices. The scheme doesn't affect the running time since the new comparision method still requires O(1), but it needs additional O(n) space.","title":"8.3-2"},{"location":"08-Sorting-in-Linear-Time/8.3-Radix-sort/#83-3","text":"We use the following loop invariant: At the start of each iteration of the for loop of 1-2 lines, the n-digit numbers are sorted by digit 1 to i - 1. Initialization : Prior to the first iteration of the loop, i = 1, digit 1 to i - 1 contains 0 digit, thus we can say the n-digit numbers are sorted by digit 1 to 0. Maintenance : After the ith iteration, the numbers are sorted in the ith digit, because we used a stable sorting algorithm, so if two numbers has same value on digit i, it won't change their relative order, and digit 1 to digit i - 1 is already sorted, so digit 1 to digit i is sorted. Termination : At termination, i = d + 1, numbers are sorted by digit 1 to digit d, so the array is sorted. We need the assumption in the maintenance step.","title":"8.3-3"},{"location":"08-Sorting-in-Linear-Time/8.3-Radix-sort/#83-4","text":"We can treat the numbers are in base n, each number contains 3 digits, thus the max number is $(n - 1)n^2 + (n - 1)n + (n - 1) = n^3 - 1$, so d = 3. There are n possible values in each digit, so k = n, so we can use the radix sort to sort the numbers. The running time is $d\\Theta(n + k) = 3\\Theta(n + n) = \\Theta(n)$, which is also O(n).","title":"8.3-4"},{"location":"08-Sorting-in-Linear-Time/8.3-Radix-sort/#83-5","text":"","title":"8.3-5"},{"location":"08-Sorting-in-Linear-Time/8.4-Bucket-sort/","text":"8.4 Bucket sort 8.4-1 8.4-2 The worst-case can happen when it takes too long to sort list B[i] . Because the worst-case running time of insertion sort is $\\Theta(n^2)$, if for a given B[i] , which happens to contain all elements, then the insertion sort takes $\\Theta(n^2)$, thus the worst-case running time becomes $\\Theta(n^2)$. We can use another sort algorithm to sort B[i] , for example, the mergesort algorithm. 8.4-3 We have: $$ \\begin{eqnarray} X &=& I\\lbrace\\text{the number of heads in two flips of a fair coin}\\rbrace \\\\ &=& \\begin{cases} 0 \\\\ 1 \\\\ 2 \\\\ \\end{cases} \\end{eqnarray} $$ And $Pr\\lbrace \\text{the number of heads in two flips is } 0\\rbrace = \\frac{1}{4}$, $Pr\\lbrace \\text{the number of heads in two flips is } 1\\rbrace = \\frac{1}{2}$, $Pr\\lbrace\\text{the number of heads in two flips is } 2\\rbrace = \\frac{1}{4}$. So $E[X] = 0 * \\frac{1}{4} + 1 * \\frac{1}{2} + 2 * \\frac{1}{4} = 1$. Thus $E[X^2] = 0^2 * \\frac{1}{4} + 1^2 * \\frac{1}{2} + 2^2 * \\frac{1}{4} = \\frac{3}{2}$, $(E[X])^2 = 1$. 8.4-4 Since the points are uniformly distributed, the probability of finding a point in any region of the circle is proportional to the area of that region. We can divide the unit circle into n parts, each part has area $\\frac{\\pi}{n}$. To make it simple, we can create n - 1 concentric circles inside the unit circle. Let $a_1$ be the first circle, so the unit circle is $a_n$. The area of $a_1$ is $\\frac{\\pi}{n}$, so we have $r_{a_1} = \\frac{1}{\\sqrt{n}}$. Notice that $\\pi{r_{a_k}^2} - \\pi{r_{a_{k - 1}}^2} = \\frac{\\pi}{n}$, so: $$ \\begin{eqnarray} r_{a_2}^2 - r_{a_1}^2 &=& \\frac{1}{n} \\\\ r_{a_3}^2 - r_{a_2}^2 &=& \\frac{1}{n} \\\\ \\ldots \\\\ r_{a_k}^2 - r_{a_{k - 1}}^2 &=& \\frac{1}{n} \\end{eqnarray} $$ So we add the equations together and have $r_{a_k}^2 - r_{a_1}^2 = \\frac{k - 1}{n}$, thus $r_{a_k} = \\sqrt{\\frac{k}{n}}, 1 \\leq k \\leq n$. Now we can create the buckets $[0, \\sqrt{\\frac{1}{n}}), [\\sqrt{\\frac{1}{n}}, \\sqrt{\\frac{2}{n}}), \\ldots, [\\sqrt{\\frac{n - 1}{n}}, 1]$. In order to sort the n points by their distance $d_i = \\sqrt{x_i^2 + y_i^2}$, let array A contains the n points ($A[i] = d_i$), then create an empty array B, and insert A[i] into $B[\\lfloor nA[i] \\rfloor]$, at last, we sort B[i] and concatenate the lists B[i] . 8.4-5 It's similar like bucket sort. We have $P(x) = \\text{Pr}\\lbrace X \\leq x \\rbrace$, so $P(1) = \\text{Pr}\\lbrace X \\leq 1 \\rbrace, P(2) = \\text{Pr}\\lbrace X \\leq 2 \\rbrace, \\ldots, P(n) = \\text{Pr}\\lbrace X \\leq n \\rbrace$. Thus $P(n) - P(n - 1) = \\text{Pr}\\lbrace n - 1 < X \\leq n \\rbrace$. So we can create the buckets $[0, P(1)), [P(1), P(2)), \\ldots, [P(n - 1), P(n)]$. In order to apply bucket sort, we let $A[i] = P(X_i)$, thus, we can use the bucket sort. At last, we have a list of sorted $P(X_i)$, which is also a list of sorted $X_i$.","title":"8.4 Bucket sort"},{"location":"08-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-bucket-sort","text":"","title":"8.4 Bucket sort"},{"location":"08-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-1","text":"","title":"8.4-1"},{"location":"08-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-2","text":"The worst-case can happen when it takes too long to sort list B[i] . Because the worst-case running time of insertion sort is $\\Theta(n^2)$, if for a given B[i] , which happens to contain all elements, then the insertion sort takes $\\Theta(n^2)$, thus the worst-case running time becomes $\\Theta(n^2)$. We can use another sort algorithm to sort B[i] , for example, the mergesort algorithm.","title":"8.4-2"},{"location":"08-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-3","text":"We have: $$ \\begin{eqnarray} X &=& I\\lbrace\\text{the number of heads in two flips of a fair coin}\\rbrace \\\\ &=& \\begin{cases} 0 \\\\ 1 \\\\ 2 \\\\ \\end{cases} \\end{eqnarray} $$ And $Pr\\lbrace \\text{the number of heads in two flips is } 0\\rbrace = \\frac{1}{4}$, $Pr\\lbrace \\text{the number of heads in two flips is } 1\\rbrace = \\frac{1}{2}$, $Pr\\lbrace\\text{the number of heads in two flips is } 2\\rbrace = \\frac{1}{4}$. So $E[X] = 0 * \\frac{1}{4} + 1 * \\frac{1}{2} + 2 * \\frac{1}{4} = 1$. Thus $E[X^2] = 0^2 * \\frac{1}{4} + 1^2 * \\frac{1}{2} + 2^2 * \\frac{1}{4} = \\frac{3}{2}$, $(E[X])^2 = 1$.","title":"8.4-3"},{"location":"08-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-4","text":"Since the points are uniformly distributed, the probability of finding a point in any region of the circle is proportional to the area of that region. We can divide the unit circle into n parts, each part has area $\\frac{\\pi}{n}$. To make it simple, we can create n - 1 concentric circles inside the unit circle. Let $a_1$ be the first circle, so the unit circle is $a_n$. The area of $a_1$ is $\\frac{\\pi}{n}$, so we have $r_{a_1} = \\frac{1}{\\sqrt{n}}$. Notice that $\\pi{r_{a_k}^2} - \\pi{r_{a_{k - 1}}^2} = \\frac{\\pi}{n}$, so: $$ \\begin{eqnarray} r_{a_2}^2 - r_{a_1}^2 &=& \\frac{1}{n} \\\\ r_{a_3}^2 - r_{a_2}^2 &=& \\frac{1}{n} \\\\ \\ldots \\\\ r_{a_k}^2 - r_{a_{k - 1}}^2 &=& \\frac{1}{n} \\end{eqnarray} $$ So we add the equations together and have $r_{a_k}^2 - r_{a_1}^2 = \\frac{k - 1}{n}$, thus $r_{a_k} = \\sqrt{\\frac{k}{n}}, 1 \\leq k \\leq n$. Now we can create the buckets $[0, \\sqrt{\\frac{1}{n}}), [\\sqrt{\\frac{1}{n}}, \\sqrt{\\frac{2}{n}}), \\ldots, [\\sqrt{\\frac{n - 1}{n}}, 1]$. In order to sort the n points by their distance $d_i = \\sqrt{x_i^2 + y_i^2}$, let array A contains the n points ($A[i] = d_i$), then create an empty array B, and insert A[i] into $B[\\lfloor nA[i] \\rfloor]$, at last, we sort B[i] and concatenate the lists B[i] .","title":"8.4-4"},{"location":"08-Sorting-in-Linear-Time/8.4-Bucket-sort/#84-5","text":"It's similar like bucket sort. We have $P(x) = \\text{Pr}\\lbrace X \\leq x \\rbrace$, so $P(1) = \\text{Pr}\\lbrace X \\leq 1 \\rbrace, P(2) = \\text{Pr}\\lbrace X \\leq 2 \\rbrace, \\ldots, P(n) = \\text{Pr}\\lbrace X \\leq n \\rbrace$. Thus $P(n) - P(n - 1) = \\text{Pr}\\lbrace n - 1 < X \\leq n \\rbrace$. So we can create the buckets $[0, P(1)), [P(1), P(2)), \\ldots, [P(n - 1), P(n)]$. In order to apply bucket sort, we let $A[i] = P(X_i)$, thus, we can use the bucket sort. At last, we have a list of sorted $P(X_i)$, which is also a list of sorted $X_i$.","title":"8.4-5"},{"location":"08-Sorting-in-Linear-Time/Problems/","text":"8-1 a There are n! permutations of n inputs, and each permutation is equally likely, thus the probability of each permutation is $\\frac{1}{n}$. So there are n! leaves are labeled $\\frac{1}{n}$, if there are more than n! leaves, the rest are labeled 0. b The external path length of a tree is the total length of all paths, from the root to the leaves. We have D(T) - D(LT) = leaves of LT, and D(T) - D(RT) = leaves of RT. So D(T) = D(LT) + D(RT) + leaves of LT + leaves of RT = D(LT) + D(RT) + k. c From the previous question, we have D(T) = D(LT) + D(RT) + k, and there are k - 1 permutations of LT and RT, so $d(k) = \\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace$. d We have: $$ \\begin{eqnarray} f'(i) &=& (i\\lg{i} + (k - i)\\lg{(k - i)})' \\\\ &=& \\lg{i} + i\\frac{1}{i\\ln2} + (-1)\\lg{(k - i)} + (k - i)\\frac{1}{(k - i)\\ln2}(-1) \\\\ &=& \\lg{i} + \\frac{1}{\\ln2} - \\lg{(k - i)} - \\frac{1}{\\ln2} \\\\ &=& \\lg{\\frac{i}{k - i}} \\end{eqnarray} $$ Let $\\lg{\\frac{i}{k - i}} \\geq 0$, we get $i \\geq \\frac{k}{2}$, let $\\lg{\\frac{i}{k - i}} < 0$, we have $i < \\frac{k}{2}$, so f(i) is monotonically decreasing at $[1, \\frac{k}{2})$ and monotonically increasing at $(\\frac{k}{2}, k - 1]$, and $f'(\\frac{k}{2}) = 0$, so f(i) is minimized at $i = \\frac{k}{2}$. So $f(\\frac{k}{2}) = k(\\lg{k} - 1)$. We assume that $d(k) = \\Omega(k\\lg{k})$, substituting it to $\\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace$ yielding: $$ \\begin{eqnarray} \\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace &\\geq& \\min_{1 \\leq i \\leq k - 1}\\lbrace ci\\lg{i} + c(k - i)\\lg{(k - i)} + k \\rbrace \\\\ &=& \\min_{1 \\leq i \\leq k - 1}\\lbrace c(i\\lg{i} + (k - i)\\lg{(k - i)}) + k \\rbrace \\\\ &\\geq& c(\\frac{k}{2}\\lg{\\frac{k}{2}} + \\frac{k}{2}\\lg{\\frac{k}{2}}) + k \\\\ &=& ck\\lg{k} + (1 - c)k \\\\ &\\geq& ck\\lg{k} \\\\ &=& \\Omega(k\\lg{k}) \\end{eqnarray} $$ where the last step holds as long as $c \\leq 1$. So $d(k) = \\Omega(k\\lg{k})$ e We know that $T_A$ has n! leaves, so k = n!, thus, $D(T_A) = d(n!) = \\Omega(n!\\lg{(n!)})$. Since each permutation has probability $\\frac{1}{n!}$, the average-case time to sort n elements is $\\frac{D(T_A)}{n!} = \\frac{\\Omega(n!\\lg{(n!)})}{n!} = \\Omega(\\lg{(n!)}) = \\Omega(n\\lg{n})$. f Suppose the randomized decision tree B has k permutations, so there exists a tree in the k permutations that has minimum comparisions, we can pick that tree as the deterministic decision tree A. 8-2 a Counting sort. b SORT-IN-PLACE(A) i = 1 j = A.length while i < j while A[i] == 0 i = i + 1 while A[j] == 1 j = j - 1 if i < j exchange A[i] with A[j] c Insertion sort. d The first algorithm satisfies, and that's the algorithm used in the book. The second algorithm is not stable, and the third algorithm doesn't run in O(n) time. e COUNTING-SORT-IN-PLACE(A, k) let C[0..k] be a new array for i = 0 to k C[i] = 0 for j = 1 to A.length C[A[j]] = C[A[j]] + 1 for i = 1 to k C[i] = C[i] + C[i - 1] i = 1 while i <= A.length element = A[i] position = C[element] if i >= position i = i + 1 else if element != A[position] exchange A[i] with A[position] C[element] = C[element] - 1 else C[element] = C[element] - 1 It's not stable. 8-3 a SORTING-VARIABLE-LENGTH-INTEGERS(A) n = A.length let B[1..n] be a new array for i = 1 to n let B[i] be a new array for i = 1 to n insert A[i] to B[j], where j is the length of A[i] for i = 1 to n: sort B[i] by RADIX-SORT concatenate the lists B[1], B[2], ..., B[n] together in order Let $a_i$ be the number of integers which have i digits, so $\\sum_{i = 1}^n{a_ii} = n$. The running time of RADIX-SORT is $O(dn)$, here we have $d = i, n = a_i$ so the total running time to sort all B[i] is $\\sum_{i = 1}^nO(ia_i) = O(n)$. The other for loops also takes O(n), so the running time of the algorithm is still O(n). b The idea is similar like the previous question, but we don't group the strings by string length, because they should be in alphabetical order. We group them by the first character, then we sort the groups by the first character using COUNTING-SORT . Then we do the same procedure recursively in each group, ignoring the first character. There is an important property that ensures the running time is O(n). If two strings have different first letter, then they are only compared once, we don't need to compare other characters in the two strings. Given a string $a_i$ with length $l_i$, then it will be sorted by COUNTING-SORT at most $l_i + 1$ times, the extra 1 time means $a_i$ is sorted as an empty string with other strings, then it will be grouped, and won't be compared any more in the next recursive procedure. Suppose there are m strings, so the running time is $O(\\sum_{i = 1}^m (l_i + 1)) = O(\\sum_{i = 1}^m l_i + m) = O(n + m) = O(n)$. SORTING-VARIABLE-LENGTH-STRINGS(A, start_letter_index) // To make it simple, we assume there are only 256 characters k = 255 let B[0..k] be a new array // Sort A by character index COUNTING-SORT(A, start_letter_index) for i = 1 to A.length insert A[i] to B by A's character at start_letter_index for i = 0 to k SORTING-VARIABLE-LENGTH-STRINGS(B[i], start_letter_index + 1) concatenate the lists B[0], B[1], ..., B[k] together in order 8-4 a WATER-JUGS(RED-JUGS, BLUE-JUGS) let WATER-JUG-PAIRS be a new array for i = 1 to RED-JUGS.length for j = 1 to BLUE-JUGS.length if RED-JUGS[i] and BLUE-JUGS[j] have the same volume put RED-JUGS[i] and BLUE-JUGS[j] into WATER-JUG-PAIRS b We are mapping n red jugs to n blue jugs, so there are $n * (n - 1) * \\ldots * 1 = n!$ permutations, in the decision tree, each node has 3 children (>, <, =). Consider the decision tree of height h with l reachable leaves. Because each of the n! permutations of the input appears as some leaf, we have $n! \\leq l$, and the tree of height h has no more than $3^h$ leaves, we have $n! \\leq l \\leq 3^h$, so $h \\geq \\log_3{(n!)} = c\\lg{(n!)} = \\Omega(n\\lg{n})$. Thus the lower bound for the number of comparisions is $\\Omega(n\\lg{n})$. c The idea is similar like QUICK-SORT , first we randomly pick a jug in red jugs as pivot, then we partition the red jugs into two parts, red jugs that have smaller volume and red jugs that have larger volume. And we also partition the blue jugs into two parts. Then we recursively solve the problem with the new jugs. WATER-JUGS(RED-JUGS, BLUE-JUGS) i = RANDOM(1, RED-JUGS.length) pivot = RED-JUGS[i] let RED-JUGS-SMALLER-THAN-PIVOT be a new array let RED-JUGS-LARGER-THAN-PIVOT be a new array let BLUE-JUGS-SMALLER-THAN-PIVOT be a new array let BLUE-JUGS-LARGER-THAN-PIVOT be a new array for i = 1 to RED-JUGS.length if RED-JUGS[i] < pivot insert RED-JUGS[i] into RED-JUGS-SMALLER-THAN-PIVOT else if RED-JUGS[i] > pivot insert RED-JUGS[i] into RED-JUGS-LARGER-THAN-PIVOT if BLUE-JUGS[i] < pivot insert RED-JUGS[i] into BLUE-JUGS-SMALLER-THAN-PIVOT else if BLUE-JUGS[i] > pivot insert BLUE-JUGS[i] into BLUE-JUGS-LARGER-THAN-PIVOT display(pivot, pivot) WATER-JUGS(RED-JUGS-SMALLER-THAN-PIVOT, BLUE-JUGS-SMALLER-THAN-PIVOT) WATER-JUGS(RED-JUGS-LARGER-THAN-PIVOT, BLUE-JUGS-LARGER-THAN-PIVOT) The running time analysis is similar like QUICK-SORT , the worst-case number of comparisions is $\\Theta(n^2)$ when the smallest or the largest jug is chosen as the pivot in each procedure, the problem size is splited into n - 1 and 1. 8-5 a It means the array is sorted. b [1, 2, 3, 4, 5, 6, 7, 8, 10, 9] c If for all $i = 1, 2, \\ldots, n - k$, $A[i] \\leq A[i + k]$, then: $$ \\begin{eqnarray} \\sum_{j = i}^{i + k - 1} A[j] &=& A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\ &\\leq& A[i + k] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\ &=& \\sum_{j = i + 1}^{i + k} A[j] \\end{eqnarray} $$ So for all $i = 1, 2, \\ldots, n - k$, $\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\leq \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}$. If for all $i = 1, 2, \\ldots, n - k$, $\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\leq \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}$, then: $$ \\begin{eqnarray} \\sum_{j = i}^{i + k - 1} A[j] &=& A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\ &\\leq& \\sum_{j = i + 1}^{i + k} A[j] \\\\ &=& \\sum_{j = i + 1}^{i + k - 1} A[j] + A[i + k] \\end{eqnarray} $$ So $A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\leq \\sum_{j = i + 1}^{i + k - 1} A[j] + A[i + k]$, thus $A[i] \\leq A[i + k]$, so for all $i = 1, 2, \\ldots, n - k$, $A[i] \\leq A[i + k]$. d We can use the property in the previous question to k-sort an n-element array. Thus, we need to split the array into k groups, each group contains $\\frac{n}{k}$ elements, then we sort the k groups separately. K-SORT-ARRAY(A, k) for i = 1 to k sort A[i], A[i + k], ... The running time to sort one group is $O(\\frac{n}{k}\\lg{\\frac{n}{k}})$, so the running time of the algorithm is $kO(\\frac{n}{k}\\lg{\\frac{n}{k}}) = O(n\\lg{\\frac{n}{k}})$. e A k-sorted array means we have k sorted lists, so we can use the algorithm in exercise 6.5-9 to sort it in $O(n\\lg{k})$ time. f The running time of k-sorting an n-element array is also $\\Omega(n\\lg{\\frac{n}{k}})$, when k is constant, we have: $$ \\begin{eqnarray} T(n) &\\geq& cn\\lg{\\frac{n}{k}} \\\\ &=& cn\\lg{n} - cn\\lg{k} \\\\ &=& \\frac{1}{2}cn\\lg{n} + cn(\\frac{1}{2}\\lg{n} - \\lg{k}) \\\\ &\\geq& \\frac{1}{2}cn\\lg{n} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ where the last step holds as long as $n \\geq k^2$. So $T(n) = \\Omega(n\\lg{n})$ 8-6 a First we select n numbers from the 2n numbers as the first sorted list, then the left numbers are belong to the other sorted list. The number of possible ways is $C_{2n}^n$. b Assume the height of decision tree is h, and there are l reachable leaves, thus we have $C_{2n}^n \\leq l \\leq 2^h$, so: $$ \\begin{eqnarray} h &\\geq& \\lg{(C_{2n}^n)} \\\\ &=& \\lg{\\frac{(2n)!}{n!n!}} \\\\ &=& \\lg{\\frac{\\sqrt{2\\pi2n}(\\frac{2n}{e})^{2n}(1 + O(\\frac{1}{2n}))}{(\\sqrt{2\\pi{n}}(\\frac{n}{e})^n(1 + O(\\frac{1}{n})))^2}} & \\text{(Stirling's approximation)} \\\\ &=& \\lg{\\frac{2^{2n}(1 + O(\\frac{1}{2n}))}{\\sqrt{\\pi{n}}(1 + O(\\frac{1}{n}))^2}} \\\\ &=& \\lg{(2^{2n}(1 + O(\\frac{1}{2n})))} - \\lg{(\\sqrt{\\pi{n}}(1 + O(\\frac{1}{n}))^2)} \\\\ &=& \\lg{2^{2n}} + \\lg{(1 + O(\\frac{1}{2n}))} - (\\lg{\\sqrt{\\pi{n}}} + \\lg{((1 + O(\\frac{1}{n}))^2)}) \\\\ &=& 2n + \\lg{(1 + O(\\frac{1}{2n}))} - \\lg{\\sqrt{\\pi{n}}} - 2\\lg{(1 + O(\\frac{1}{n}))} \\\\ &=& 2n - \\frac{1}{2}\\lg{(\\pi{n})} - \\lg{(1 + O(\\frac{1}{2n}))} \\\\ &=& 2n - o(n) \\end{eqnarray} $$ So it must perform at least $2n - o(n)$ comparisions. c If they are from different lists, we have to compare them to know which is larger or smaller. d There are 2n - 1 consecutive elements in the sorted 2n elements, thus we need 2n - 1 comparisions. 8-7 a We know that A[p] is put into a wrong location, and A[q] is the value that algorithm X moves to the location into which A[p] should have gone. So A[q] is also put into a wrong location, but A[p] is the smallest value in A that algorithm X puts into the wrong location, so A[p] < A[q] , thus, B[p] = 0 and B[q] = 1 . b","title":"Problems"},{"location":"08-Sorting-in-Linear-Time/Problems/#8-1","text":"","title":"8-1"},{"location":"08-Sorting-in-Linear-Time/Problems/#a","text":"There are n! permutations of n inputs, and each permutation is equally likely, thus the probability of each permutation is $\\frac{1}{n}$. So there are n! leaves are labeled $\\frac{1}{n}$, if there are more than n! leaves, the rest are labeled 0.","title":"a"},{"location":"08-Sorting-in-Linear-Time/Problems/#b","text":"The external path length of a tree is the total length of all paths, from the root to the leaves. We have D(T) - D(LT) = leaves of LT, and D(T) - D(RT) = leaves of RT. So D(T) = D(LT) + D(RT) + leaves of LT + leaves of RT = D(LT) + D(RT) + k.","title":"b"},{"location":"08-Sorting-in-Linear-Time/Problems/#c","text":"From the previous question, we have D(T) = D(LT) + D(RT) + k, and there are k - 1 permutations of LT and RT, so $d(k) = \\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace$.","title":"c"},{"location":"08-Sorting-in-Linear-Time/Problems/#d","text":"We have: $$ \\begin{eqnarray} f'(i) &=& (i\\lg{i} + (k - i)\\lg{(k - i)})' \\\\ &=& \\lg{i} + i\\frac{1}{i\\ln2} + (-1)\\lg{(k - i)} + (k - i)\\frac{1}{(k - i)\\ln2}(-1) \\\\ &=& \\lg{i} + \\frac{1}{\\ln2} - \\lg{(k - i)} - \\frac{1}{\\ln2} \\\\ &=& \\lg{\\frac{i}{k - i}} \\end{eqnarray} $$ Let $\\lg{\\frac{i}{k - i}} \\geq 0$, we get $i \\geq \\frac{k}{2}$, let $\\lg{\\frac{i}{k - i}} < 0$, we have $i < \\frac{k}{2}$, so f(i) is monotonically decreasing at $[1, \\frac{k}{2})$ and monotonically increasing at $(\\frac{k}{2}, k - 1]$, and $f'(\\frac{k}{2}) = 0$, so f(i) is minimized at $i = \\frac{k}{2}$. So $f(\\frac{k}{2}) = k(\\lg{k} - 1)$. We assume that $d(k) = \\Omega(k\\lg{k})$, substituting it to $\\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace$ yielding: $$ \\begin{eqnarray} \\min_{1 \\leq i \\leq k - 1}\\lbrace d(i) + d(k - i) + k \\rbrace &\\geq& \\min_{1 \\leq i \\leq k - 1}\\lbrace ci\\lg{i} + c(k - i)\\lg{(k - i)} + k \\rbrace \\\\ &=& \\min_{1 \\leq i \\leq k - 1}\\lbrace c(i\\lg{i} + (k - i)\\lg{(k - i)}) + k \\rbrace \\\\ &\\geq& c(\\frac{k}{2}\\lg{\\frac{k}{2}} + \\frac{k}{2}\\lg{\\frac{k}{2}}) + k \\\\ &=& ck\\lg{k} + (1 - c)k \\\\ &\\geq& ck\\lg{k} \\\\ &=& \\Omega(k\\lg{k}) \\end{eqnarray} $$ where the last step holds as long as $c \\leq 1$. So $d(k) = \\Omega(k\\lg{k})$","title":"d"},{"location":"08-Sorting-in-Linear-Time/Problems/#e","text":"We know that $T_A$ has n! leaves, so k = n!, thus, $D(T_A) = d(n!) = \\Omega(n!\\lg{(n!)})$. Since each permutation has probability $\\frac{1}{n!}$, the average-case time to sort n elements is $\\frac{D(T_A)}{n!} = \\frac{\\Omega(n!\\lg{(n!)})}{n!} = \\Omega(\\lg{(n!)}) = \\Omega(n\\lg{n})$.","title":"e"},{"location":"08-Sorting-in-Linear-Time/Problems/#f","text":"Suppose the randomized decision tree B has k permutations, so there exists a tree in the k permutations that has minimum comparisions, we can pick that tree as the deterministic decision tree A.","title":"f"},{"location":"08-Sorting-in-Linear-Time/Problems/#8-2","text":"","title":"8-2"},{"location":"08-Sorting-in-Linear-Time/Problems/#a_1","text":"Counting sort.","title":"a"},{"location":"08-Sorting-in-Linear-Time/Problems/#b_1","text":"SORT-IN-PLACE(A) i = 1 j = A.length while i < j while A[i] == 0 i = i + 1 while A[j] == 1 j = j - 1 if i < j exchange A[i] with A[j]","title":"b"},{"location":"08-Sorting-in-Linear-Time/Problems/#c_1","text":"Insertion sort.","title":"c"},{"location":"08-Sorting-in-Linear-Time/Problems/#d_1","text":"The first algorithm satisfies, and that's the algorithm used in the book. The second algorithm is not stable, and the third algorithm doesn't run in O(n) time.","title":"d"},{"location":"08-Sorting-in-Linear-Time/Problems/#e_1","text":"COUNTING-SORT-IN-PLACE(A, k) let C[0..k] be a new array for i = 0 to k C[i] = 0 for j = 1 to A.length C[A[j]] = C[A[j]] + 1 for i = 1 to k C[i] = C[i] + C[i - 1] i = 1 while i <= A.length element = A[i] position = C[element] if i >= position i = i + 1 else if element != A[position] exchange A[i] with A[position] C[element] = C[element] - 1 else C[element] = C[element] - 1 It's not stable.","title":"e"},{"location":"08-Sorting-in-Linear-Time/Problems/#8-3","text":"","title":"8-3"},{"location":"08-Sorting-in-Linear-Time/Problems/#a_2","text":"SORTING-VARIABLE-LENGTH-INTEGERS(A) n = A.length let B[1..n] be a new array for i = 1 to n let B[i] be a new array for i = 1 to n insert A[i] to B[j], where j is the length of A[i] for i = 1 to n: sort B[i] by RADIX-SORT concatenate the lists B[1], B[2], ..., B[n] together in order Let $a_i$ be the number of integers which have i digits, so $\\sum_{i = 1}^n{a_ii} = n$. The running time of RADIX-SORT is $O(dn)$, here we have $d = i, n = a_i$ so the total running time to sort all B[i] is $\\sum_{i = 1}^nO(ia_i) = O(n)$. The other for loops also takes O(n), so the running time of the algorithm is still O(n).","title":"a"},{"location":"08-Sorting-in-Linear-Time/Problems/#b_2","text":"The idea is similar like the previous question, but we don't group the strings by string length, because they should be in alphabetical order. We group them by the first character, then we sort the groups by the first character using COUNTING-SORT . Then we do the same procedure recursively in each group, ignoring the first character. There is an important property that ensures the running time is O(n). If two strings have different first letter, then they are only compared once, we don't need to compare other characters in the two strings. Given a string $a_i$ with length $l_i$, then it will be sorted by COUNTING-SORT at most $l_i + 1$ times, the extra 1 time means $a_i$ is sorted as an empty string with other strings, then it will be grouped, and won't be compared any more in the next recursive procedure. Suppose there are m strings, so the running time is $O(\\sum_{i = 1}^m (l_i + 1)) = O(\\sum_{i = 1}^m l_i + m) = O(n + m) = O(n)$. SORTING-VARIABLE-LENGTH-STRINGS(A, start_letter_index) // To make it simple, we assume there are only 256 characters k = 255 let B[0..k] be a new array // Sort A by character index COUNTING-SORT(A, start_letter_index) for i = 1 to A.length insert A[i] to B by A's character at start_letter_index for i = 0 to k SORTING-VARIABLE-LENGTH-STRINGS(B[i], start_letter_index + 1) concatenate the lists B[0], B[1], ..., B[k] together in order","title":"b"},{"location":"08-Sorting-in-Linear-Time/Problems/#8-4","text":"","title":"8-4"},{"location":"08-Sorting-in-Linear-Time/Problems/#a_3","text":"WATER-JUGS(RED-JUGS, BLUE-JUGS) let WATER-JUG-PAIRS be a new array for i = 1 to RED-JUGS.length for j = 1 to BLUE-JUGS.length if RED-JUGS[i] and BLUE-JUGS[j] have the same volume put RED-JUGS[i] and BLUE-JUGS[j] into WATER-JUG-PAIRS","title":"a"},{"location":"08-Sorting-in-Linear-Time/Problems/#b_3","text":"We are mapping n red jugs to n blue jugs, so there are $n * (n - 1) * \\ldots * 1 = n!$ permutations, in the decision tree, each node has 3 children (>, <, =). Consider the decision tree of height h with l reachable leaves. Because each of the n! permutations of the input appears as some leaf, we have $n! \\leq l$, and the tree of height h has no more than $3^h$ leaves, we have $n! \\leq l \\leq 3^h$, so $h \\geq \\log_3{(n!)} = c\\lg{(n!)} = \\Omega(n\\lg{n})$. Thus the lower bound for the number of comparisions is $\\Omega(n\\lg{n})$.","title":"b"},{"location":"08-Sorting-in-Linear-Time/Problems/#c_2","text":"The idea is similar like QUICK-SORT , first we randomly pick a jug in red jugs as pivot, then we partition the red jugs into two parts, red jugs that have smaller volume and red jugs that have larger volume. And we also partition the blue jugs into two parts. Then we recursively solve the problem with the new jugs. WATER-JUGS(RED-JUGS, BLUE-JUGS) i = RANDOM(1, RED-JUGS.length) pivot = RED-JUGS[i] let RED-JUGS-SMALLER-THAN-PIVOT be a new array let RED-JUGS-LARGER-THAN-PIVOT be a new array let BLUE-JUGS-SMALLER-THAN-PIVOT be a new array let BLUE-JUGS-LARGER-THAN-PIVOT be a new array for i = 1 to RED-JUGS.length if RED-JUGS[i] < pivot insert RED-JUGS[i] into RED-JUGS-SMALLER-THAN-PIVOT else if RED-JUGS[i] > pivot insert RED-JUGS[i] into RED-JUGS-LARGER-THAN-PIVOT if BLUE-JUGS[i] < pivot insert RED-JUGS[i] into BLUE-JUGS-SMALLER-THAN-PIVOT else if BLUE-JUGS[i] > pivot insert BLUE-JUGS[i] into BLUE-JUGS-LARGER-THAN-PIVOT display(pivot, pivot) WATER-JUGS(RED-JUGS-SMALLER-THAN-PIVOT, BLUE-JUGS-SMALLER-THAN-PIVOT) WATER-JUGS(RED-JUGS-LARGER-THAN-PIVOT, BLUE-JUGS-LARGER-THAN-PIVOT) The running time analysis is similar like QUICK-SORT , the worst-case number of comparisions is $\\Theta(n^2)$ when the smallest or the largest jug is chosen as the pivot in each procedure, the problem size is splited into n - 1 and 1.","title":"c"},{"location":"08-Sorting-in-Linear-Time/Problems/#8-5","text":"","title":"8-5"},{"location":"08-Sorting-in-Linear-Time/Problems/#a_4","text":"It means the array is sorted.","title":"a"},{"location":"08-Sorting-in-Linear-Time/Problems/#b_4","text":"[1, 2, 3, 4, 5, 6, 7, 8, 10, 9]","title":"b"},{"location":"08-Sorting-in-Linear-Time/Problems/#c_3","text":"If for all $i = 1, 2, \\ldots, n - k$, $A[i] \\leq A[i + k]$, then: $$ \\begin{eqnarray} \\sum_{j = i}^{i + k - 1} A[j] &=& A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\ &\\leq& A[i + k] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\ &=& \\sum_{j = i + 1}^{i + k} A[j] \\end{eqnarray} $$ So for all $i = 1, 2, \\ldots, n - k$, $\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\leq \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}$. If for all $i = 1, 2, \\ldots, n - k$, $\\frac{\\sum_{j = i}^{i + k - 1} A[j]}{k} \\leq \\frac{\\sum_{j = i + 1}^{i + k} A[j]}{k}$, then: $$ \\begin{eqnarray} \\sum_{j = i}^{i + k - 1} A[j] &=& A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\\\ &\\leq& \\sum_{j = i + 1}^{i + k} A[j] \\\\ &=& \\sum_{j = i + 1}^{i + k - 1} A[j] + A[i + k] \\end{eqnarray} $$ So $A[i] + \\sum_{j = i + 1}^{i + k - 1} A[j] \\leq \\sum_{j = i + 1}^{i + k - 1} A[j] + A[i + k]$, thus $A[i] \\leq A[i + k]$, so for all $i = 1, 2, \\ldots, n - k$, $A[i] \\leq A[i + k]$.","title":"c"},{"location":"08-Sorting-in-Linear-Time/Problems/#d_2","text":"We can use the property in the previous question to k-sort an n-element array. Thus, we need to split the array into k groups, each group contains $\\frac{n}{k}$ elements, then we sort the k groups separately. K-SORT-ARRAY(A, k) for i = 1 to k sort A[i], A[i + k], ... The running time to sort one group is $O(\\frac{n}{k}\\lg{\\frac{n}{k}})$, so the running time of the algorithm is $kO(\\frac{n}{k}\\lg{\\frac{n}{k}}) = O(n\\lg{\\frac{n}{k}})$.","title":"d"},{"location":"08-Sorting-in-Linear-Time/Problems/#e_2","text":"A k-sorted array means we have k sorted lists, so we can use the algorithm in exercise 6.5-9 to sort it in $O(n\\lg{k})$ time.","title":"e"},{"location":"08-Sorting-in-Linear-Time/Problems/#f_1","text":"The running time of k-sorting an n-element array is also $\\Omega(n\\lg{\\frac{n}{k}})$, when k is constant, we have: $$ \\begin{eqnarray} T(n) &\\geq& cn\\lg{\\frac{n}{k}} \\\\ &=& cn\\lg{n} - cn\\lg{k} \\\\ &=& \\frac{1}{2}cn\\lg{n} + cn(\\frac{1}{2}\\lg{n} - \\lg{k}) \\\\ &\\geq& \\frac{1}{2}cn\\lg{n} \\\\ &=& \\Omega(n\\lg{n}) \\end{eqnarray} $$ where the last step holds as long as $n \\geq k^2$. So $T(n) = \\Omega(n\\lg{n})$","title":"f"},{"location":"08-Sorting-in-Linear-Time/Problems/#8-6","text":"","title":"8-6"},{"location":"08-Sorting-in-Linear-Time/Problems/#a_5","text":"First we select n numbers from the 2n numbers as the first sorted list, then the left numbers are belong to the other sorted list. The number of possible ways is $C_{2n}^n$.","title":"a"},{"location":"08-Sorting-in-Linear-Time/Problems/#b_5","text":"Assume the height of decision tree is h, and there are l reachable leaves, thus we have $C_{2n}^n \\leq l \\leq 2^h$, so: $$ \\begin{eqnarray} h &\\geq& \\lg{(C_{2n}^n)} \\\\ &=& \\lg{\\frac{(2n)!}{n!n!}} \\\\ &=& \\lg{\\frac{\\sqrt{2\\pi2n}(\\frac{2n}{e})^{2n}(1 + O(\\frac{1}{2n}))}{(\\sqrt{2\\pi{n}}(\\frac{n}{e})^n(1 + O(\\frac{1}{n})))^2}} & \\text{(Stirling's approximation)} \\\\ &=& \\lg{\\frac{2^{2n}(1 + O(\\frac{1}{2n}))}{\\sqrt{\\pi{n}}(1 + O(\\frac{1}{n}))^2}} \\\\ &=& \\lg{(2^{2n}(1 + O(\\frac{1}{2n})))} - \\lg{(\\sqrt{\\pi{n}}(1 + O(\\frac{1}{n}))^2)} \\\\ &=& \\lg{2^{2n}} + \\lg{(1 + O(\\frac{1}{2n}))} - (\\lg{\\sqrt{\\pi{n}}} + \\lg{((1 + O(\\frac{1}{n}))^2)}) \\\\ &=& 2n + \\lg{(1 + O(\\frac{1}{2n}))} - \\lg{\\sqrt{\\pi{n}}} - 2\\lg{(1 + O(\\frac{1}{n}))} \\\\ &=& 2n - \\frac{1}{2}\\lg{(\\pi{n})} - \\lg{(1 + O(\\frac{1}{2n}))} \\\\ &=& 2n - o(n) \\end{eqnarray} $$ So it must perform at least $2n - o(n)$ comparisions.","title":"b"},{"location":"08-Sorting-in-Linear-Time/Problems/#c_4","text":"If they are from different lists, we have to compare them to know which is larger or smaller.","title":"c"},{"location":"08-Sorting-in-Linear-Time/Problems/#d_3","text":"There are 2n - 1 consecutive elements in the sorted 2n elements, thus we need 2n - 1 comparisions.","title":"d"},{"location":"08-Sorting-in-Linear-Time/Problems/#8-7","text":"","title":"8-7"},{"location":"08-Sorting-in-Linear-Time/Problems/#a_6","text":"We know that A[p] is put into a wrong location, and A[q] is the value that algorithm X moves to the location into which A[p] should have gone. So A[q] is also put into a wrong location, but A[p] is the smallest value in A that algorithm X puts into the wrong location, so A[p] < A[q] , thus, B[p] = 0 and B[q] = 1 .","title":"a"},{"location":"08-Sorting-in-Linear-Time/Problems/#b_6","text":"","title":"b"},{"location":"09-Medians-and-Order-Statistics/9.1-Minimum-and-maximum/","text":"9.1 Minimum and maximum 9.1-1 We know that we need n - 1 comparisions to find the smallest element. Let's define an algorithm to find the smallest element: FIND-THE-SMALLEST(A, low, high) mid = (low + high) / 2 left-min = FIND-THE-SMALLEST(A, low, mid) right-min = FIND-THE-SMALLEST(A, mid + 1, high) return min(left-min, right-min) The root node of the recursive tree has two children, one of them is the smallest element, let's assume the left child is the smallest element and let k denote the right child, then in order to find the second smallest element, we can traverse through the left branch, we let the smaller value of left and right children be the value of each node. When we are traversing, we compare the current second smallest element with the child with larger value (because the smaller one is the smallest element), if it's smaller than the second smallest element, we update the second smaller element. Because we only compare the smaller element with k in each node, so we don't need to traverse all branches in left branch. Since the height of recursive tree is $\\lceil \\lg{n} \\rceil$, thus we need $\\lceil \\lg{n} \\rceil - 1$ comparisions. So we need $n - 1 + \\lceil \\lg{n} \\rceil - 1 = n + \\lceil \\lg{n} \\rceil - 2$ comparisions. 9.1-2 The analysis is similar like the analysis in the book. When n is even, we need $\\frac{3n}{2} - 2$ comparisions, which is also $\\lceil \\frac{3n}{2} \\rceil - 2$. When n is odd, we need $3\\lfloor \\frac{n}{2} \\rfloor$ comparisions. And: $$ \\begin{eqnarray} 3\\lfloor \\frac{n}{2} \\rfloor &=& 3\\lceil \\frac{n - 2 + 1}{2} \\rceil \\\\ &=& 3\\lceil \\frac{n - 1}{2} \\rceil \\\\ &=& \\lceil \\frac{3(n - 1)}{2} \\rceil & (n \\text{ is odd}) \\\\ &=& \\lceil \\frac{3n}{2} - \\frac{3}{2} \\rceil \\\\ &=& \\lceil (\\frac{3n}{2} + \\frac{1}{2}) - (\\frac{3}{2} + \\frac{1}{2}) \\rceil \\\\ &=& \\lceil \\frac{3n}{2} + \\frac{1}{2} \\rceil - \\lceil \\frac{3}{2} + \\frac{1}{2} \\rceil \\\\ &=& \\lceil \\frac{3n}{2} \\rceil - 2 \\end{eqnarray} $$ So when n is odd, it also needs $\\lceil \\frac{3n}{2} \\rceil - 2$ comparisions.","title":"9.1 Minimum and maximum"},{"location":"09-Medians-and-Order-Statistics/9.1-Minimum-and-maximum/#91-minimum-and-maximum","text":"","title":"9.1 Minimum and maximum"},{"location":"09-Medians-and-Order-Statistics/9.1-Minimum-and-maximum/#91-1","text":"We know that we need n - 1 comparisions to find the smallest element. Let's define an algorithm to find the smallest element: FIND-THE-SMALLEST(A, low, high) mid = (low + high) / 2 left-min = FIND-THE-SMALLEST(A, low, mid) right-min = FIND-THE-SMALLEST(A, mid + 1, high) return min(left-min, right-min) The root node of the recursive tree has two children, one of them is the smallest element, let's assume the left child is the smallest element and let k denote the right child, then in order to find the second smallest element, we can traverse through the left branch, we let the smaller value of left and right children be the value of each node. When we are traversing, we compare the current second smallest element with the child with larger value (because the smaller one is the smallest element), if it's smaller than the second smallest element, we update the second smaller element. Because we only compare the smaller element with k in each node, so we don't need to traverse all branches in left branch. Since the height of recursive tree is $\\lceil \\lg{n} \\rceil$, thus we need $\\lceil \\lg{n} \\rceil - 1$ comparisions. So we need $n - 1 + \\lceil \\lg{n} \\rceil - 1 = n + \\lceil \\lg{n} \\rceil - 2$ comparisions.","title":"9.1-1"},{"location":"09-Medians-and-Order-Statistics/9.1-Minimum-and-maximum/#91-2","text":"The analysis is similar like the analysis in the book. When n is even, we need $\\frac{3n}{2} - 2$ comparisions, which is also $\\lceil \\frac{3n}{2} \\rceil - 2$. When n is odd, we need $3\\lfloor \\frac{n}{2} \\rfloor$ comparisions. And: $$ \\begin{eqnarray} 3\\lfloor \\frac{n}{2} \\rfloor &=& 3\\lceil \\frac{n - 2 + 1}{2} \\rceil \\\\ &=& 3\\lceil \\frac{n - 1}{2} \\rceil \\\\ &=& \\lceil \\frac{3(n - 1)}{2} \\rceil & (n \\text{ is odd}) \\\\ &=& \\lceil \\frac{3n}{2} - \\frac{3}{2} \\rceil \\\\ &=& \\lceil (\\frac{3n}{2} + \\frac{1}{2}) - (\\frac{3}{2} + \\frac{1}{2}) \\rceil \\\\ &=& \\lceil \\frac{3n}{2} + \\frac{1}{2} \\rceil - \\lceil \\frac{3}{2} + \\frac{1}{2} \\rceil \\\\ &=& \\lceil \\frac{3n}{2} \\rceil - 2 \\end{eqnarray} $$ So when n is odd, it also needs $\\lceil \\frac{3n}{2} \\rceil - 2$ comparisions.","title":"9.1-2"},{"location":"09-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/","text":"9.2 Selection in expected linear time 9.2-1 If an array has 0 length, we have q - p = 0 or r - q = 0 . If q - p = 0 , we have k = 1 and line 8 will be executed, but we have i < k , so i < 1 , which is not possible. If r - q = 0 , we have i > k , but k = q - p + 1 = r - p + 1 , so k is the length of array, thus i could not be larger than k . So the algorithm never makes a recursive call to a 0-length array. 9.2-2 The value of $X_k$ doesn't affect the subproblem T(max(k - 1, n - k)). So they are independent. 9.2-3 RANDOMIZED-SELECT(A, p, r, i) while p <= r if p == r return A[p] q = RANDOMIZED-PARTITION(A, p, r) k = q - p + 1 if i = k return A[q] elseif i < k r = q - 1 else p = q + 1 i = i - k 9.2-4 The worst-case happens when the partition procedure always select the largest element as pivot. pivot = 9, subarray = { 3, 2, 1, 0, 7, 5, 4, 8, 6} pivot = 8, subarray = { 3, 2, 1, 0, 7, 5, 4, 6 } pivot = 7, subarray = { 3, 2, 1, 0, 6, 5, 4 } pivot = 6, subarray = { 3, 2, 1, 0, 4, 5 } pivot = 5, subarray = { 3, 2, 1, 0, 4} pivot = 4, subarray = { 3, 2, 1, 0 } pivot = 3, subarray = { 0, 2, 1 } pivot = 2, subarray = { 0, 1 } pivot = 1, subarray = { 0 } minimum = 0","title":"9.2 Selection in expected linear time"},{"location":"09-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-selection-in-expected-linear-time","text":"","title":"9.2 Selection in expected linear time"},{"location":"09-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-1","text":"If an array has 0 length, we have q - p = 0 or r - q = 0 . If q - p = 0 , we have k = 1 and line 8 will be executed, but we have i < k , so i < 1 , which is not possible. If r - q = 0 , we have i > k , but k = q - p + 1 = r - p + 1 , so k is the length of array, thus i could not be larger than k . So the algorithm never makes a recursive call to a 0-length array.","title":"9.2-1"},{"location":"09-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-2","text":"The value of $X_k$ doesn't affect the subproblem T(max(k - 1, n - k)). So they are independent.","title":"9.2-2"},{"location":"09-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-3","text":"RANDOMIZED-SELECT(A, p, r, i) while p <= r if p == r return A[p] q = RANDOMIZED-PARTITION(A, p, r) k = q - p + 1 if i = k return A[q] elseif i < k r = q - 1 else p = q + 1 i = i - k","title":"9.2-3"},{"location":"09-Medians-and-Order-Statistics/9.2-Selection-in-expected-linear-time/#92-4","text":"The worst-case happens when the partition procedure always select the largest element as pivot. pivot = 9, subarray = { 3, 2, 1, 0, 7, 5, 4, 8, 6} pivot = 8, subarray = { 3, 2, 1, 0, 7, 5, 4, 6 } pivot = 7, subarray = { 3, 2, 1, 0, 6, 5, 4 } pivot = 6, subarray = { 3, 2, 1, 0, 4, 5 } pivot = 5, subarray = { 3, 2, 1, 0, 4} pivot = 4, subarray = { 3, 2, 1, 0 } pivot = 3, subarray = { 0, 2, 1 } pivot = 2, subarray = { 0, 1 } pivot = 1, subarray = { 0 } minimum = 0","title":"9.2-4"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/","text":"9.3 Selection in worst-case linear time 9.3-1 Let's assume the input elements are divided into groups of k. Similar like the analysis in the book, at least half of the $\\lceil \\frac{n}{k} \\rceil$ groups contribute at least $\\lceil \\frac{k}{2} \\rceil$ elements that are greater than x, except for the one group that has fewer than k elements if k does not divide n exactly, and the one group containing x itself. Discounting these two groups, it follows that the number of elements greater than x is at least $\\lceil \\frac{k}{2} \\rceil(\\lceil \\frac{1}{2} \\lceil \\frac{n}{k} \\rceil \\rceil - 2) \\geq \\frac{k}{2}(\\frac{1}{2}\\frac{n}{k} - 2) = \\frac{n}{4} - k$. Similarly, at least $\\frac{n}{4} - k$ elements are less than x. Thus, in the worst case, step 5 calls SELECT recursively on at most $\\frac{3n}{4} + k$ elements. So when n is greater than some constant, we have $T(n) \\leq T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + O(n)$. We assume T(n) runs in linear time, substituting it into the recurrence yields: $$ \\begin{eqnarray} T(n) &\\leq& T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + O(n) \\\\ &\\leq& T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + an \\\\ &\\leq& c\\frac{n}{k} + c(\\frac{3n}{4} + k) + an \\\\ &=& cn + (\\frac{c}{k}n + an - \\frac{c}{4}n + ck) \\\\ &\\leq& cn \\end{eqnarray} $$ where the last step holds as long as $\\frac{c}{k}n + an - \\frac{c}{4}n + ck \\leq 0$. So we need to find some k such that there exists constants c and a such that $\\frac{c}{k}n + an - \\frac{c}{4}n + ck \\leq 0$. We have $\\frac{c}{k}n + an - \\frac{c}{4}n + ck = c(\\frac{n}{k} - \\frac{n}{4} + k) + an \\leq 0$. Because both c and a are positive, so it could only be $\\frac{n}{k} - \\frac{n}{4} + k \\leq 0$. Let $f(k) = \\frac{n}{k} - \\frac{n}{4} + k$, so $f(4) = 4 > 0, f(5) = -\\frac{n}{20} + 5 \\leq 0 \\text{ when } n \\geq 100$. So we can always find a $n_0$ such that $f(k) \\leq 0$ when $k \\geq 5$. Thus the algorithm work in linear time if the input elements are divided into groups of 7, but doesn't run in linear time if they are divided into groups of 3. 9.3-2 We already know that there are at least $\\frac{3n}{10} - 6$ elements are less (greater) than x, because $\\lceil \\frac{n}{4} \\rceil < \\frac{n}{4} + 1$, let $\\frac{3n}{10} - 6 \\geq \\frac{n}{4} + 1$, we get $n \\geq 140$. 9.3-3 In the previous question, we already know that if $n \\geq 140$, then at least $\\lceil \\frac{n}{4} \\rceil$ elements are less (greater) than x. So the partition procedure splits the problem size to $\\frac{n}{4}$ and $\\frac{3n}{4}$ in quicksort. Thus, $T(n) = T(\\frac{n}{4}) + T(\\frac{3n}{4}) + \\Theta(n)$. And the solution is $T(n) = \\Theta(n\\lg{n})$, which is solved in exercise 4.4-9. 9.3-4 9.3-5 The idea is similar. SELECT(A, p, r, i) if p == r return A[p] median = BLACK-BOX-MEDIAN(A, p, r) q = PARTITION(A, p, r, median) k = q - p + 1 if i == k return A[q] elseif i < k return SELECT(A, p, q - 1, i) else return SELECT(A, q + 1, r, i - k) First we find the median of the array, and use that median as pivot to partition the array. Then we recursively call the procedure to get the ith element. Now let's analysis the running time. The BLAK-BOX-MEDIAN takes O(n), and partition method also takes O(n), then it at least reduces the problem size to $\\frac{n}{2}$, thus we have $T(n) = T(\\frac{n}{2}) + O(n)$. We guess T(n) = O(n), substituting it to the recurrence yielding: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + O(n) \\\\ &\\leq& c\\frac{n}{2} + dn \\\\ &=& (\\frac{c}{2} + d)n \\\\ &=& O(n) \\end{eqnarray} $$ So the algorithm solves the problem in linear time. 9.3-6 The k quantiles of an n-element set are $A[\\lceil \\frac{n}{k} \\rceil], A[2\\lceil \\frac{n}{k} \\rceil], \\ldots, A[(k - 1)\\lceil \\frac{n}{k} \\rceil]$. And the idea is simple, first we divide the elements into two parts, one part contains $\\lceil \\frac{k}{2} \\rceil$ quantiles, and the other part contains $ k - \\lceil \\frac{k}{2} \\rceil$ quantiles. Then we recursively call the procedure on the two parts. In order to divide the elements into two parts, we use the SELECT procedure to get the $\\lceil \\frac{k}{2} \\rceil\\lceil \\frac{n}{k} \\rceil$th smallest element, which is the last element of the $\\lceil \\frac{k}{2} \\rceil$ quantiles. KTH-QUANTILES(A, p, r, k) if k == 1 return else: i = MATH.CEIL(k / 2) * SIZE-OF-ELEMENTS-IN-EACH-QUANTILE index, quantile = SELECT(A, p, r, i) OUTPUT quantile KTH-QUANTILES(A, p, index, MATH.CEIL(k / 2)) KTH-QUANTILES(A, index + 1, r, k - MATH.CEIL(k / 2)) Now let's analysis the running time. We have $T(n, k) = 2T(\\frac{n}{2}, \\frac{k}{2}) + O(n)$. If we draw a recursion tree, we can see that each level of the tree costs O(n) and the depth of tree is $\\lg{k}$, thus the running time is $O(n\\lg{k})$. 9.3-7 First, we find the median of the set, it costs O(n), then we create another array that contains the absolute distance between the median and each element. Then we use the SELECT procedure to select the kth smallest element p in the new array, at last, we compare each element in S with median, if the distance between element and median is not greater than p, then the element one of the k closest elements. Each step costs O(n), so the algorithm runs in O(n). K-CLOSEST(A, k) i = MATH.CEIL(A.length / 2) median = SELECT(A, i) let B[1..n] be a new array for j = 1 to n B[j] = MATH.ABS(A[j] - median) kth = SELECT(B, k) let C be a new array for j = 1 to n if MATH.ABS(A[j] - median) < kth add A[j] to C for j = 1 to n MATH.ABS(A[j] - median) == kth add A[j] to C if C.length >= k break return C Because the elements in A are all distinct, so each element in B have at most 1 duplicate. The reason to scan A twice to get C is that A is not sorted, if we combine the last two loops together, it is possible we cannot get the correct k closest elements to the median. 9.3-8 We can first get the medians of x and y, if the median of x is smaller than the median of y, then the median of all is in x's right part and y's left part, if the median of x is larger than the median of y, then the median of all is in x's left part and y's right part. So we can recursively solves the problem. MEDIAN-OF-TWO-SORTED-ARRAYS(X, px, rx, Y, py, ry) if rx - px == 1 if X[px] <= Y[py] return MATH.MIN(X[rx], Y[py]) else: return MATH.MIN(X[px], Y[ry]) median-index-of-x = MATH.FLOOR((px + rx) / 2) median-index-of-y = MATH.FLOOR((py + ry) / 2) if X[index] < Y[median-index-of-y] return MEDIAN-OF-TWO-SORTED-ARRAYS(X, index, rx, Y, py, median-index-of-y) elif X[index] > Y[median-index-of-y] return MEDIAN-OF-TWO-SORTED-ARRAYS(X, px, index, Y, median-index-of-y, ry) else: return X[index] Each recursive procedure reduces the problem size to $\\frac{n}{2}$, so we have $T(n) = T(\\frac{n}{2}) + \\Theta(1)$. Let's solve the recurrence by the master method. Here we have a = 1, b = 2, $f(n) = \\Theta(1)$. so $n^{\\log_b{a}} = 1$, thus $f(n) = \\Theta(n^{\\log_b{a}})$, so $T(n) = \\Theta(n^{\\log_b{a}}\\lg{n}) = \\Theta(\\lg{n})$. 9.3-9 Suppose the n wells have y-coordinates $y_1, y_2, \\ldots, y_n$, and we need to find $y_0$ such that the value $\\sum_{i = 1}^{n}|y_i - y_0|$ is minimum. This is the 1-dimensional case of Geometric median , and the median minimize the sum. Explanation .","title":"9.3 Selection in worst-case linear time"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-selection-in-worst-case-linear-time","text":"","title":"9.3 Selection in worst-case linear time"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-1","text":"Let's assume the input elements are divided into groups of k. Similar like the analysis in the book, at least half of the $\\lceil \\frac{n}{k} \\rceil$ groups contribute at least $\\lceil \\frac{k}{2} \\rceil$ elements that are greater than x, except for the one group that has fewer than k elements if k does not divide n exactly, and the one group containing x itself. Discounting these two groups, it follows that the number of elements greater than x is at least $\\lceil \\frac{k}{2} \\rceil(\\lceil \\frac{1}{2} \\lceil \\frac{n}{k} \\rceil \\rceil - 2) \\geq \\frac{k}{2}(\\frac{1}{2}\\frac{n}{k} - 2) = \\frac{n}{4} - k$. Similarly, at least $\\frac{n}{4} - k$ elements are less than x. Thus, in the worst case, step 5 calls SELECT recursively on at most $\\frac{3n}{4} + k$ elements. So when n is greater than some constant, we have $T(n) \\leq T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + O(n)$. We assume T(n) runs in linear time, substituting it into the recurrence yields: $$ \\begin{eqnarray} T(n) &\\leq& T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + O(n) \\\\ &\\leq& T(\\lceil \\frac{n}{k} \\rceil) + T(\\frac{3n}{4} + k) + an \\\\ &\\leq& c\\frac{n}{k} + c(\\frac{3n}{4} + k) + an \\\\ &=& cn + (\\frac{c}{k}n + an - \\frac{c}{4}n + ck) \\\\ &\\leq& cn \\end{eqnarray} $$ where the last step holds as long as $\\frac{c}{k}n + an - \\frac{c}{4}n + ck \\leq 0$. So we need to find some k such that there exists constants c and a such that $\\frac{c}{k}n + an - \\frac{c}{4}n + ck \\leq 0$. We have $\\frac{c}{k}n + an - \\frac{c}{4}n + ck = c(\\frac{n}{k} - \\frac{n}{4} + k) + an \\leq 0$. Because both c and a are positive, so it could only be $\\frac{n}{k} - \\frac{n}{4} + k \\leq 0$. Let $f(k) = \\frac{n}{k} - \\frac{n}{4} + k$, so $f(4) = 4 > 0, f(5) = -\\frac{n}{20} + 5 \\leq 0 \\text{ when } n \\geq 100$. So we can always find a $n_0$ such that $f(k) \\leq 0$ when $k \\geq 5$. Thus the algorithm work in linear time if the input elements are divided into groups of 7, but doesn't run in linear time if they are divided into groups of 3.","title":"9.3-1"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-2","text":"We already know that there are at least $\\frac{3n}{10} - 6$ elements are less (greater) than x, because $\\lceil \\frac{n}{4} \\rceil < \\frac{n}{4} + 1$, let $\\frac{3n}{10} - 6 \\geq \\frac{n}{4} + 1$, we get $n \\geq 140$.","title":"9.3-2"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-3","text":"In the previous question, we already know that if $n \\geq 140$, then at least $\\lceil \\frac{n}{4} \\rceil$ elements are less (greater) than x. So the partition procedure splits the problem size to $\\frac{n}{4}$ and $\\frac{3n}{4}$ in quicksort. Thus, $T(n) = T(\\frac{n}{4}) + T(\\frac{3n}{4}) + \\Theta(n)$. And the solution is $T(n) = \\Theta(n\\lg{n})$, which is solved in exercise 4.4-9.","title":"9.3-3"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-4","text":"","title":"9.3-4"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-5","text":"The idea is similar. SELECT(A, p, r, i) if p == r return A[p] median = BLACK-BOX-MEDIAN(A, p, r) q = PARTITION(A, p, r, median) k = q - p + 1 if i == k return A[q] elseif i < k return SELECT(A, p, q - 1, i) else return SELECT(A, q + 1, r, i - k) First we find the median of the array, and use that median as pivot to partition the array. Then we recursively call the procedure to get the ith element. Now let's analysis the running time. The BLAK-BOX-MEDIAN takes O(n), and partition method also takes O(n), then it at least reduces the problem size to $\\frac{n}{2}$, thus we have $T(n) = T(\\frac{n}{2}) + O(n)$. We guess T(n) = O(n), substituting it to the recurrence yielding: $$ \\begin{eqnarray} T(n) &=& T(\\frac{n}{2}) + O(n) \\\\ &\\leq& c\\frac{n}{2} + dn \\\\ &=& (\\frac{c}{2} + d)n \\\\ &=& O(n) \\end{eqnarray} $$ So the algorithm solves the problem in linear time.","title":"9.3-5"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-6","text":"The k quantiles of an n-element set are $A[\\lceil \\frac{n}{k} \\rceil], A[2\\lceil \\frac{n}{k} \\rceil], \\ldots, A[(k - 1)\\lceil \\frac{n}{k} \\rceil]$. And the idea is simple, first we divide the elements into two parts, one part contains $\\lceil \\frac{k}{2} \\rceil$ quantiles, and the other part contains $ k - \\lceil \\frac{k}{2} \\rceil$ quantiles. Then we recursively call the procedure on the two parts. In order to divide the elements into two parts, we use the SELECT procedure to get the $\\lceil \\frac{k}{2} \\rceil\\lceil \\frac{n}{k} \\rceil$th smallest element, which is the last element of the $\\lceil \\frac{k}{2} \\rceil$ quantiles. KTH-QUANTILES(A, p, r, k) if k == 1 return else: i = MATH.CEIL(k / 2) * SIZE-OF-ELEMENTS-IN-EACH-QUANTILE index, quantile = SELECT(A, p, r, i) OUTPUT quantile KTH-QUANTILES(A, p, index, MATH.CEIL(k / 2)) KTH-QUANTILES(A, index + 1, r, k - MATH.CEIL(k / 2)) Now let's analysis the running time. We have $T(n, k) = 2T(\\frac{n}{2}, \\frac{k}{2}) + O(n)$. If we draw a recursion tree, we can see that each level of the tree costs O(n) and the depth of tree is $\\lg{k}$, thus the running time is $O(n\\lg{k})$.","title":"9.3-6"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-7","text":"First, we find the median of the set, it costs O(n), then we create another array that contains the absolute distance between the median and each element. Then we use the SELECT procedure to select the kth smallest element p in the new array, at last, we compare each element in S with median, if the distance between element and median is not greater than p, then the element one of the k closest elements. Each step costs O(n), so the algorithm runs in O(n). K-CLOSEST(A, k) i = MATH.CEIL(A.length / 2) median = SELECT(A, i) let B[1..n] be a new array for j = 1 to n B[j] = MATH.ABS(A[j] - median) kth = SELECT(B, k) let C be a new array for j = 1 to n if MATH.ABS(A[j] - median) < kth add A[j] to C for j = 1 to n MATH.ABS(A[j] - median) == kth add A[j] to C if C.length >= k break return C Because the elements in A are all distinct, so each element in B have at most 1 duplicate. The reason to scan A twice to get C is that A is not sorted, if we combine the last two loops together, it is possible we cannot get the correct k closest elements to the median.","title":"9.3-7"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-8","text":"We can first get the medians of x and y, if the median of x is smaller than the median of y, then the median of all is in x's right part and y's left part, if the median of x is larger than the median of y, then the median of all is in x's left part and y's right part. So we can recursively solves the problem. MEDIAN-OF-TWO-SORTED-ARRAYS(X, px, rx, Y, py, ry) if rx - px == 1 if X[px] <= Y[py] return MATH.MIN(X[rx], Y[py]) else: return MATH.MIN(X[px], Y[ry]) median-index-of-x = MATH.FLOOR((px + rx) / 2) median-index-of-y = MATH.FLOOR((py + ry) / 2) if X[index] < Y[median-index-of-y] return MEDIAN-OF-TWO-SORTED-ARRAYS(X, index, rx, Y, py, median-index-of-y) elif X[index] > Y[median-index-of-y] return MEDIAN-OF-TWO-SORTED-ARRAYS(X, px, index, Y, median-index-of-y, ry) else: return X[index] Each recursive procedure reduces the problem size to $\\frac{n}{2}$, so we have $T(n) = T(\\frac{n}{2}) + \\Theta(1)$. Let's solve the recurrence by the master method. Here we have a = 1, b = 2, $f(n) = \\Theta(1)$. so $n^{\\log_b{a}} = 1$, thus $f(n) = \\Theta(n^{\\log_b{a}})$, so $T(n) = \\Theta(n^{\\log_b{a}}\\lg{n}) = \\Theta(\\lg{n})$.","title":"9.3-8"},{"location":"09-Medians-and-Order-Statistics/9.3-Selection-in-worst-case-linear-time/#93-9","text":"Suppose the n wells have y-coordinates $y_1, y_2, \\ldots, y_n$, and we need to find $y_0$ such that the value $\\sum_{i = 1}^{n}|y_i - y_0|$ is minimum. This is the 1-dimensional case of Geometric median , and the median minimize the sum. Explanation .","title":"9.3-9"},{"location":"09-Medians-and-Order-Statistics/Problems/","text":"Problems 9-1 a Sorting requires $O(n\\lg{n})$, plus $O(i)$ to list the i numbers, the total running time is $O(n\\lg{n}) + O(i)$. b It requires $O(n)$ to build a max-priority queue, the EXTRACT-MAX costs $O(\\lg{n})$, thus the total running time is $O(n) + iO(\\lg{n})$. c First we need to find the n - i + 1 smallest element, this requires O(n). And the array is partitioned around the n - i + 1 smallest element. So we need to sort the i - 1 numbers, it costs $O((i - 1)\\lg{(i - 1)})$, so the total running time is $O(n) + O((i - 1)\\lg{(i - 1)})$. 9-2 a The median of $x_1, x_2, \\ldots, x_n$ is $x_{\\lceil \\frac{n}{2} \\rceil}$. So $\\sum_{x_i < x_k} w_i = \\frac{1}{n}(\\lceil \\frac{n}{2} \\rceil - 1) < \\frac{1}{n}(\\frac{n}{2} + 1 - 1) = \\frac{1}{2}$. $\\sum_{x_i > x_k} w_i = \\frac{1}{n}(n - \\lceil \\frac{n}{2} \\rceil) \\leq \\frac{1}{n}(n - \\frac{n}{2}) = \\frac{1}{2}$. b First we sort the elements, and then iterate the sorted elements and sum the corresponding weight, if the sum of weights is bigger than 0.5, then the current element is the weighted median. WEIGHTED-MEDIAN(A) SORT(A) weight-sum = 0 for i = 1 to n weight-sum += get weight of A[i] if weight-sum >= 0.5: return A[i] We need $O(n\\lg{n})$ to sort all elements, and need O(n) to find the weighted median, the total running time is thus $O(n\\lg{n}) + O(n) = O(n\\lg{n})$. c First we get the median of all x, and sum the weights of elements whose value is less than median, and sum the weights of elements whose value is larger than median. If the sum of weights in left part is smaller than 0.5 and the sum of weights in right part is also not larger than 0.5, then the median is weighted median. Otherwise, we do the same procedure on the on the left part if its sum of weights is larger than 0.5, or the right part. WEIGHTED-MEDIAN(A) if n = 1 return A[1] if n = 2 if A[1].weight >= A[2].weight return A[1] else return A[2] else median = SELECT(A, MATH.CEIL(A / 2)) left-weights = 0 right-weights = 0 for i = 1 to MATH.CEIL(A / 2) - 1 left-weights = left-weights + A[i].weight for i = MATH.CEIL(A / 2) + 1 to n right-weights = right-weights + A[i].weight if left-weights < 0.5 and right-weights <= 0.5 return median elseif left-weights >= 0.5 median.weight = median.weight + right-weights A' = { x <= median } return WEIGHTED-MEDIAN(A') elseif right-weights > 0.5 median.weight = median.weight + left-weights A' = { x >= median } return WEIGHTED-MEDIAN(A') The recurrence of the algorithm is $T(n) = T(\\frac{n}{2}) + \\Theta(n)$. Let's solve it by the master method. Here we have a = 1, b = 2, $f(n) = \\Theta(n)$. So $\\log_b{a} = \\log_2{1} = 0$, so $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$ for $\\epsilon = 0.5$. And $af(\\frac{n}{b}) = f(\\frac{n}{2}) \\leq cf(n)$ for $c = \\frac{1}{2}$ and all sufficiently large n, thus $T(n) = \\Theta(f(n)) = \\Theta(n)$. d Let p denotes the weighted median, so $f(p) = \\sum_{i = 1}^n w_id(p - p_i) = \\sum_{i = 1}^n w_i|p - p_i|$. Since we need to prove the weighted median is the best solution, so for any other point x other than p we should have $f(x) \\geq f(p)$, or $f(x) - f(p) \\geq 0$. So $f(x) - f(p) = \\sum_{i = 1}^n w_i|x - p_i| - \\sum_{i = 1}^n w_i|p - p_i| = \\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|)$. First let's check the situation when x < p. When $p_i \\leq x < p$, $|x - p_i| - |p - p_i| = x - p_i - (p - p_i) = x - p$, when $x < p_i < p$, $|x - p_i| - |p - p_i| = (p_i - x) - (p - p_i) > 0 - (p - x) = x - p$. When $x < p \\leq p_i$, $|x - p_i| - |p - p_i| = p_i - x - (p_i - p) = p - x$. Thus: $$ \\begin{eqnarray} \\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|) &>& (x - p)\\sum_{p_i < p}w_i + (p - x)\\sum_{p_i \\geq p}w_i \\\\ &=& (p - x)(\\sum_{p_i \\geq p}w_i - \\sum_{p_i < p}w_i) \\end{eqnarray} $$ Because p is weighted median, so $\\sum_{p_i \\geq p}w_i > \\frac{1}{2}$, $\\sum_{p_i < p}w_i < \\frac{1}{2}$, so $(p - x)(\\sum_{p_i \\geq p}w_i - \\sum_{p_i < p}w_i) > 0$. Now let's check the situation when x > p. When $p_i \\leq p < x$, $|x - p_i| - |p - p_i| = x - p_i - (p - p_i) = x - p$, when $p < p_i < x$, $|x - p_i| - |p - p_i| = x - p_i - (p_i - p) > 0 - (x - p) = p - x$, when $p < x \\leq p_i$, $|x - p_i| - |p - p_i| = p_i - x - (p_i - p) = p - x$. Thus: $$ \\begin{eqnarray} \\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|) &>& (p - x)\\sum_{p_i > p}w_i + (x - p)\\sum_{p_i \\leq p}w_i \\\\ &=& (x - p)(\\sum_{p_i \\leq p}w_i - \\sum_{p_i > p}w_i) \\end{eqnarray} $$ Because $\\sum_{p_i \\leq p}w_i > \\frac{1}{2}$ and $\\sum_{p_i > p}w_i < \\frac{1}{2}$, so $(x - p)(\\sum_{p_i \\leq p}w_i - \\sum_{p_i > p}w_i) > 0$. So for any point x other than p we have $f(x) > f(p)$, so the weighted median is the best solution. e We need to find a point p(x, y) such that $\\sum_{i = 1}^n w_i(|x - x_i| + |y - y_i|)$ is minimum. Because $\\sum_{i = 1}^n w_i(|x - x_i| + |y - y_i|) = \\sum_{i = 1}^n w_i|x - x_i| + \\sum_{i = 1}^n w_i|y - y_i|$, the problem is actually 2 1-dimensional problems. Thus let x be the weighted median of all x coordinate values, and let y be the weighted median of all y coordinate values, so p(x, y) is the best solution. 9-3 a If $i \\geq \\frac{n}{2}$, then we use the SELECT algorithm, otherwise we group every two elements into pairs $(a_j, b_j)$ and make sure $a_j \\leq b_j$, if n is odd, we also let the last element be a pair, this step needs $\\lfloor \\frac{n}{2} \\rfloor$. So now we have $\\lceil \\frac{n}{2} \\rceil$ pairs. Then we recursively call the algorithm on $a_j$, so we can get the ith smallest element of all $a_j$, this step requires $U_i(\\lceil \\frac{n}{2} \\rceil)$. Notice that the partition method partition all $a_j$ into two parts, so the ith smallest element of all elements could only be among $a_1\\ldots{a_i}$ and $b_1\\ldots{b_i}$. Then we run the SELECT algorithm on the 2i elements to find the ith smallest element. SMALL-ORDER-STATISTICS(A, i) if i >= n / 2 return SELECT(A, i) let pairs be a new array for i = 1 to n with step = 2 if i + 1 <= n if A[i] <= A[i + 1] insert [A[i], A[i + 1]] into pairs else insert [A[i + 1], A[i]] into pairs else insert [A[i]] into pairs SMALL-ORDER-STATISTICS(pairs, i) // Run algorithm on all aj return SELECT(pairs, i) // Run algorithm on a1 to ai plus b1 to bi b Let's solve by the substitution method. We start by assuming that $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i}))$ holds for all positive m < n, in particular for $m = \\lceil \\frac{n}{2} \\rceil$, yielding $U_i(\\lceil \\frac{n}{2} \\rceil) = \\lceil \\frac{n}{2} \\rceil + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}))$. Substituting into the recurrence yields: $$ \\begin{eqnarray} U_i(n) &=& \\lfloor \\frac{n}{2} \\rfloor + \\lceil \\frac{n}{2} \\rceil + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i})) + T(2i) \\\\ &=& n + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i})) + O(T(2i)) \\\\ &=& n + O(T(2i)(\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}) + 1)) \\\\ &=& n + O(T(2i)(\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}) + \\lg2)) \\\\ &=& n + O(T(2i)\\lg(\\frac{n}{i})) \\end{eqnarray} $$ c If i is a constant, and because $T(n) = O(n)$, so T(2i) = O(2i), so T(2i) is also a constant, and $O(\\lg{\\frac{n}{i}}) = O(\\lg{n})$. Thus $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i})) = n + O(\\lg{n})$. d It's so obvious, we just replace i with $\\frac{n}{k}$ and yields $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i})) = n + O(T(2\\frac{n}{k})\\lg(\\frac{n}{\\frac{n}{k}})) = n + O(T(\\frac{2n}{k})\\lg{k})$. 9-4 a $z_i$ and $z_j$ are compared if and only if the first element to be chosen as a pivot from $Z_{ijk}$ is either $z_i$ or $z_j$. And the range of $Z_{ijk}$ depends on k. So: $$ \\begin{eqnarray} E[X_{ijk}] &=& \\begin{cases} \\frac{2}{j - k + 1} & \\text{if } k \\leq i < j \\\\ \\frac{2}{j - i + 1} & \\text{if } i < k \\leq j \\\\ \\frac{2}{k - i + 1} & \\text{if } i < j < k \\end{cases} \\end{eqnarray} $$ b $$ \\begin{eqnarray} E[X_k] &=& \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n E[X_{ijk}] \\\\ &=& \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} E[X_{ijk}] + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} E[X_{ijk}] + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} E[X_{ijk}] \\\\ &=& \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{2}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{2}{j - i + 1} + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} \\frac{2}{k - i + 1} \\\\ &=& 2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} \\frac{1}{k - i + 1}) \\\\ &=& 2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\\\ &=& 2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} - \\sum_{j = k}^n \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\\\ &=& 2(\\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} - \\sum_{j = k}^n \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\end{eqnarray} $$ c We have $\\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} \\leq n$ ( source ), and $\\sum_{k + 1}^n \\frac{j - k - 1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1} < \\sum_{k + 1}^n 1 + \\sum_{i = 1}^{k - 2} 1 = n - k + k - 2 = n - 2 < n$, so $E[X_k] \\leq 2(n + n) = 4n$. d Since $E[X_k] \\leq 4n$, thus $T(n) = O(n)$.","title":"Problems"},{"location":"09-Medians-and-Order-Statistics/Problems/#problems","text":"","title":"Problems"},{"location":"09-Medians-and-Order-Statistics/Problems/#9-1","text":"","title":"9-1"},{"location":"09-Medians-and-Order-Statistics/Problems/#a","text":"Sorting requires $O(n\\lg{n})$, plus $O(i)$ to list the i numbers, the total running time is $O(n\\lg{n}) + O(i)$.","title":"a"},{"location":"09-Medians-and-Order-Statistics/Problems/#b","text":"It requires $O(n)$ to build a max-priority queue, the EXTRACT-MAX costs $O(\\lg{n})$, thus the total running time is $O(n) + iO(\\lg{n})$.","title":"b"},{"location":"09-Medians-and-Order-Statistics/Problems/#c","text":"First we need to find the n - i + 1 smallest element, this requires O(n). And the array is partitioned around the n - i + 1 smallest element. So we need to sort the i - 1 numbers, it costs $O((i - 1)\\lg{(i - 1)})$, so the total running time is $O(n) + O((i - 1)\\lg{(i - 1)})$.","title":"c"},{"location":"09-Medians-and-Order-Statistics/Problems/#9-2","text":"","title":"9-2"},{"location":"09-Medians-and-Order-Statistics/Problems/#a_1","text":"The median of $x_1, x_2, \\ldots, x_n$ is $x_{\\lceil \\frac{n}{2} \\rceil}$. So $\\sum_{x_i < x_k} w_i = \\frac{1}{n}(\\lceil \\frac{n}{2} \\rceil - 1) < \\frac{1}{n}(\\frac{n}{2} + 1 - 1) = \\frac{1}{2}$. $\\sum_{x_i > x_k} w_i = \\frac{1}{n}(n - \\lceil \\frac{n}{2} \\rceil) \\leq \\frac{1}{n}(n - \\frac{n}{2}) = \\frac{1}{2}$.","title":"a"},{"location":"09-Medians-and-Order-Statistics/Problems/#b_1","text":"First we sort the elements, and then iterate the sorted elements and sum the corresponding weight, if the sum of weights is bigger than 0.5, then the current element is the weighted median. WEIGHTED-MEDIAN(A) SORT(A) weight-sum = 0 for i = 1 to n weight-sum += get weight of A[i] if weight-sum >= 0.5: return A[i] We need $O(n\\lg{n})$ to sort all elements, and need O(n) to find the weighted median, the total running time is thus $O(n\\lg{n}) + O(n) = O(n\\lg{n})$.","title":"b"},{"location":"09-Medians-and-Order-Statistics/Problems/#c_1","text":"First we get the median of all x, and sum the weights of elements whose value is less than median, and sum the weights of elements whose value is larger than median. If the sum of weights in left part is smaller than 0.5 and the sum of weights in right part is also not larger than 0.5, then the median is weighted median. Otherwise, we do the same procedure on the on the left part if its sum of weights is larger than 0.5, or the right part. WEIGHTED-MEDIAN(A) if n = 1 return A[1] if n = 2 if A[1].weight >= A[2].weight return A[1] else return A[2] else median = SELECT(A, MATH.CEIL(A / 2)) left-weights = 0 right-weights = 0 for i = 1 to MATH.CEIL(A / 2) - 1 left-weights = left-weights + A[i].weight for i = MATH.CEIL(A / 2) + 1 to n right-weights = right-weights + A[i].weight if left-weights < 0.5 and right-weights <= 0.5 return median elseif left-weights >= 0.5 median.weight = median.weight + right-weights A' = { x <= median } return WEIGHTED-MEDIAN(A') elseif right-weights > 0.5 median.weight = median.weight + left-weights A' = { x >= median } return WEIGHTED-MEDIAN(A') The recurrence of the algorithm is $T(n) = T(\\frac{n}{2}) + \\Theta(n)$. Let's solve it by the master method. Here we have a = 1, b = 2, $f(n) = \\Theta(n)$. So $\\log_b{a} = \\log_2{1} = 0$, so $f(n) = \\Omega(n^{\\log_b{a} + \\epsilon})$ for $\\epsilon = 0.5$. And $af(\\frac{n}{b}) = f(\\frac{n}{2}) \\leq cf(n)$ for $c = \\frac{1}{2}$ and all sufficiently large n, thus $T(n) = \\Theta(f(n)) = \\Theta(n)$.","title":"c"},{"location":"09-Medians-and-Order-Statistics/Problems/#d","text":"Let p denotes the weighted median, so $f(p) = \\sum_{i = 1}^n w_id(p - p_i) = \\sum_{i = 1}^n w_i|p - p_i|$. Since we need to prove the weighted median is the best solution, so for any other point x other than p we should have $f(x) \\geq f(p)$, or $f(x) - f(p) \\geq 0$. So $f(x) - f(p) = \\sum_{i = 1}^n w_i|x - p_i| - \\sum_{i = 1}^n w_i|p - p_i| = \\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|)$. First let's check the situation when x < p. When $p_i \\leq x < p$, $|x - p_i| - |p - p_i| = x - p_i - (p - p_i) = x - p$, when $x < p_i < p$, $|x - p_i| - |p - p_i| = (p_i - x) - (p - p_i) > 0 - (p - x) = x - p$. When $x < p \\leq p_i$, $|x - p_i| - |p - p_i| = p_i - x - (p_i - p) = p - x$. Thus: $$ \\begin{eqnarray} \\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|) &>& (x - p)\\sum_{p_i < p}w_i + (p - x)\\sum_{p_i \\geq p}w_i \\\\ &=& (p - x)(\\sum_{p_i \\geq p}w_i - \\sum_{p_i < p}w_i) \\end{eqnarray} $$ Because p is weighted median, so $\\sum_{p_i \\geq p}w_i > \\frac{1}{2}$, $\\sum_{p_i < p}w_i < \\frac{1}{2}$, so $(p - x)(\\sum_{p_i \\geq p}w_i - \\sum_{p_i < p}w_i) > 0$. Now let's check the situation when x > p. When $p_i \\leq p < x$, $|x - p_i| - |p - p_i| = x - p_i - (p - p_i) = x - p$, when $p < p_i < x$, $|x - p_i| - |p - p_i| = x - p_i - (p_i - p) > 0 - (x - p) = p - x$, when $p < x \\leq p_i$, $|x - p_i| - |p - p_i| = p_i - x - (p_i - p) = p - x$. Thus: $$ \\begin{eqnarray} \\sum_{i = 1}^n w_i(|x - p_i| - |p - p_i|) &>& (p - x)\\sum_{p_i > p}w_i + (x - p)\\sum_{p_i \\leq p}w_i \\\\ &=& (x - p)(\\sum_{p_i \\leq p}w_i - \\sum_{p_i > p}w_i) \\end{eqnarray} $$ Because $\\sum_{p_i \\leq p}w_i > \\frac{1}{2}$ and $\\sum_{p_i > p}w_i < \\frac{1}{2}$, so $(x - p)(\\sum_{p_i \\leq p}w_i - \\sum_{p_i > p}w_i) > 0$. So for any point x other than p we have $f(x) > f(p)$, so the weighted median is the best solution.","title":"d"},{"location":"09-Medians-and-Order-Statistics/Problems/#e","text":"We need to find a point p(x, y) such that $\\sum_{i = 1}^n w_i(|x - x_i| + |y - y_i|)$ is minimum. Because $\\sum_{i = 1}^n w_i(|x - x_i| + |y - y_i|) = \\sum_{i = 1}^n w_i|x - x_i| + \\sum_{i = 1}^n w_i|y - y_i|$, the problem is actually 2 1-dimensional problems. Thus let x be the weighted median of all x coordinate values, and let y be the weighted median of all y coordinate values, so p(x, y) is the best solution.","title":"e"},{"location":"09-Medians-and-Order-Statistics/Problems/#9-3","text":"","title":"9-3"},{"location":"09-Medians-and-Order-Statistics/Problems/#a_2","text":"If $i \\geq \\frac{n}{2}$, then we use the SELECT algorithm, otherwise we group every two elements into pairs $(a_j, b_j)$ and make sure $a_j \\leq b_j$, if n is odd, we also let the last element be a pair, this step needs $\\lfloor \\frac{n}{2} \\rfloor$. So now we have $\\lceil \\frac{n}{2} \\rceil$ pairs. Then we recursively call the algorithm on $a_j$, so we can get the ith smallest element of all $a_j$, this step requires $U_i(\\lceil \\frac{n}{2} \\rceil)$. Notice that the partition method partition all $a_j$ into two parts, so the ith smallest element of all elements could only be among $a_1\\ldots{a_i}$ and $b_1\\ldots{b_i}$. Then we run the SELECT algorithm on the 2i elements to find the ith smallest element. SMALL-ORDER-STATISTICS(A, i) if i >= n / 2 return SELECT(A, i) let pairs be a new array for i = 1 to n with step = 2 if i + 1 <= n if A[i] <= A[i + 1] insert [A[i], A[i + 1]] into pairs else insert [A[i + 1], A[i]] into pairs else insert [A[i]] into pairs SMALL-ORDER-STATISTICS(pairs, i) // Run algorithm on all aj return SELECT(pairs, i) // Run algorithm on a1 to ai plus b1 to bi","title":"a"},{"location":"09-Medians-and-Order-Statistics/Problems/#b_2","text":"Let's solve by the substitution method. We start by assuming that $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i}))$ holds for all positive m < n, in particular for $m = \\lceil \\frac{n}{2} \\rceil$, yielding $U_i(\\lceil \\frac{n}{2} \\rceil) = \\lceil \\frac{n}{2} \\rceil + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}))$. Substituting into the recurrence yields: $$ \\begin{eqnarray} U_i(n) &=& \\lfloor \\frac{n}{2} \\rfloor + \\lceil \\frac{n}{2} \\rceil + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i})) + T(2i) \\\\ &=& n + O(T(2i)\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i})) + O(T(2i)) \\\\ &=& n + O(T(2i)(\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}) + 1)) \\\\ &=& n + O(T(2i)(\\lg(\\frac{\\lceil \\frac{n}{2} \\rceil}{i}) + \\lg2)) \\\\ &=& n + O(T(2i)\\lg(\\frac{n}{i})) \\end{eqnarray} $$","title":"b"},{"location":"09-Medians-and-Order-Statistics/Problems/#c_2","text":"If i is a constant, and because $T(n) = O(n)$, so T(2i) = O(2i), so T(2i) is also a constant, and $O(\\lg{\\frac{n}{i}}) = O(\\lg{n})$. Thus $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i})) = n + O(\\lg{n})$.","title":"c"},{"location":"09-Medians-and-Order-Statistics/Problems/#d_1","text":"It's so obvious, we just replace i with $\\frac{n}{k}$ and yields $U_i(n) = n + O(T(2i)\\lg(\\frac{n}{i})) = n + O(T(2\\frac{n}{k})\\lg(\\frac{n}{\\frac{n}{k}})) = n + O(T(\\frac{2n}{k})\\lg{k})$.","title":"d"},{"location":"09-Medians-and-Order-Statistics/Problems/#9-4","text":"","title":"9-4"},{"location":"09-Medians-and-Order-Statistics/Problems/#a_3","text":"$z_i$ and $z_j$ are compared if and only if the first element to be chosen as a pivot from $Z_{ijk}$ is either $z_i$ or $z_j$. And the range of $Z_{ijk}$ depends on k. So: $$ \\begin{eqnarray} E[X_{ijk}] &=& \\begin{cases} \\frac{2}{j - k + 1} & \\text{if } k \\leq i < j \\\\ \\frac{2}{j - i + 1} & \\text{if } i < k \\leq j \\\\ \\frac{2}{k - i + 1} & \\text{if } i < j < k \\end{cases} \\end{eqnarray} $$","title":"a"},{"location":"09-Medians-and-Order-Statistics/Problems/#b_3","text":"$$ \\begin{eqnarray} E[X_k] &=& \\sum_{i = 1}^{n - 1}\\sum_{j = i + 1}^n E[X_{ijk}] \\\\ &=& \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} E[X_{ijk}] + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} E[X_{ijk}] + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} E[X_{ijk}] \\\\ &=& \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{2}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{2}{j - i + 1} + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} \\frac{2}{k - i + 1} \\\\ &=& 2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = 1}^{k - 2}\\sum_{j = i + 1}^{k - 1} \\frac{1}{k - i + 1}) \\\\ &=& 2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 1}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\\\ &=& 2(\\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} - \\sum_{j = k}^n \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\\\ &=& 2(\\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} + \\sum_{i = k}^{n - 1}\\sum_{j = i + 1}^{n} \\frac{1}{j - k + 1} - \\sum_{j = k}^n \\frac{1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1}) \\end{eqnarray} $$","title":"b"},{"location":"09-Medians-and-Order-Statistics/Problems/#c_3","text":"We have $\\sum_{i = 1}^{k}\\sum_{j = k}^{n} \\frac{1}{j - i + 1} \\leq n$ ( source ), and $\\sum_{k + 1}^n \\frac{j - k - 1}{j - k + 1} + \\sum_{i = 1}^{k - 2} \\frac{k - i - 1}{k - i + 1} < \\sum_{k + 1}^n 1 + \\sum_{i = 1}^{k - 2} 1 = n - k + k - 2 = n - 2 < n$, so $E[X_k] \\leq 2(n + n) = 4n$.","title":"c"},{"location":"09-Medians-and-Order-Statistics/Problems/#d_2","text":"Since $E[X_k] \\leq 4n$, thus $T(n) = O(n)$.","title":"d"},{"location":"10-Elementary-Data-Structures/10.1-Stacks-and-queues/","text":"10.1 Stacks and queues 10.1-1 10.1-2 The first stack S1 starts at the first index, when pushing element into S1 , we let S1.top += 1 , and the second stack S2 starts at the last index, when pushing element into S2 , we let S2.top -= 1 , when S1.top > S2.top , it overflows. 10.1-3 10.1-4 Initially, we let Q.head = 0 and Q.tail = 1 . ENQUEUE(Q, x) if Q.head == Q.tail error \"overflow\" Q[Q.tail] = x if Q.tail == Q.length Q.tail = 1 else Q.tail = Q.tail + 1 if Q.head == 0 Q.head = 1 DEQUEUE(Q) if Q.head == 0 error \"underflow\" x = Q[Q.head] if Q.head = Q.length Q.head = 1 else Q.head = Q.head + 1 if Q.head == Q.tail Q.head = 0 Q.tail = 1 return x 10.1-5 IS-EMPTY(DQ) return DQ.left == 0 and DQ.right == DQ.size IS-FULL(DQ) return DQ.right - DQ.left == 1 APPEND(DQ, x) if IS-FULL(DQ) error \"overflow\" DQ.right = DQ.right - 1 if DQ.right == 0 DQ.right = DQ.size DQ[DQ.right] = x POP(DQ) if IS-EMPTY(DQ) error \"underflow\" if DQ.right == DQ.size + 1 DQ.right = 1 x = DQ[DQ.right] if DQ.left == DQ.right DQ.left = 0 DQ.right = DQ.size + 1 else DQ.right = (DQ.right + 1) % DQ.size return x APPEND-LEFT(DQ, x) if IS-FULL(DQ) error \"overflow\" DQ.left = DQ.left + 1 if DQ.left == DQ.size + 1 DQ.left = 1 DQ[DQ.left] = x SHIFT(DQ) if IS-EMPTY(DQ) error \"underflow\" if DQ.left == 0 DQ.left = DQ.size x = DQ[DQ.left] if DQ.left == DQ.right DQ.left = 0 DQ.right = DQ.size + 1 else DQ.left = DQ.left - 1 if DQ.left == 0 DQ.left = DQ.size return x 10.1-6 ENQUEUE(Q, x) if stack-a.count + stack-b.count == n error \"overflow\" stack-a.push(x) DEQUEUE(Q) if stack-a.count == 0 and stack-b.count == 0 error \"underflow\" if stack-b.count == 0 while stack-a.count != 0 stack-b.push(stack-a.pop()) return stack-b.pop() We create two stacks stack-a and stack-b , the ENQUEUE operation push x to stack-a , the DEQUEUE operation checks if stack-b is empty first, if it's empty, then pop every elements from stack-a , and push them to stack-b . Then call pop on stack-b . The running time of ENQUEUE is O(1), but the running time of DEQUEUE is O(n). 10.1-7 PUSH(S, x) if queue.count == n error \"overflow\" queue.enqueue(x) POP(S) if queue.count == 0 error \"underflow\" while not queue.count == 0 x = queue.dequeue() if not queue.count == 0 queue-auxiliary.enqueue(x) exchange queue with queue-auxiliary return x We create two queues queue and queue-auxiliary , the PUSH operation insert x to queue , and the POP operation moves all elements except the last one into queue-auxiliary , and exchange queue with queue-auxiliary , then return x . The running time of PUSH is O(1), but the running time of POP is O(n).","title":"10.1 Stacks and queues"},{"location":"10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-stacks-and-queues","text":"","title":"10.1 Stacks and queues"},{"location":"10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-1","text":"","title":"10.1-1"},{"location":"10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-2","text":"The first stack S1 starts at the first index, when pushing element into S1 , we let S1.top += 1 , and the second stack S2 starts at the last index, when pushing element into S2 , we let S2.top -= 1 , when S1.top > S2.top , it overflows.","title":"10.1-2"},{"location":"10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-3","text":"","title":"10.1-3"},{"location":"10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-4","text":"Initially, we let Q.head = 0 and Q.tail = 1 . ENQUEUE(Q, x) if Q.head == Q.tail error \"overflow\" Q[Q.tail] = x if Q.tail == Q.length Q.tail = 1 else Q.tail = Q.tail + 1 if Q.head == 0 Q.head = 1 DEQUEUE(Q) if Q.head == 0 error \"underflow\" x = Q[Q.head] if Q.head = Q.length Q.head = 1 else Q.head = Q.head + 1 if Q.head == Q.tail Q.head = 0 Q.tail = 1 return x","title":"10.1-4"},{"location":"10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-5","text":"IS-EMPTY(DQ) return DQ.left == 0 and DQ.right == DQ.size IS-FULL(DQ) return DQ.right - DQ.left == 1 APPEND(DQ, x) if IS-FULL(DQ) error \"overflow\" DQ.right = DQ.right - 1 if DQ.right == 0 DQ.right = DQ.size DQ[DQ.right] = x POP(DQ) if IS-EMPTY(DQ) error \"underflow\" if DQ.right == DQ.size + 1 DQ.right = 1 x = DQ[DQ.right] if DQ.left == DQ.right DQ.left = 0 DQ.right = DQ.size + 1 else DQ.right = (DQ.right + 1) % DQ.size return x APPEND-LEFT(DQ, x) if IS-FULL(DQ) error \"overflow\" DQ.left = DQ.left + 1 if DQ.left == DQ.size + 1 DQ.left = 1 DQ[DQ.left] = x SHIFT(DQ) if IS-EMPTY(DQ) error \"underflow\" if DQ.left == 0 DQ.left = DQ.size x = DQ[DQ.left] if DQ.left == DQ.right DQ.left = 0 DQ.right = DQ.size + 1 else DQ.left = DQ.left - 1 if DQ.left == 0 DQ.left = DQ.size return x","title":"10.1-5"},{"location":"10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-6","text":"ENQUEUE(Q, x) if stack-a.count + stack-b.count == n error \"overflow\" stack-a.push(x) DEQUEUE(Q) if stack-a.count == 0 and stack-b.count == 0 error \"underflow\" if stack-b.count == 0 while stack-a.count != 0 stack-b.push(stack-a.pop()) return stack-b.pop() We create two stacks stack-a and stack-b , the ENQUEUE operation push x to stack-a , the DEQUEUE operation checks if stack-b is empty first, if it's empty, then pop every elements from stack-a , and push them to stack-b . Then call pop on stack-b . The running time of ENQUEUE is O(1), but the running time of DEQUEUE is O(n).","title":"10.1-6"},{"location":"10-Elementary-Data-Structures/10.1-Stacks-and-queues/#101-7","text":"PUSH(S, x) if queue.count == n error \"overflow\" queue.enqueue(x) POP(S) if queue.count == 0 error \"underflow\" while not queue.count == 0 x = queue.dequeue() if not queue.count == 0 queue-auxiliary.enqueue(x) exchange queue with queue-auxiliary return x We create two queues queue and queue-auxiliary , the PUSH operation insert x to queue , and the POP operation moves all elements except the last one into queue-auxiliary , and exchange queue with queue-auxiliary , then return x . The running time of PUSH is O(1), but the running time of POP is O(n).","title":"10.1-7"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/","text":"10.2 Linked lists 10.2-1 We can implement INSERT in O(1) time, but cannot implement DELETE in O(1) time, it's O(n). 10.2-2 PUSH(S, x) S.L.insert(x) POP(S) if S.L.head == NIL error \"underflow\" x = S.L.head S.L.head = S.L.head.next return x 10.2-3 ENQUEUE(Q, x) new = SINGLY-LINKED-NODE(x) if Q.L.head == NIL Q.L.head = new Q.L.tail = new else Q.L.tail.next = new Q.L.tail = new DEQUEUE(Q) if Q.L.head == NIL error \"underflow\" x = Q.L.head Q.L.head = Q.L.head.next return x 10.2-4 LIST-SEARCH'(L, k) x = L.nil.next L.nil.key = k while x.key != k x = x.next if x != L.nil return x else return NIL 10.2-5 SEARCH(D, k) x = D.L.search(k) if x != D.L.nil return x else return NIL INSERT(D, k, v) x = SEARCH(D, k) if x != D.L.nil: error \"The specified key already exists\" else: INSERT(D.L, k, v) DELETE(D, k) x = SEARCH(D, k) if x == D.L.nil: error \"The specified key does not exist\" else: DELETE(D.L, k) The running time of INSERT , DELETE and SEARCH are all O(n). 10.2-6 We can use two singly linked lists to represent s1 and s2 . In order to union s1 and s2 , we simply let the tail of s1 points to the head of s2 , and let the tail of s2 be the tail of s1 . SET-UNION(s1, s2) s1.tail.next = s2.head s1.tail = s2.tail return s1 10.2-7 REVERSE-SINGLY-LINKED-LIST(L) prev = L.head current = prev.next if prev != NIL else NIL while current != NIL next = current.next current.next = prev prev = current current = next L.head.next = NIL temp = L.head L.head = L.tail L.tail = temp 10.2-8 We can use the memory addresses of the pointers as the k-bit integers, so x.np = memory-address(x.next) XOR memory-address(x.pre) . SEARCH(L, k) node = L.head prev-pointer = 0 while node != NIL and node.key != k: if node == L.tail: return NIL new-prev-pointer = memory-address(node) node = value-at-memory-address(node.np XOR prev-pointer) prev-pointer = new-prev-pointer return node INSERT(L, k) new = ONE-POINTER-DOUBLY-LINKED-NODE(key) if L.head is NIL: L.head = new L.head.np = 0 XOR 0 L.tail = new L.tail.np = 0 XOR 0 else: new.np = memory-address(L.head) XOR 0 L.head.np = (L.head.np XOR 0) XOR memory-address(new) L.head = new DELETE(L, k) node = L.head prev-pointer = 0 prev-node = NIL while node != NIL and node.key != key: if node == L.tail: error \"The specified key does not exist\" new-prev-pointer = memory-address(node) prev-node = node node = value-at-memory-address(node.np XOR prev-pointer) prev-pointer = new-prev-pointer if node == NIL: error \"The specified key does not exist\" if node == L.head: if L.head == L.tail: L.head = NIL L.tail = NIL else: next = value-at-memory-address(node.np XOR 0) next.np = (next.np XOR memory-address(node)) XOR 0 L.head = next else if node == L.tail: prev-node.np = (prev-node.np XOR memory-address(node)) XOR 0 L.tail = prev-node else: next = value-at-memory-address(node.np XOR prev_pointer) next.np = (next.np XOR memory-address(node)) XOR prev-pointer prev-node.np = (prev-node.np XOR memory-address(node)) XOR memory-address(next) REVERSE(L) temp = L.head L.head = L.tail L.tail = temp","title":"10.2 Linked lists"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/#102-linked-lists","text":"","title":"10.2 Linked lists"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/#102-1","text":"We can implement INSERT in O(1) time, but cannot implement DELETE in O(1) time, it's O(n).","title":"10.2-1"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/#102-2","text":"PUSH(S, x) S.L.insert(x) POP(S) if S.L.head == NIL error \"underflow\" x = S.L.head S.L.head = S.L.head.next return x","title":"10.2-2"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/#102-3","text":"ENQUEUE(Q, x) new = SINGLY-LINKED-NODE(x) if Q.L.head == NIL Q.L.head = new Q.L.tail = new else Q.L.tail.next = new Q.L.tail = new DEQUEUE(Q) if Q.L.head == NIL error \"underflow\" x = Q.L.head Q.L.head = Q.L.head.next return x","title":"10.2-3"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/#102-4","text":"LIST-SEARCH'(L, k) x = L.nil.next L.nil.key = k while x.key != k x = x.next if x != L.nil return x else return NIL","title":"10.2-4"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/#102-5","text":"SEARCH(D, k) x = D.L.search(k) if x != D.L.nil return x else return NIL INSERT(D, k, v) x = SEARCH(D, k) if x != D.L.nil: error \"The specified key already exists\" else: INSERT(D.L, k, v) DELETE(D, k) x = SEARCH(D, k) if x == D.L.nil: error \"The specified key does not exist\" else: DELETE(D.L, k) The running time of INSERT , DELETE and SEARCH are all O(n).","title":"10.2-5"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/#102-6","text":"We can use two singly linked lists to represent s1 and s2 . In order to union s1 and s2 , we simply let the tail of s1 points to the head of s2 , and let the tail of s2 be the tail of s1 . SET-UNION(s1, s2) s1.tail.next = s2.head s1.tail = s2.tail return s1","title":"10.2-6"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/#102-7","text":"REVERSE-SINGLY-LINKED-LIST(L) prev = L.head current = prev.next if prev != NIL else NIL while current != NIL next = current.next current.next = prev prev = current current = next L.head.next = NIL temp = L.head L.head = L.tail L.tail = temp","title":"10.2-7"},{"location":"10-Elementary-Data-Structures/10.2-Linked-lists/#102-8","text":"We can use the memory addresses of the pointers as the k-bit integers, so x.np = memory-address(x.next) XOR memory-address(x.pre) . SEARCH(L, k) node = L.head prev-pointer = 0 while node != NIL and node.key != k: if node == L.tail: return NIL new-prev-pointer = memory-address(node) node = value-at-memory-address(node.np XOR prev-pointer) prev-pointer = new-prev-pointer return node INSERT(L, k) new = ONE-POINTER-DOUBLY-LINKED-NODE(key) if L.head is NIL: L.head = new L.head.np = 0 XOR 0 L.tail = new L.tail.np = 0 XOR 0 else: new.np = memory-address(L.head) XOR 0 L.head.np = (L.head.np XOR 0) XOR memory-address(new) L.head = new DELETE(L, k) node = L.head prev-pointer = 0 prev-node = NIL while node != NIL and node.key != key: if node == L.tail: error \"The specified key does not exist\" new-prev-pointer = memory-address(node) prev-node = node node = value-at-memory-address(node.np XOR prev-pointer) prev-pointer = new-prev-pointer if node == NIL: error \"The specified key does not exist\" if node == L.head: if L.head == L.tail: L.head = NIL L.tail = NIL else: next = value-at-memory-address(node.np XOR 0) next.np = (next.np XOR memory-address(node)) XOR 0 L.head = next else if node == L.tail: prev-node.np = (prev-node.np XOR memory-address(node)) XOR 0 L.tail = prev-node else: next = value-at-memory-address(node.np XOR prev_pointer) next.np = (next.np XOR memory-address(node)) XOR prev-pointer prev-node.np = (prev-node.np XOR memory-address(node)) XOR memory-address(next) REVERSE(L) temp = L.head L.head = L.tail L.tail = temp","title":"10.2-8"},{"location":"10-Elementary-Data-Structures/10.3-Implementing-pointers-and-objects/","text":"10.3 Implementing pointers and objects 10.3-1 Multiple-array representation:","title":"10.3 Implementing pointers and objects:"},{"location":"10-Elementary-Data-Structures/10.3-Implementing-pointers-and-objects/#103-implementing-pointers-and-objects","text":"","title":"10.3 Implementing pointers and objects"},{"location":"10-Elementary-Data-Structures/10.3-Implementing-pointers-and-objects/#103-1","text":"Multiple-array representation:","title":"10.3-1"}]}